{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFmjOdE6BZXh"
      },
      "source": [
        "#### External vlaidation\n",
        "#### This notebook checks node connectivity change by iterating random groups and calculating the mean median CVS of node-group connections (between the node and all nodes in the group, in the form of node pairs).\n",
        "#### A histogram of mean CVS is plotted and the scores of reseection nodes are marked on the histogram.\n",
        "##### A range of CVS scores - a standard deviation around the mean score - captures the resection in all subjects.\n",
        "##### However, non-resected nodes are also included.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzimKl2MBZXj",
        "outputId": "954618e5-5bd8-4191-b442-b8641316bf58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "main_folder = \"/content/gdrive/My Drive/Ext-val/\"\n",
        "\n",
        "df = pd.read_csv(main_folder + f\"cvs_pairs_ext.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eg00pc90BZXk",
        "outputId": "b44f9dec-b2aa-4967-c3aa-b4a04d86e54b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['PAC', 'SCR-(1,4)', 'SCI-(1,4)', 'PLV-(1,4)', 'PLI-(1,4)', 'CC-(1,4)', 'SCR-(4,8)', 'SCI-(4,8)', 'PLV-(4,8)', 'PLI-(4,8)', 'CC-(4,8)', 'SCR-(8,13)', 'SCI-(8,13)', 'PLV-(8,13)', 'PLI-(8,13)', 'CC-(8,13)', 'SCR-(13,30)', 'SCI-(13,30)', 'PLV-(13,30)', 'PLI-(13,30)', 'CC-(13,30)', 'SCR-(30,70)', 'SCI-(30,70)', 'PLV-(30,70)', 'PLI-(30,70)', 'CC-(30,70)', 'SCR-(70,150)', 'SCI-(70,150)', 'PLV-(70,150)', 'PLI-(70,150)', 'CC-(70,150)']\n"
          ]
        }
      ],
      "source": [
        "connectivity_measures = list(df['CM'].unique())\n",
        "print(connectivity_measures)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from inspect import ismethod\n",
        "from datetime import timedelta as _time\n",
        "from datetime import datetime\n",
        "from collections.abc import Iterable as iterable\n",
        "\n",
        "def some(field): return (field is not None and field != [] and field != {} and field != ()) or field == True\n",
        "def no(field): return not some(field) or field==False or field==''\n",
        "\n",
        "class class_of:\n",
        "    _instance = None\n",
        "    def __init__(_, object):\n",
        "        _._is = type(object)\n",
        "    def inherits(_, *types):\n",
        "        return issubclass(_._is, types)\n",
        "    def has(_, *types): return _.inherits(*types)\n",
        "    def __enter__(self):\n",
        "        self._instance = self\n",
        "        return self\n",
        "    def __exit__(self, type, value, traceback): self._instance = None\n",
        "    @staticmethod\n",
        "    def each_in(list):\n",
        "        if isiterable(list):\n",
        "            return [type(item) for item in list]\n",
        "\n",
        "class struct:\n",
        "    def __init__(table, **sets): table.__dict__.update(sets)\n",
        "    @property\n",
        "    def sets(this): return set(dir(this)) - set(dir(type(this)))\n",
        "    def set(object, **fields):\n",
        "        for field in fields: setattr(object, field, fields[field])\n",
        "    def get(object, *fields): return [getattr(object, field) for field in fields if field in object.__dict__]\n",
        "    def _clonable(set, mask=None):\n",
        "        check = set.__dict__.copy()\n",
        "        clonable = check.copy()\n",
        "        if some(mask): pass\n",
        "#            for field in check:\n",
        "#                if sum([int(_(check[field])) for _ in mask])+sum([int(_(field)) for _ in mask]): clonable.pop(field)\n",
        "        return clonable\n",
        "    @staticmethod\n",
        "    def _from(type):\n",
        "        if hasattr(type, '__dict__'): return struct(**type.__dict__.copy())\n",
        "        return struct()\n",
        "\n",
        "def meta(data, *mask): return struct._from(data)._clonable(mask)\n",
        "def get(data, *fields):\n",
        "    if not issubclass(type(data), dict): data=struct._from(data)._clonable()\n",
        "    return struct(**data).get(*fields)\n",
        "\n",
        "class table(struct):\n",
        "    def _default(field, name, value):\n",
        "        try: return getattr(field, name)\n",
        "        except: setattr(field, name, value)\n",
        "        return value\n",
        "    def clear(this, *fields):\n",
        "        sets = this.sets\n",
        "        if not fields: fields = sets\n",
        "        if fields:\n",
        "            set = [field for field in fields if hasattr(this,field) and not ismethod(getattr(this, field))]\n",
        "            for field in set: delattr(this, field)\n",
        "    def has(this, *fields):\n",
        "        return all([hasattr(this, field) for field in fields])\n",
        "    def has_not(this, *fields): return not this.has(*fields)\n",
        "    def check(this, **KV):\n",
        "        try: check = [KV[key]==this.__dict__[key] for key in KV]\n",
        "        except: return False\n",
        "        return all(check)\n",
        "    def find(this, _type):\n",
        "        return [value for value in this.sets if class_of(get(this,value)[0]).inherits(_type)]\n",
        "    def clone(this):\n",
        "        clone = type(this)()\n",
        "        sets = this._clonable()\n",
        "        clone.set(**sets)\n",
        "        return clone\n",
        "\n",
        "def isiterable(this): return isinstance(this, iterable) and type(this) is not str\n",
        "def default(field, name, value): return table(**field)._default(name, value)\n",
        "\n",
        "def ni(list):\n",
        "    if isiterable(list):\n",
        "        for n,i in enumerate(list): yield n,i\n",
        "    else:\n",
        "        for n,i in enumerate(list.__dict__.keys()): yield n,i\n",
        "\n",
        "class at(table):\n",
        "    DAY, HOUR, MIN = 86400, 3600, 60\n",
        "    def __init__(_, dtime=None, **sets):\n",
        "        _.set(**sets)\n",
        "        if some(dtime) and issubclass(type(dtime), _time): _._time = dtime\n",
        "        else:\n",
        "            d,h,m,s,ms = _._default('d',0), _._default('h',0), _._default('m',0), _._default('s',0), _._default('ms',0)\n",
        "            if not any([d,h,m,s,ms]): now=datetime.now(); _._time = now-datetime(now.year, now.month, now.day)\n",
        "            else: _._time = _time(days=d, hours=h, minutes=m, seconds=s, milliseconds=ms)\n",
        "        _.clear('d','h','m','s','ms')\n",
        "    def __sub__(_, dtime):\n",
        "        of=type(dtime); sets=_._clonable()\n",
        "        if issubclass(of, _time): return at(_._time-dtime, **sets)\n",
        "        elif issubclass(of, at): sets.update(dtime._clonable()); return at(_._time-dtime._time, **sets)\n",
        "    def __add__(_, dtime):\n",
        "        of=type(dtime); sets=_._clonable()\n",
        "        if issubclass(of, _time): return at(_._time+dtime, **sets)\n",
        "        elif issubclass(of, at): sets.update(dtime._clonable()); return at(_._time+dtime._time, **sets)\n",
        "    def __str__(_): return str(_._time)\n",
        "    @property\n",
        "    def seconds(_): return _._time.seconds\n",
        "    @property\n",
        "    def S(_): return _.seconds\n",
        "    @property\n",
        "    def minutes(_): return _._time.seconds/60\n",
        "    @property\n",
        "    def M(_): return _.minutes\n",
        "    @property\n",
        "    def hours(_): return _.minutes/60\n",
        "    @property\n",
        "    def H(_): return _.hours\n",
        "    @property\n",
        "    def days(_): return _._time.days\n",
        "    @property\n",
        "    def D(_): return _.days\n",
        "    @staticmethod\n",
        "    def zero(): return at(_time())\n",
        "\n",
        "from inspect import isfunction, ismethod, isgeneratorfunction, isgenerator, isroutine\n",
        "from inspect import isabstract, isclass, ismodule, istraceback, isframe, iscode, isbuiltin\n",
        "from inspect import ismethoddescriptor, isdatadescriptor, isgetsetdescriptor, ismemberdescriptor\n",
        "from inspect import isawaitable, iscoroutinefunction, iscoroutine\n",
        "\n",
        "from collections.abc import Iterable as iterable\n",
        "\n",
        "import pickle\n",
        "\n",
        "def isfx(field): return ismethod(field) or isfunction(field)\n",
        "\n",
        "class GhostSet:\n",
        "    \"\"\" enhanced interface (ghost) to retrieve class fields \"\"\"\n",
        "    def _meta(data): return {k:v for k,v in data.__dict__.items() if not isfx(v)}\n",
        "    def _at_last(_, sets): pass\n",
        "    def _set(object, **sets):\n",
        "        ''' use to fast initialize fields | needed to avoid initialization problems at copy by value '''\n",
        "        for field in sets: setattr(object, field, sets[field])\n",
        "        object._at_last(sets)\n",
        "GSet = GhostSet\n",
        "\n",
        "def meta(object):\n",
        "    ''' retrieves clonable object metadata (__dict__) as a copy '''\n",
        "    if isinstance(object, GSet): return object._meta()\n",
        "    return {}\n",
        "\n",
        "class ClonableObjectGhost:\n",
        "    \"\"\" enhanced interface (ghost) for clonable objects \"\"\"\n",
        "    def _by_val(_, depth=-1, _layer=0): pass\n",
        "GCo = ClonableObjectGhost\n",
        "\n",
        "class ClonableObject(GSet, GCo):\n",
        "    \"\"\" base clonable object \"\"\"\n",
        "    def __init__(this, **data): this._set(**data)\n",
        "    def __call__(_, **options): _._set(**options)\n",
        "    def _by_val(_, depth=-1, _layer=0):\n",
        "        copy = type(_)()\n",
        "        copy._set(**_._meta())\n",
        "        if depth<0 or depth>_layer:\n",
        "            for field in copy.__dict__:\n",
        "                if isinstance(copy.__dict__[field], ClonableObjectGhost):\n",
        "                    copy.__dict__[field] = copy.__dict__[field]._by_val(depth,_layer+1)\n",
        "        return copy\n",
        "COb = ClonableObject\n",
        "\n",
        "def copy_by_val(object, depth=-1, _layer=0):\n",
        "    if isinstance(object, GCo): return object._by_val(depth,_layer)\n",
        "    return object\n",
        "copy = by_val = vof = copy_by_val\n",
        "\n",
        "class ComparableGhost:\n",
        "    \"\"\" enhanced interface (ghost) for comparing instances \"\"\"\n",
        "    def _compare(a, b):\n",
        "        if type(a) != type(b): return False\n",
        "        if a.__dict__ == b.__dict__: return True\n",
        "        return False\n",
        "    def __eq__(a, b): return a._compare(b)\n",
        "GEq = ComparableGhost\n",
        "\n",
        "class IterableObjectGhost(GSet):\n",
        "    \"\"\" enhanced interface (ghost) for iterables: exposes __dict__,\n",
        "        therefore Iterable Objects are like lua dictionaries \"\"\"\n",
        "    def __contains__(this, key): return key in this.__dict__\n",
        "    def __iter__(this): return iter(this.__dict__)\n",
        "    def items(my): return my.__dict__.items()\n",
        "    def __getitem__(by, field): return by.__dict__[field]\n",
        "    def __setitem__(by, field, value): by.__dict__[field] = value\n",
        "    def pop(by, field): return by.__dict__.pop(field)\n",
        "GIo = IterableObjectGhost\n",
        "\n",
        "class ReprGhost:\n",
        "    \"\"\" enhanced interface (ghost) for the skeleton method _repr,\n",
        "        see implementation of Struct for a working example;\n",
        "        Record __repr__ override uses _lines_ for max lines display \"\"\"\n",
        "    _lines_ = 31\n",
        "    _chars_ = 13\n",
        "    _msgsz_ = 62\n",
        "    _ellipsis_ = ' ... '\n",
        "    def _repr(my, value):\n",
        "        _type = ''.join(''.join(str(type(value)).split('class ')).split(\"'\"))\n",
        "        _value = '{}'.format(value)\n",
        "        if len(_value)>my._chars_:\n",
        "            show = int(my._chars_/2)\n",
        "            _value = _value[:show]+my._ellipsis_+_value[-show:]\n",
        "        return '{} {}'.format(_type, _value)\n",
        "    def _resize(this, message, at=.7):\n",
        "        if len(message)>this._msgsz_:\n",
        "            start = int(at*this._msgsz_)\n",
        "            end = this._msgsz_-start\n",
        "            return message[:start]+this._ellipsis_+message[-end:]\n",
        "        return message\n",
        "GRe = ReprGhost\n",
        "\n",
        "def set_repr_to(lines): GRe._lines_ = lines\n",
        "\n",
        "class Struct(COb, GEq, GIo, GRe):\n",
        "    \"\"\" structured autoprintable object, behaves like a lua dictionary \"\"\"\n",
        "    def __repr__(_):\n",
        "        return '\\n'.join(['{}:\\t{}'.format(k, _._repr(v)) for k,v in _.items()])\n",
        "struct = Struct\n",
        "\n",
        "class RecordableGhost:\n",
        "    \"\"\" enhanced interface (ghost) for type recording,\n",
        "        see Record for a working example \"\"\"\n",
        "    @staticmethod\n",
        "    def load(filename):\n",
        "        with open(filename, 'rb') as file: return pickle.load(file)\n",
        "    def save(data, filename):\n",
        "        with open(filename, 'wb') as file: pickle.dump(data, file)\n",
        "\n",
        "GRec = RecordableGhost\n",
        "\n",
        "class Record(GSet, GCo, GRec, GEq, GRe):\n",
        "    \"\"\" wrapper for any object or value, auto-inspects and provides load/save type structure \"\"\"\n",
        "    data = None\n",
        "    _check = dict(\n",
        "            isfunction=isfunction, ismethod=ismethod, isgeneratorfunction=isgeneratorfunction, isgenerator=isgenerator, isroutine=isroutine,\n",
        "            isabstract=isabstract, isclass=isclass, ismodule=ismodule, istraceback=istraceback, isframe=isframe, iscode=iscode, isbuiltin=isbuiltin,\n",
        "            ismethoddescriptor=ismethoddescriptor, isdatadescriptor=isdatadescriptor, isgetsetdescriptor=isgetsetdescriptor, ismemberdescriptor=ismemberdescriptor,\n",
        "            isawaitable=isawaitable, iscoroutinefunction=iscoroutinefunction, iscoroutine=iscoroutine\n",
        "                   )\n",
        "    def __init__(this, token, **meta):\n",
        "        this.data = token\n",
        "        this.__dict__.update({k:v(token) for k,v in this._check.items()})\n",
        "        super()._set(**meta)\n",
        "    @property\n",
        "    def type(_): return type(_.data)\n",
        "    def inherits(_, *types): return issubclass(_.type, types)\n",
        "    @property\n",
        "    def isbaseiterable(_): return _.inherits(tuple, list, dict, set) or _.isgenerator or _.isgeneratorfunction\n",
        "    @property\n",
        "    def isiterable(_): return isinstance(_.data, iterable) and _.type is not str\n",
        "    def _clone_iterable(_):\n",
        "        if _.inherits(dict): return _.data.copy()\n",
        "        elif _.isgenerator or _.isgeneratorfunction: return (i for i in list(_.data))\n",
        "        else: return type(_.data)(list(_.data)[:])\n",
        "    def _meta(data): return {k:v for k,v in data.__dict__.items() if k != 'data' and not isfx(v)}\n",
        "    def _by_val(_, depth=-1, layer=0):\n",
        "        data = _.data\n",
        "        if _.isiterable: data = _._clone_iterable()\n",
        "        elif _.inherits(ClonableObjectGhost): data = by_val(data, depth, layer)\n",
        "        return type(_)(data, **meta(_))\n",
        "    def __enter__(self): self._instance = self; return self\n",
        "    def __exit__(self, type, value, traceback): self._instance = None\n",
        "    def __repr__(self):\n",
        "        if not hasattr(self, '_preprint'): return Record(self.data, _preprint='', _lines=Record(Record._lines_)).__repr__()\n",
        "        if self.isbaseiterable:\n",
        "            pre, repr = self._preprint, ''\n",
        "            for n,i in enumerate(self.data):\n",
        "                if self._lines.data == 0: break\n",
        "                else: self._lines.data -= 1\n",
        "                index, item = str(n), i\n",
        "                if self.inherits(dict): index += ' ({})'.format(str(i)); item = self.data[i]\n",
        "                repr += pre+'{}: '.format(index)\n",
        "                next = Record(item, _preprint=pre+'\\t', _lines=self._lines)\n",
        "                if next.isiterable: repr += '\\n'\n",
        "                repr += next.__repr__()\n",
        "                repr += '\\n'\n",
        "            return repr\n",
        "        elif self.inherits(GCo): return Record(self.data._meta(), _preprint=self._preprint, _lines=self._lines).__repr__()\n",
        "        else: return self._repr(self.data)\n",
        "REc = Record\n",
        "\n",
        "class Bisect(list, COb):\n",
        "    \"\"\" bisect implementation using clonable objects \"\"\"\n",
        "    def __init__(set, *items, key=None, reverse=False):\n",
        "        if not key: key = lambda  x:x\n",
        "        super().__init__(sorted(items, reverse=reverse, key=key))\n",
        "    def _bisect(set, item, key, reverse, bottom, top):\n",
        "        def _(check):\n",
        "            if key: return key(check)\n",
        "            return check\n",
        "        at = int((top-bottom)/2)+bottom\n",
        "        if len(set)==0: return (0,-1)\n",
        "        if item==_(set[at]): return (at,0)\n",
        "        bigger = item<_(set[at])\n",
        "        if bigger != reverse:\n",
        "            if at-bottom>0: return set._bisect(item, key, reverse, bottom, at)\n",
        "            return (at,-1)\n",
        "        elif top-at>1: return set._bisect(item, key, reverse, at, top)\n",
        "        return (at,1)\n",
        "    def search(_, item, key=None, reverse=False):\n",
        "        if not key: key = lambda x:x\n",
        "        return _._bisect(item, key, reverse, 0, len(_))\n",
        "    def _by_val(_, depth=-1, _layer=0):\n",
        "        copy = super()._by_val(depth, _layer)\n",
        "        copy += _[:]\n",
        "        return copy\n",
        "BSx = Bisect"
      ],
      "metadata": {
        "id": "HPtrvpROPZxi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "I1jZ1qaIBZXk"
      },
      "outputs": [],
      "source": [
        "def get_non_operated(main_folder):\n",
        "    try:\n",
        "        # Read the Excel file\n",
        "        df = pd.read_excel(main_folder + 'metadata.xlsx')\n",
        "        # Filter rows where OUTCOME is -1\n",
        "        outcome_minus_one = df[df['OUTCOME'] == -1]\n",
        "        # Get the SCC_ID values from the filtered rows\n",
        "        nonop = outcome_minus_one['SCC_ID'].tolist()\n",
        "        print(nonop)\n",
        "        return nonop\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def get_outcome_for_subject(sub, main_folder):\n",
        "    try:\n",
        "        df = pd.read_excel(main_folder + 'metadata.xlsx')\n",
        "        # Find the row corresponding to the subject ID\n",
        "        row = df[df['SCC_ID'] == sub]\n",
        "        # Get the outcome from the row\n",
        "        outcome = row['OUTCOME'].values[0] if not row.empty else None\n",
        "        return outcome\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_soz_labels(sub, main_folder):\n",
        "    try:\n",
        "        df = pd.read_excel(main_folder + \"metadata.xlsx\")\n",
        "\n",
        "        # Find the row corresponding to the given sub\n",
        "        row = df[df['SCC_ID'] == sub]\n",
        "        # Extract the SOZ labels from the row\n",
        "        soz_labels = row['SOZ'].values[0]\n",
        "        # Split the string by comma and remove spaces\n",
        "        soz_labels = soz_labels.replace(' ', '').split(',')\n",
        "        # Convert all letters to uppercase\n",
        "        soz_labels = [label.upper() for label in soz_labels]\n",
        "        return soz_labels\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return None\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEaJuX2-BZXl"
      },
      "source": [
        "Helper functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "pvT1zlgkBZXl"
      },
      "outputs": [],
      "source": [
        "def MM(x):\n",
        "    return (np.min(x)+np.max(x))/np.mean(x)\n",
        "\n",
        "\n",
        "def plot_scores_hist(data):\n",
        "    # Plot a histogram of the 'Score' column\n",
        "    plt.hist(data['Score'], bins=20, edgecolor='grey', color='lightgrey')\n",
        "    plt.xlabel('Score')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Histogram of Mean CVS for All Groups')\n",
        "\n",
        "    # Calculate mean, median, and standard deviation\n",
        "    mean_score = data['Score'].mean()\n",
        "    std_score = data['Score'].std()\n",
        "\n",
        "    # Plot vertical lines for mean, median, 1 std, 2 std, and 3 std\n",
        "    plt.axvline(mean_score, color='r', linestyle='dashed', linewidth=2, label='Mean')\n",
        "    plt.axvline(mean_score + std_score, color='b', linestyle='dashed', linewidth=2, label='1 Std Dev')\n",
        "    plt.axvline(mean_score + 2*std_score, color='y', linestyle='dashed', linewidth=2, label='2 Std Dev')\n",
        "    plt.axvline(mean_score + 3*std_score, color='c', linestyle='dashed', linewidth=2, label='3 Std Dev')\n",
        "    plt.axvline(mean_score + 4*std_score, color='m', linestyle='dashed', linewidth=2, label='4 Std Dev')\n",
        "\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_hist_en(node_mean_scores, resection_scores):\n",
        "    # Create a histogram of all scores\n",
        "    plt.hist(node_mean_scores['Score'], bins=10, edgecolor='black', alpha=0.7, label='All Scores')\n",
        "\n",
        "    # Add vertical lines to mark the scores of resection nodes\n",
        "    for score in resection_scores:\n",
        "        plt.axvline(score, color='red', linestyle='dashed', linewidth=2)\n",
        "\n",
        "    plt.xlabel('Scores')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Histogram of Scores with Resection Scores Marked')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Function to check for overlap\n",
        "def compute_overlap(group1, group2):\n",
        "    return list(set(group1).intersection(group2))\n",
        "\n",
        "# Create a function to compute the percentage of overlap\n",
        "def compute_overlap_percentage(group, resection_nodes):\n",
        "    overlap_count = sum(1 for node in group if node in resection_nodes)\n",
        "    overlap_percentage = (overlap_count / len(group))\n",
        "    return overlap_percentage\n",
        "\n",
        "def jaccard_index(group, resection):\n",
        "    \"\"\"\n",
        "    Calculate the Jaccard index for the overlap between a group and a resection.\n",
        "\n",
        "    Args:\n",
        "    group (set or list): A set or list of nodes in the group.\n",
        "    resection (set or list): A set or list of nodes in the resection.\n",
        "\n",
        "    Returns:\n",
        "    float: The Jaccard index, a value between 0 and 1.\n",
        "    \"\"\"\n",
        "    # Convert input to sets for efficient set operations\n",
        "    group_set = set(group)\n",
        "    resection_set = set(resection)\n",
        "\n",
        "    # Calculate the size of the intersection and union of the sets\n",
        "    intersection_size = len(group_set.intersection(resection_set))\n",
        "    union_size = len(group_set.union(resection_set))\n",
        "\n",
        "    # Calculate the Jaccard index\n",
        "    if union_size == 0:\n",
        "        return 0.0  # Handle the case when the union is empty\n",
        "    else:\n",
        "        return intersection_size / union_size\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Dictionary for band symbol substitutions\n",
        "band_subs = {\n",
        "    \"0,4\": \"δ\",\n",
        "    \"4,8\": \"ϴ\",\n",
        "    \"8,13\": \"α\",\n",
        "    \"13,30\": \"β\",\n",
        "    \"30,70\": \"low γ\",\n",
        "    \"70,150\": \"high γ\"\n",
        "}\n",
        "\n",
        "# Dictionary for connectivity method substitutions\n",
        "cm_subs = {\n",
        "    \"SCR-\": \"Real spectral coherence\",\n",
        "    \"SCI-\": \"Imaginary spectral coherence\",\n",
        "    \"PLV-\": \"Phase-locking value\",\n",
        "    \"PLI-\": \"Phase lag index\",\n",
        "    \"CC-\": \"Cross-correlation\",\n",
        "    \"PAC-\": \"PAC\",\n",
        "    \"PEC-\": \"PEC\"\n",
        "}\n",
        "\n",
        "# Function to replace bands with their corresponding symbols\n",
        "def substitute_bands(s):\n",
        "    # Find all occurrences of band ranges in the format \"(a,b)\"\n",
        "    bands = re.findall(r'\\(\\d+,\\d+\\)',s)\n",
        "\n",
        "    # Substitute each band range with its corresponding symbol\n",
        "    for band in bands:\n",
        "        band_range = band[1:-1]  # Remove parentheses\n",
        "        if band_range in band_subs:\n",
        "            s = s.replace('-','').replace(band, f\" ({band_subs[band_range]})\")\n",
        "\n",
        "    return s\n",
        "\n",
        "# Function to replace connectivity methods with their corresponding names\n",
        "def substitute_cm(s):\n",
        "    # Find all occurrences of connectivity method prefixes\n",
        "    for cm in cm_subs:\n",
        "        s = re.sub(r'\\b' + cm + r'\\b', cm_subs[cm], s)\n",
        "\n",
        "    return s\n",
        "\n",
        "# Test\n",
        "input_str = \"CC-(30,70), SCR-(70,150)\"\n",
        "output_str = substitute_cm(input_str)\n",
        "output_str = substitute_bands(output_str)\n",
        "\n",
        "print(output_str)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvDOUNPtiRJ0",
        "outputId": "6f6e058c-2cc2-4ef0-bbfe-e5f74d557c11"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CC (low γ), SCR (high γ)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TClrVc52BZXm"
      },
      "source": [
        "44-EpiGAME"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "connectivity_measures = ['PLI-(4,8)', 'PLI-(70,150)']"
      ],
      "metadata": {
        "id": "Bw6BmPFzVqaf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5Gj0_LiBZXm"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from scipy import stats\n",
        "\n",
        "# Set a specific random seed (e.g., 42 for reproducibility)\n",
        "random_seed = 42\n",
        "np.random.seed(random_seed)\n",
        "\n",
        "subs = list(range(1,28))\n",
        "\n",
        "for sub in subs:\n",
        "\n",
        "  if sub not in [str(id) for id in get_non_operated(main_folder)]:\n",
        "\n",
        "    surgical_outcome = get_outcome_for_subject(int(sub), main_folder)\n",
        "    print(surgical_outcome)\n",
        "\n",
        "    # we need resection_nodes and group_size to check the winners\n",
        "    all_nodes = REc.load(main_folder + f\"result/{sub}-PAC.res\").data.data.nodes\n",
        "\n",
        "    resection_nodes = get_soz_labels(int(sub), main_folder)\n",
        "\n",
        "    ratio_resected = len(resection_nodes) / len(all_nodes) # for later normalization\n",
        "    print(f\"Ratio of resected nodes = {ratio_resected}\")\n",
        "\n",
        "    df_sub = df[df['Subject'] == sub]\n",
        "\n",
        "    for measure in connectivity_measures:\n",
        "      df_sub_cm = df_sub[df_sub['CM'] == measure]\n",
        "\n",
        "      r = 0.1\n",
        "      group_size = int(len(all_nodes)*r)\n",
        "      print(\"Random group size =\", group_size)\n",
        "\n",
        "      groups, mm_cvs = [],[]\n",
        "\n",
        "      for i in range(len(all_nodes)*5):\n",
        "\n",
        "          # random groups are created in each iteration\n",
        "          group_labels = sorted(list(np.random.choice(all_nodes, size=group_size, replace=False)))\n",
        "\n",
        "          if group_labels not in groups:\n",
        "\n",
        "            # Split the Labels column into two nodes using '<->'\n",
        "            split_labels = df_sub_cm['Labels'].str.split('<->', expand=True)\n",
        "\n",
        "            # Check if both nodes are in group_labels\n",
        "            relevant_rows = df_sub_cm[((split_labels[0].isin(group_labels)) & split_labels[1].isin(group_labels))]\n",
        "            # Calculate (max(CVS)+min(CVS)) / mean(CVS) scores (MM scores)\n",
        "            mm_cvs.append(list(relevant_rows['CVS'].apply(lambda x: MM(list(map(float, x.strip('[]').split()))))))\n",
        "            groups.append(group_labels)\n",
        "\n",
        "      data = {\n",
        "          'Group': groups,\n",
        "          'Score': [np.median(x) for x in mm_cvs]} # median MM score for a random group\n",
        "\n",
        "      data = pd.DataFrame(data)\n",
        "\n",
        "      # Count rows with NaN values\n",
        "      nan_rows_count = data.isna().sum().sum()\n",
        "      print(\"Number of Rows with NaN Values:\", nan_rows_count)\n",
        "      # Drop NaN rows\n",
        "      data = data.dropna()\n",
        "\n",
        "      plot_scores_hist(data)\n",
        "\n",
        "      # # Find the common nodes (overlap)\n",
        "      # data['Overlap_with_Resection'] = data['Group'].apply(lambda row: compute_overlap(row, resection_nodes))\n",
        "\n",
        "      # # Apply the function to each row\n",
        "      # data['Overlap_Percentage'] = data['Group'].apply(lambda row: compute_overlap_percentage(row, resection_nodes))\n",
        "\n",
        "      # Define score ranges relative to standard deviations of MM score distribution\n",
        "      # Calculate the median and standard deviation of MM score\n",
        "      median_score = data['Score'].median()\n",
        "      std_dev_score = data['Score'].std()\n",
        "\n",
        "      # Define the number of standard deviations to consider\n",
        "      num_std_devs = 7\n",
        "      # Create score ranges relative to median and standard deviation\n",
        "      score_ranges = []\n",
        "      min_std, max_std = num_std_devs, -num_std_devs\n",
        "      # Create an array of x-values with an offset\n",
        "      x_values = []\n",
        "\n",
        "      for i in range(-num_std_devs*2, num_std_devs*2, 1):\n",
        "          range_start = median_score + i * std_dev_score /2\n",
        "          range_end = median_score + (i + 1) * std_dev_score /2\n",
        "          # Check if there are score values in the range, and only add the range if there are values\n",
        "          if any((range_start <= data['Score']) & (data['Score'] < range_end)):\n",
        "              score_ranges.append((range_start, range_end))\n",
        "              min_std = min(min_std, i)\n",
        "              max_std = max(max_std, i)\n",
        "              x_values.append(i+0.5)\n",
        "\n",
        "      # Initialize lists\n",
        "      overlap_percentages = []\n",
        "      best_score_range = None\n",
        "      best_overlap_percentage = 0.0\n",
        "      all_group_overlaps = []\n",
        "\n",
        "      for score_range in score_ranges:\n",
        "\n",
        "          group_overlaps = []\n",
        "          lower_bound = score_range[0]\n",
        "          upper_bound = score_range[1]\n",
        "\n",
        "          # Filter groups within the current score range\n",
        "          filtered_groups = data[(data['Score'] >= lower_bound) & (data['Score'] < upper_bound)]\n",
        "\n",
        "          # Calculate overlap scores and create a new \"Group_Overlap\" column\n",
        "          data['Group_Overlap'] = 0  # Initialize the column with zeros\n",
        "\n",
        "          # Compute the overlap between the filtered groups\n",
        "          group_overlap_scores = []\n",
        "\n",
        "          if len(filtered_groups) == 1:\n",
        "\n",
        "            resection_overlap = compute_overlap_percentage(filtered_groups.iloc[0]['Group'], resection_nodes)\n",
        "            group_overlaps.append(resection_overlap)\n",
        "\n",
        "          else:\n",
        "\n",
        "            for i, group1 in filtered_groups.iterrows():\n",
        "                for j, group2 in filtered_groups.iterrows():\n",
        "\n",
        "                    if group1['Group'] != group2['Group']:\n",
        "                        # # 1) Overlap between groups' intersection and the resection or Jaccard Index\n",
        "                        group_intersection = compute_overlap(group1['Group'], group2['Group'])\n",
        "                        resection_overlap = compute_overlap_percentage(group_intersection, resection_nodes) if group_intersection else 0\n",
        "                        group_overlap_scores.append(resection_overlap)\n",
        "\n",
        "                group_overlaps += [np.mean(group_overlap_scores)] # save the mean overlap of group1\n",
        "\n",
        "          all_group_overlaps.append(group_overlaps)\n",
        "\n",
        "      # Check if all lists contain only zeros\n",
        "      all_zeros = [all(x == 0 for x in sublist) for sublist in all_group_overlaps]\n",
        "      # Check if all elements in 'all_zeros' list are True\n",
        "      if all(all_zeros): print(\"All lists contain only zeros\")\n",
        "\n",
        "      else:\n",
        "        # Plot overlap percentages across score ranges\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.boxplot(all_group_overlaps, labels=x_values, positions=x_values, showfliers=True)\n",
        "\n",
        "        # Add the scatter plot\n",
        "        for i, x in enumerate(x_values):\n",
        "            y = all_group_overlaps[i]\n",
        "            x_coords = np.random.normal(x, 0.04, size=len(y))  # Set x-coordinates to position on the x-axis\n",
        "            plt.scatter(x_coords, y, marker='o', alpha=0.5, label=x)\n",
        "            # Annotate the number of samples next to each boxplot\n",
        "            plt.text(x, plt.ylim()[1], f'{len(y)}', ha='center', va='bottom', size='large')\n",
        "\n",
        "        plt.ylabel('Resection Overlap', size=20)\n",
        "        plt.xlabel(f'ΔC Score Distribution Ranges: {substitute_cm(substitute_bands(measure))}', size=20)\n",
        "\n",
        "        # Set custom x-axis ticks at points between the ranges\n",
        "        plt.xticks(x_values)\n",
        "        # Add vertical lines for median and each standard deviation\n",
        "        borders = x_values+[x_values[-1]+1]\n",
        "        for k in [int(x-0.5) for x in borders]:\n",
        "            label = 'M' if k == 0 else f'{k/2}σ'\n",
        "            plt.axvline(x=k, color='gray', linestyle='--', linewidth=0.5)\n",
        "            plt.text(k, plt.ylim()[0], label, horizontalalignment='left', size='large')\n",
        "        plt.yticks(fontsize=14)\n",
        "        plt.xticks([])  # Remove x-axis ticks\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aY7fLPEXeQ-U"
      },
      "execution_count": 7,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "xnn",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}