{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ivkarla/epigame/blob/main/notebooks/node1-0_connectivity_change_CM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook extracts the connectivity values from each epoch in sequence. For each node pair, the connectivity value series is analyzed using the measures of randomness of a time series.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrHPjp0JTxEv",
        "outputId": "37f4d20d-b8ac-4808-c944-eed2c8398503"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jkUyC9LUVrq",
        "outputId": "90faf39e-3c76-4e39-b92d-4e7c04d6cc45"
      },
      "outputs": [],
      "source": [
        "from inspect import ismethod\n",
        "from datetime import timedelta as _time\n",
        "from datetime import datetime\n",
        "from collections.abc import Iterable as iterable\n",
        "\n",
        "def some(field): return (field != None and field != [] and field != {} and field != ()) or field == True\n",
        "def no(field): return not some(field) or field==False or field==''\n",
        "\n",
        "class class_of:\n",
        "    _instance = None\n",
        "    def __init__(_, object):\n",
        "        _._is = type(object)\n",
        "    def inherits(_, *types):\n",
        "        return issubclass(_._is, types)\n",
        "    def has(_, *types): return _.inherits(*types)\n",
        "    def __enter__(self):\n",
        "        self._instance = self\n",
        "        return self\n",
        "    def __exit__(self, type, value, traceback): self._instance = None\n",
        "    @staticmethod\n",
        "    def each_in(list):\n",
        "        if isiterable(list):\n",
        "            return [type(item) for item in list]\n",
        "\n",
        "class struct:\n",
        "    def __init__(table, **sets): table.__dict__.update(sets)\n",
        "    @property\n",
        "    def sets(this): return set(dir(this)) - set(dir(type(this)))\n",
        "    def set(object, **fields):\n",
        "        for field in fields: setattr(object, field, fields[field])\n",
        "    def get(object, *fields): return [getattr(object, field) for field in fields if field in object.__dict__]\n",
        "    def _clonable(set, mask=None):\n",
        "        check = set.__dict__.copy()\n",
        "        clonable = check.copy()\n",
        "        if some(mask): pass\n",
        "#            for field in check:\n",
        "#                if sum([int(_(check[field])) for _ in mask])+sum([int(_(field)) for _ in mask]): clonable.pop(field)\n",
        "        return clonable\n",
        "    @staticmethod\n",
        "    def _from(type):\n",
        "        if hasattr(type, '__dict__'): return struct(**type.__dict__.copy())\n",
        "        return struct()\n",
        "\n",
        "def meta(data, *mask): return struct._from(data)._clonable(mask)\n",
        "def get(data, *fields):\n",
        "    if not issubclass(type(data), dict): data=struct._from(data)._clonable()\n",
        "    return struct(**data).get(*fields)\n",
        "\n",
        "class table(struct):\n",
        "    def _default(field, name, value):\n",
        "        try: return getattr(field, name)\n",
        "        except: setattr(field, name, value)\n",
        "        return value\n",
        "    def clear(this, *fields):\n",
        "        sets = this.sets\n",
        "        if not fields: fields = sets\n",
        "        if fields:\n",
        "            set = [field for field in fields if hasattr(this,field) and not ismethod(getattr(this, field))]\n",
        "            for field in set: delattr(this, field)\n",
        "    def has(this, *fields):\n",
        "        return all([hasattr(this, field) for field in fields])\n",
        "    def has_not(this, *fields): return not this.has(*fields)\n",
        "    def check(this, **KV):\n",
        "        try: check = [KV[key]==this.__dict__[key] for key in KV]\n",
        "        except: return False\n",
        "        return all(check)\n",
        "    def find(this, _type):\n",
        "        return [value for value in this.sets if class_of(get(this,value)[0]).inherits(_type)]\n",
        "    def clone(this):\n",
        "        clone = type(this)()\n",
        "        sets = this._clonable()\n",
        "        clone.set(**sets)\n",
        "        return clone\n",
        "\n",
        "def isiterable(this): return isinstance(this, iterable) and type(this) is not str\n",
        "def default(field, name, value): return table(**field)._default(name, value)\n",
        "\n",
        "def ni(list):\n",
        "    if isiterable(list):\n",
        "        for n,i in enumerate(list): yield n,i\n",
        "    else:\n",
        "        for n,i in enumerate(list.__dict__.keys()): yield n,i\n",
        "\n",
        "class at(table):\n",
        "    DAY, HOUR, MIN = 86400, 3600, 60\n",
        "    def __init__(_, dtime=None, **sets):\n",
        "        _.set(**sets)\n",
        "        if some(dtime) and issubclass(type(dtime), _time): _._time = dtime\n",
        "        else:\n",
        "            d,h,m,s,ms = _._default('d',0), _._default('h',0), _._default('m',0), _._default('s',0), _._default('ms',0)\n",
        "            if not any([d,h,m,s,ms]): now=datetime.now(); _._time = now-datetime(now.year, now.month, now.day)\n",
        "            else: _._time = _time(days=d, hours=h, minutes=m, seconds=s, milliseconds=ms)\n",
        "        _.clear('d','h','m','s','ms')\n",
        "    def __sub__(_, dtime):\n",
        "        of=type(dtime); sets=_._clonable()\n",
        "        if issubclass(of, _time): return at(_._time-dtime, **sets)\n",
        "        elif issubclass(of, at): sets.update(dtime._clonable()); return at(_._time-dtime._time, **sets)\n",
        "    def __add__(_, dtime):\n",
        "        of=type(dtime); sets=_._clonable()\n",
        "        if issubclass(of, _time): return at(_._time+dtime, **sets)\n",
        "        elif issubclass(of, at): sets.update(dtime._clonable()); return at(_._time+dtime._time, **sets)\n",
        "    def __str__(_): return str(_._time)\n",
        "    @property\n",
        "    def seconds(_): return _._time.seconds\n",
        "    @property\n",
        "    def S(_): return _.seconds\n",
        "    @property\n",
        "    def minutes(_): return _._time.seconds/60\n",
        "    @property\n",
        "    def M(_): return _.minutes\n",
        "    @property\n",
        "    def hours(_): return _.minutes/60\n",
        "    @property\n",
        "    def H(_): return _.hours\n",
        "    @property\n",
        "    def days(_): return _._time.days\n",
        "    @property\n",
        "    def D(_): return _.days\n",
        "    @staticmethod\n",
        "    def zero(): return at(_time())\n",
        "\n",
        "from inspect import isfunction, ismethod, isgeneratorfunction, isgenerator, isroutine\n",
        "from inspect import isabstract, isclass, ismodule, istraceback, isframe, iscode, isbuiltin\n",
        "from inspect import ismethoddescriptor, isdatadescriptor, isgetsetdescriptor, ismemberdescriptor\n",
        "from inspect import isawaitable, iscoroutinefunction, iscoroutine\n",
        "\n",
        "from collections.abc import Iterable as iterable\n",
        "\n",
        "from pickle import load, dump\n",
        "\n",
        "def isfx(field): return ismethod(field) or isfunction(field)\n",
        "\n",
        "class GhostSet:\n",
        "    \"\"\" enhanced interface (ghost) to retrieve class fields \"\"\"\n",
        "    def _meta(data): return {k:v for k,v in data.__dict__.items() if not isfx(v)}\n",
        "    def _at_last(_, sets): pass\n",
        "    def _set(object, **sets):\n",
        "        ''' use to fast initialize fields | needed to avoid initialization problems at copy by value '''\n",
        "        for field in sets: setattr(object, field, sets[field])\n",
        "        object._at_last(sets)\n",
        "GSet = GhostSet\n",
        "\n",
        "def meta(object):\n",
        "    ''' retrieves clonable object metadata (__dict__) as a copy '''\n",
        "    if isinstance(object, GSet): return object._meta()\n",
        "    return {}\n",
        "\n",
        "class ClonableObjectGhost:\n",
        "    \"\"\" enhanced interface (ghost) for clonable objects \"\"\"\n",
        "    def _by_val(_, depth=-1, _layer=0): pass\n",
        "GCo = ClonableObjectGhost\n",
        "\n",
        "class ClonableObject(GSet, GCo):\n",
        "    \"\"\" base clonable object \"\"\"\n",
        "    def __init__(this, **data): this._set(**data)\n",
        "    def __call__(_, **options): _._set(**options)\n",
        "    def _by_val(_, depth=-1, _layer=0):\n",
        "        copy = type(_)()\n",
        "        copy._set(**_._meta())\n",
        "        if depth<0 or depth>_layer:\n",
        "            for field in copy.__dict__:\n",
        "                if isinstance(copy.__dict__[field], ClonableObjectGhost):\n",
        "                    copy.__dict__[field] = copy.__dict__[field]._by_val(depth,_layer+1)\n",
        "        return copy\n",
        "COb = ClonableObject\n",
        "\n",
        "def copy_by_val(object, depth=-1, _layer=0):\n",
        "    if isinstance(object, GCo): return object._by_val(depth,_layer)\n",
        "    return object\n",
        "copy = by_val = vof = copy_by_val\n",
        "\n",
        "class ComparableGhost:\n",
        "    \"\"\" enhanced interface (ghost) for comparing instances \"\"\"\n",
        "    def _compare(a, b):\n",
        "        if type(a) != type(b): return False\n",
        "        if a.__dict__ == b.__dict__: return True\n",
        "        return False\n",
        "    def __eq__(a, b): return a._compare(b)\n",
        "GEq = ComparableGhost\n",
        "\n",
        "class IterableObjectGhost(GSet):\n",
        "    \"\"\" enhanced interface (ghost) for iterables: exposes __dict__,\n",
        "        therefore Iterable Objects are like lua dictionaries \"\"\"\n",
        "    def __contains__(this, key): return key in this.__dict__\n",
        "    def __iter__(this): return iter(this.__dict__)\n",
        "    def items(my): return my.__dict__.items()\n",
        "    def __getitem__(by, field): return by.__dict__[field]\n",
        "    def __setitem__(by, field, value): by.__dict__[field] = value\n",
        "    def pop(by, field): return by.__dict__.pop(field)\n",
        "GIo = IterableObjectGhost\n",
        "\n",
        "class ReprGhost:\n",
        "    \"\"\" enhanced interface (ghost) for the skeleton method _repr,\n",
        "        see implementation of Struct for a working example;\n",
        "        Record __repr__ override uses _lines_ for max lines display \"\"\"\n",
        "    _lines_ = 31\n",
        "    _chars_ = 13\n",
        "    _msgsz_ = 62\n",
        "    _ellipsis_ = ' ... '\n",
        "    def _repr(my, value):\n",
        "        _type = ''.join(''.join(str(type(value)).split('class ')).split(\"'\"))\n",
        "        _value = '{}'.format(value)\n",
        "        if len(_value)>my._chars_:\n",
        "            show = int(my._chars_/2)\n",
        "            _value = _value[:show]+my._ellipsis_+_value[-show:]\n",
        "        return '{} {}'.format(_type, _value)\n",
        "    def _resize(this, message, at=.7):\n",
        "        if len(message)>this._msgsz_:\n",
        "            start = int(at*this._msgsz_)\n",
        "            end = this._msgsz_-start\n",
        "            return message[:start]+this._ellipsis_+message[-end:]\n",
        "        return message\n",
        "GRe = ReprGhost\n",
        "\n",
        "def set_repr_to(lines): GRe._lines_ = lines\n",
        "\n",
        "class Struct(COb, GEq, GIo, GRe):\n",
        "    \"\"\" structured autoprintable object, behaves like a lua dictionary \"\"\"\n",
        "    def __repr__(_):\n",
        "        return '\\n'.join(['{}:\\t{}'.format(k, _._repr(v)) for k,v in _.items()])\n",
        "struct = Struct\n",
        "\n",
        "class RecordableGhost:\n",
        "    \"\"\" enhanced interface (ghost) for type recording,\n",
        "        see Record for a working example \"\"\"\n",
        "    @staticmethod\n",
        "    def load(filename):\n",
        "        with open(filename, 'rb') as file: return load(file)\n",
        "    def save(data, filename):\n",
        "        with open(filename, 'wb') as file: dump(data, file)\n",
        "\n",
        "GRec = RecordableGhost\n",
        "\n",
        "class Record(GSet, GCo, GRec, GEq, GRe):\n",
        "    \"\"\" wrapper for any object or value, auto-inspects and provides load/save type structure \"\"\"\n",
        "    data = None\n",
        "    _check = dict(\n",
        "            isfunction=isfunction, ismethod=ismethod, isgeneratorfunction=isgeneratorfunction, isgenerator=isgenerator, isroutine=isroutine,\n",
        "            isabstract=isabstract, isclass=isclass, ismodule=ismodule, istraceback=istraceback, isframe=isframe, iscode=iscode, isbuiltin=isbuiltin,\n",
        "            ismethoddescriptor=ismethoddescriptor, isdatadescriptor=isdatadescriptor, isgetsetdescriptor=isgetsetdescriptor, ismemberdescriptor=ismemberdescriptor,\n",
        "            isawaitable=isawaitable, iscoroutinefunction=iscoroutinefunction, iscoroutine=iscoroutine\n",
        "                   )\n",
        "    def __init__(this, token, **meta):\n",
        "        this.data = token\n",
        "        this.__dict__.update({k:v(token) for k,v in this._check.items()})\n",
        "        super()._set(**meta)\n",
        "    @property\n",
        "    def type(_): return type(_.data)\n",
        "    def inherits(_, *types): return issubclass(_.type, types)\n",
        "    @property\n",
        "    def isbaseiterable(_): return _.inherits(tuple, list, dict, set) or _.isgenerator or _.isgeneratorfunction\n",
        "    @property\n",
        "    def isiterable(_): return isinstance(_.data, iterable) and _.type != str\n",
        "    def _clone_iterable(_):\n",
        "        if _.inherits(dict): return _.data.copy()\n",
        "        elif _.isgenerator or _.isgeneratorfunction: return (i for i in list(_.data))\n",
        "        else: return type(_.data)(list(_.data)[:])\n",
        "    def _meta(data): return {k:v for k,v in data.__dict__.items() if k != 'data' and not isfx(v)}\n",
        "    def _by_val(_, depth=-1, layer=0):\n",
        "        data = _.data\n",
        "        if _.isiterable: data = _._clone_iterable()\n",
        "        elif _.inherits(ClonableObjectGhost): data = by_val(data, depth, layer)\n",
        "        return type(_)(data, **meta(_))\n",
        "    def __enter__(self): self._instance = self; return self\n",
        "    def __exit__(self, type, value, traceback): self._instance = None\n",
        "    def __repr__(self):\n",
        "        if not hasattr(self, '_preprint'): return Record(self.data, _preprint='', _lines=Record(Record._lines_)).__repr__()\n",
        "        if self.isbaseiterable:\n",
        "            pre, repr = self._preprint, ''\n",
        "            for n,i in enumerate(self.data):\n",
        "                if self._lines.data == 0: break\n",
        "                else: self._lines.data -= 1\n",
        "                index, item = str(n), i\n",
        "                if self.inherits(dict): index += ' ({})'.format(str(i)); item = self.data[i]\n",
        "                repr += pre+'{}: '.format(index)\n",
        "                next = Record(item, _preprint=pre+'\\t', _lines=self._lines)\n",
        "                if next.isiterable: repr += '\\n'\n",
        "                repr += next.__repr__()\n",
        "                repr += '\\n'\n",
        "            return repr\n",
        "        elif self.inherits(GCo): return Record(self.data._meta(), _preprint=self._preprint, _lines=self._lines).__repr__()\n",
        "        else: return self._repr(self.data)\n",
        "REc = Record\n",
        "\n",
        "class Bisect(list, COb):\n",
        "    \"\"\" bisect implementation using clonable objects \"\"\"\n",
        "    def __init__(set, *items, key=None, reverse=False):\n",
        "        if not key: key = lambda  x:x\n",
        "        super().__init__(sorted(items, reverse=reverse, key=key))\n",
        "    def _bisect(set, item, key, reverse, bottom, top):\n",
        "        def _(check):\n",
        "            if key: return key(check)\n",
        "            return check\n",
        "        at = int((top-bottom)/2)+bottom\n",
        "        if len(set)==0: return (0,-1)\n",
        "        if item==_(set[at]): return (at,0)\n",
        "        bigger = item<_(set[at])\n",
        "        if bigger != reverse:\n",
        "            if at-bottom>0: return set._bisect(item, key, reverse, bottom, at)\n",
        "            return (at,-1)\n",
        "        elif top-at>1: return set._bisect(item, key, reverse, at, top)\n",
        "        return (at,1)\n",
        "    def search(_, item, key=None, reverse=False):\n",
        "        if not key: key = lambda x:x\n",
        "        return _._bisect(item, key, reverse, 0, len(_))\n",
        "    def _by_val(_, depth=-1, _layer=0):\n",
        "        copy = super()._by_val(depth, _layer)\n",
        "        copy += _[:]\n",
        "        return copy\n",
        "BSx = Bisect\n",
        "\n",
        "from numpy import ndarray, resize, linspace, arange\n",
        "from numpy import min, max, average, floor\n",
        "from numpy import ubyte, zeros, array\n",
        "from scipy.signal import lfilter, butter\n",
        "from matplotlib import pylab as lab\n",
        "\n",
        "_NOTCH = _FR = 50\n",
        "_SAMPLING = 500\n",
        "_CONTINUOUS = 1\n",
        "_UNIT = 'ms'\n",
        "\n",
        "class rec(table, ndarray):\n",
        "    @property\n",
        "    def dimensions(of): return len(of.shape)\n",
        "    @property\n",
        "    def is_scalar(this): return this.shape == ()\n",
        "    @property\n",
        "    def is_vector(this): return len(this.shape)==1\n",
        "    @property\n",
        "    def is_matrix(this): return len(this.shape)>1\n",
        "    @property\n",
        "    def is_cube(this): return len(this.shape) == 3\n",
        "    @property\n",
        "    def is_binary(this): return this.dtype == ubyte and max(this) == 1\n",
        "    @property\n",
        "    def serialized(data):\n",
        "        if not data.is_scalar and data.dimensions>1:\n",
        "            return rec.read(data.T.flatten(), _deser=data.T.shape, **meta(data))\n",
        "        return data\n",
        "    @property\n",
        "    def deserialized(data):\n",
        "        if data.has('_deser'):\n",
        "            deser = rec.read(resize(data, data._deser).T, **meta(data))\n",
        "            deser.clear('_deser')\n",
        "            return deser\n",
        "        return data\n",
        "    @property\n",
        "    def as_matrix(data):    #implement numpy matrix\n",
        "        if data.is_vector: return rec.read([data], to=type(data), **meta(data))\n",
        "        return data\n",
        "    @property\n",
        "    def raw(data):\n",
        "        if data.shape[0] == 1: return rec.read(data[0], **meta(data)).raw\n",
        "        return data\n",
        "    def join(base, *parts, **sets):\n",
        "        flip, parts = None, list(parts)\n",
        "        if 'flip' in sets: flip=sets.pop('flip')\n",
        "        next = parts[0]\n",
        "        if len(parts)>1: next = rec.join(parts[0], parts[1:])\n",
        "        congruent = base.dimensions == next.dimensions and base.dimensions < 3\n",
        "        if congruent:\n",
        "            sets.update(base._clonable())\n",
        "            A, B = base, next\n",
        "            if flip: A, B = base.T, next.T\n",
        "            C = record(A.tolist()+B.tolist(), **sets)\n",
        "            if flip: return record(C.T, **sets)\n",
        "            return C\n",
        "    def get_as(this, data, cast=None):\n",
        "        source = this\n",
        "        if no(cast):\n",
        "            if issubclass(type(data), rec): cast = type(data)\n",
        "            else: cast = type(this)\n",
        "        if issubclass(type(data), ndarray): source = resize(this, data.shape)\n",
        "        return rec.read(source, to=cast, **meta(data))\n",
        "    @staticmethod\n",
        "    def read(iterable, to=None, **sets):\n",
        "        if no(to) or not issubclass(to, rec): to = rec\n",
        "        data = array(iterable).view(to)\n",
        "        data.set(**sets)\n",
        "        return data\n",
        "    def clone(this, **sets):\n",
        "        copy = this.copy().view(type(this))\n",
        "        sets.update(this._clonable())\n",
        "        copy.set(**sets)\n",
        "        return copy\n",
        "    def exclude(data, *items, **sets):\n",
        "        if no(items) or data.is_scalar: return data\n",
        "        excluded, items = None, [item for item in range(len(data)) if item not in items]\n",
        "        if data.is_vector: excluded = rec.read([data])[:,items][0]\n",
        "        else: excluded = data[items,:]\n",
        "        return rec.read(excluded, to=type(data), **meta(data), **sets)\n",
        "    def include(data, *items, **sets):\n",
        "        if no(items) or data.is_scalar: return data\n",
        "        included = []\n",
        "        if data.is_vector: included = rec.read([data])[:,items][0]\n",
        "        else: included = data[items,:]\n",
        "        return rec.read(included, to=type(data), **meta(data), **sets)\n",
        "\n",
        "create = record = rec.read\n",
        "line = linspace\n",
        "\n",
        "def series(ori,end=None,by=1):\n",
        "    if no(end): end=ori; ori=0\n",
        "    if not issubclass(type(by), int): return create(arange(ori,end,by))\n",
        "    return array(range(ori,end,by))\n",
        "\n",
        "def plot(data, at = 0., spacing = 1., color = 'k', width = 1., offset=0.): #review\n",
        "    draw = record(data, **meta(data)); at = spacing*draw.as_matrix.shape[0]\n",
        "    axes = lab.gca(); axes.set_ylim([at+max(data),0-max(data)]); at=0\n",
        "    for n, row in ni(draw.as_matrix):\n",
        "        if some(offset): row = draw[n]-average(row)+offset\n",
        "        c, w = color, width\n",
        "        if isiterable(color): c = color[n]\n",
        "        if isiterable(width): w = width[n]\n",
        "        lab.plot(at+row+n*spacing, color = c, linewidth = w)\n",
        "\n",
        "def butter_type(lowcut, highcut, fs, order=5, type='band'):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = butter(order, [low, high], btype=type)\n",
        "    return b, a\n",
        "\n",
        "def butter_filter(data, lowcut, highcut, fs, order=5, type='band'):\n",
        "    b, a = butter_type(lowcut, highcut, fs, order=order, type=type)\n",
        "    y = lfilter(b, a, data)\n",
        "    return y\n",
        "\n",
        "def _to_rec(this):\n",
        "    if not issubclass(type(this), rec): return rec.read(this, **meta(this)), rec\n",
        "    return this, type(this)\n",
        "\n",
        "def _prefilt(data, fs):\n",
        "    pre = []\n",
        "    for line in data.as_matrix:\n",
        "        vector = line.tolist()\n",
        "        pre.append(vector[:int(fs)]+vector)\n",
        "    return record(pre)\n",
        "\n",
        "def _postfilt(data, fs):\n",
        "    post = []\n",
        "    for line in data:\n",
        "        vector = line.tolist()\n",
        "        post.append(vector[int(fs):])\n",
        "    return post\n",
        "\n",
        "def notch(this, using=butter_type, fs=_SAMPLING, size=2, at=_NOTCH, order=5):\n",
        "    data, type = _to_rec(this)\n",
        "    if data.has('sampling'): fs=data.sampling\n",
        "    nyq, cutoff = fs / 2., []\n",
        "    for f in range(int(at), int(nyq), int(at)):\n",
        "        cutoff.append((f - size, f + size))\n",
        "    signal = _prefilt(data, fs)\n",
        "    for bs in cutoff:\n",
        "        low,hi = bs\n",
        "        b,a = butter_type(low,hi,fs,order,'bandstop')\n",
        "        signal = lfilter(b,a,signal)\n",
        "    return record(_postfilt(signal, fs), to=type, **meta(data))\n",
        "\n",
        "def band(this, low_high, fs=_SAMPLING, using=butter_filter, order=5):\n",
        "    data, type = _to_rec(this)\n",
        "    if data.has('sampling'): fs=data.sampling\n",
        "    low, high = min(low_high), max(low_high)\n",
        "    if low<1.: low = 1.\n",
        "    tailed = _prefilt(data, fs)\n",
        "    tailed = using(tailed, low, high, fs, order)\n",
        "    return rec.read(_postfilt(tailed, fs), to=type, **meta(data))\n",
        "\n",
        "def binarize(this):\n",
        "    data, type = _to_rec(this)\n",
        "    if data.is_binary: return data\n",
        "    rows = []\n",
        "    for row in data.as_matrix:\n",
        "        d = row - array([row[-1]]+row[:-1].tolist())\n",
        "        d[d>=0] = 1; d[d<0] = 0\n",
        "        rows.append(d.astype(ubyte))\n",
        "    return rec.read(rows, to=type).get_as(data)\n",
        "\n",
        "def halve(matrix):\n",
        "    halved, (data, type) = [], _to_rec(matrix)\n",
        "    for line in data.as_matrix:\n",
        "        h = resize(line, (int(len(line)/2), 2))\n",
        "        halved.append((h[:,0]+h[:,1])/2.)\n",
        "    return rec.read(halved, to=type, **meta(data))\n",
        "\n",
        "def dwindle(matrix, by=1):\n",
        "    if by: return dwindle(halve(matrix), by-1)\n",
        "    return matrix\n",
        "\n",
        "def upsample(matrix, fs1, fs2):\n",
        "    y=zeros((matrix.shape[0],fs2))\n",
        "    if fs1 < fs2:\n",
        "        #upsampling by a factor R\n",
        "        L=matrix.shape[1]\n",
        "        R=int(floor(fs2/fs1)+1)\n",
        "        for i,e in enumerate(matrix):\n",
        "            ups=[]\n",
        "            for j in range(L-1):\n",
        "                if j>0: ups.append(list(linspace(matrix[i][j],matrix[i][j+1],R)[1:3]))\n",
        "                else: ups.append(list(linspace(matrix[i][j],matrix[i][j+1],R)[0:3]))\n",
        "            for k,s in enumerate(sum(ups, [])): y[i][k]=s\n",
        "            y[i][-1]=y[i][-2]\n",
        "        return rec.read(y)\n",
        "    else: print(\"Error: fs1 >= fs2\")\n",
        "\n",
        "def remap(this, axis=None, base=0, top=1., e=0):\n",
        "    def map(x, b, t, e): return ((x-min(x)+e)/(max(x)-min(x)+e)+b)*(t-b)\n",
        "    data, type = _to_rec(this)\n",
        "    if no(axis): return rec.read(map(this, base, top, e), to=type, **meta(data))\n",
        "    rows = data.as_matrix\n",
        "    if axis==0 or axis>1: rows = rows.T\n",
        "    remapped = []\n",
        "    for row in rows: remapped.append(map(row, base, top, e))\n",
        "    if axis==0 or axis>1: rows = rows.T\n",
        "    return rec.read(remapped).get_as(data)\n",
        "\n",
        "this = Record\n",
        "\n",
        "from numpy import median, random\n",
        "import scipy.stats as stats\n",
        "\n",
        "class Table(COb, GEq, GRe):\n",
        "    data = None\n",
        "    default = None\n",
        "    PAD = 3\n",
        "    ELLIPSIS_AT = int(GRe._lines_*.3)\n",
        "    class _axes(list):\n",
        "        def insert(_from, this, item):\n",
        "            super().insert(this, item)\n",
        "            _from.__dict__[item.name] = item\n",
        "        def __setitem__(_, pos, axis):\n",
        "            super().__setitem__(pos, axis)\n",
        "            _.__dict__[axis.name] = axis\n",
        "    class axis(list, GSet, GRe):\n",
        "        name = None\n",
        "        root = None\n",
        "        _to = 0\n",
        "        def __init__(axis, root, labels, name='ax', force_at=None):\n",
        "            if force_at: root.axes[force_at].name = None\n",
        "            with this(labels) as dim:\n",
        "                if not dim.isiterable and dim.inherits(int): labels = range(labels)\n",
        "            super().__init__(labels)\n",
        "            names = [ax.name for ax in root.axes]\n",
        "            name_, n = name, 1\n",
        "            while name in names: name = name_ + str(n); n+=1\n",
        "            axis._set(root=root, name=name)\n",
        "            if force_at: root.axes[force_at] = axis\n",
        "            else: root.axes.insert(0, axis)\n",
        "        def at(axis, field):\n",
        "            field = int(field) if this(field).inherits(str) and field.isdecimal() else field\n",
        "            found = axis.index(field) if field in axis else None\n",
        "            axis._to = found if found != None else field\n",
        "        def __repr__(_):\n",
        "            return '{}: {}'.format(_.name, _._resize(' '.join([str(i) for i in _])))\n",
        "    def __init__(this, **table_description):\n",
        "        super().__init__(axes=this._axes())\n",
        "        this.set(**table_description)\n",
        "    def reset(data):\n",
        "        base = None\n",
        "        if len(data.axes)>0:\n",
        "            base = [data.default]*len(data.axes[-1])\n",
        "            for ax in reversed(data.axes[0:-1]): base = [base]*len(ax)\n",
        "        data._set(data=array(base))\n",
        "    @property\n",
        "    def ax_names(_): return [ax.name for ax in _.axes]\n",
        "    def at(data, axis):\n",
        "        with this(axis) as _axis:\n",
        "            if _axis.inherits(int):\n",
        "                if axis>0 and axis<len(data.axes): return data.axes[axis]\n",
        "            elif _axis.inherits(str):\n",
        "                axes = data.ax_names\n",
        "                if axis in axes: return data.axes[axes.index(axis)]\n",
        "        return None\n",
        "    def _check(build):\n",
        "        if build.data == None: build.reset()\n",
        "        return build.data\n",
        "    def _find(_, inverted, ax_field):\n",
        "        _._check()\n",
        "        def index(axis, entry):\n",
        "            fields = _.at(axis)\n",
        "            if fields != None:\n",
        "                if this(entry).isiterable:\n",
        "                    return tuple([fields.index(field) for field in entry])\n",
        "                else: return ':'\n",
        "            return None\n",
        "        def translate(axis, found):\n",
        "            if axis.name in found:\n",
        "                _range = found[axis.name]\n",
        "                if this(_range).inherits(tuple):\n",
        "                    if inverted: found[axis.name] = tuple([field for field in range(len(axis)) if field not in _range])\n",
        "                    return \"_from['{}']\".format(axis.name)\n",
        "            return ':'\n",
        "        found={field:index(field,entry) for field,entry in ax_field.items()}\n",
        "        found={field:value for field,value in found.items() if value is not None}\n",
        "        reshape='M['+','.join([translate(axis,found) for axis in _.axes])+']'\n",
        "        _._set(_reshape_ = (reshape, found))\n",
        "    def _by_val(_, depth=-1, _layer=0):\n",
        "        M, axes = _._check(), _.axes\n",
        "        do, _from = _.__dict__.pop('_reshape_') if '_reshape_' in _._meta() else (None, {})\n",
        "        copy = super()._by_val(depth, _layer)\n",
        "        copy.axes = []\n",
        "        for ax in reversed(axes):\n",
        "            fields = [field for n,field in enumerate(ax) if n in _from[ax.name]] if ax.name in _from else ax\n",
        "            copy.set(**{ax.name:fields})\n",
        "        copy.data = eval(do) if do else M.copy()\n",
        "        return copy\n",
        "    def _translate(_, directions):\n",
        "        axes = directions.split(',')\n",
        "        for ax_dir in axes:\n",
        "            ax, field = [token.strip() for token in ax_dir.split(':')]\n",
        "            axis = _.at(ax)\n",
        "            if axis: axis.at(field)\n",
        "        return '['+','.join([str(ax._to) for ax in _.axes])+']'\n",
        "    def _get_set(_, directions, mode='get', value=None):\n",
        "        if mode == 'get' and not '_MGET' in _.sets: _._MGET = []\n",
        "        if len(directions) == len(_.axes):\n",
        "            resolve, message = True, []\n",
        "            for n,part in enumerate(directions):\n",
        "                _part = this(part)\n",
        "                if _part.inherits(int, str) or _part.isiterable and len(part)==1:\n",
        "                    token = part if _part.inherits(str, int) else part[0]\n",
        "                    message.append(':'.join([str(_.axes[n].name),str(part)]))\n",
        "                else:\n",
        "                    resolve = False\n",
        "                    for token in part:\n",
        "                        redirection = list(directions)\n",
        "                        redirection[n] = token\n",
        "                        _._get_set(tuple(redirection), mode, value)\n",
        "            if resolve:\n",
        "                message = ','.join(message)\n",
        "                if mode=='get': _._MGET.append(_[message])\n",
        "                else: _[message] = value\n",
        "    def __getitem__(by, field_directions):\n",
        "        M = by._check()\n",
        "        if this(field_directions).inherits(tuple):\n",
        "            by._get_set(field_directions)\n",
        "            result = by.__dict__.pop('_MGET')\n",
        "            return result\n",
        "        else: return eval('M'+by._translate(field_directions))\n",
        "    def __setitem__(by, field_directions, value):\n",
        "        M = by._check()\n",
        "        if this(field_directions).inherits(tuple): by._get_set(field_directions, 'set', value)\n",
        "        else: exec('M'+by._translate(field_directions)+'=value')\n",
        "    def set(data, **ax_field):\n",
        "        for name, fields in ax_field.items(): data.axis(data, fields, name)\n",
        "    def get(data, **ax_field):\n",
        "        data._find(0, ax_field)\n",
        "        return data._by_val()\n",
        "    def let(data, **ax_field):\n",
        "        data._find(1, ax_field)\n",
        "        return data._by_val()\n",
        "    @property\n",
        "    def sets(tree): return set(meta(tree))\n",
        "    def __repr__(self):\n",
        "        M = self._check()\n",
        "        _repr, dimensions = '', len(self.axes)\n",
        "        if not dimensions: _repr += 'void table\\n'\n",
        "        else:\n",
        "            dimensions = len(self.axes)\n",
        "            y = self.axes[-2] if dimensions >= 2 else None\n",
        "            if dimensions>2:\n",
        "                y = self.axes[-2]\n",
        "                for n,ax in enumerate(self.axes[:-2]): _repr += '{}{}: {}/{}\\n'.format('\\t'*n, ax.name, ax._to, len(ax))\n",
        "            mr = eval('M'+str([ax.index(ax._to) for ax in self.axes][:-2])) if dimensions>2 else M\n",
        "            pad = max([len(y.name)]+[len(str(field)) for field in y]+[len(str(value)) for line in mr for value in line])+self.PAD if dimensions>1 else 0\n",
        "            _repr, x, spaces = _repr+y.name+'\\n' if y else '', self.axes[-1], ' '*pad if pad>0 else '\\t'\n",
        "            header = spaces+''.join([str(field).ljust(pad) for field in x])\n",
        "            _repr += self._resize(header) + '\\n'\n",
        "            ellipsis_at = self._lines_-self.ELLIPSIS_AT-1\n",
        "            last_values_from = len(mr)-self.ELLIPSIS_AT\n",
        "            if last_values_from<=ellipsis_at: last_values_from = ellipsis_at+1\n",
        "            for n, line in enumerate(mr):\n",
        "                if n<ellipsis_at or n>last_values_from:\n",
        "                    values = str(y[n]).ljust(pad) if y else ''\n",
        "                    values += ''.join([str(value).ljust(pad) for value in line])\n",
        "                    _repr += self._resize(values) + '\\n'\n",
        "                elif n==ellipsis_at:\n",
        "                    _repr += self._ellipsis_ + '\\n'\n",
        "        extra = {k:v for k,v in meta(self).items() if k != 'data' and k != 'axes'}\n",
        "        _repr += self._resize(spaces*len(x)+x.name)+'\\n'+'\\n'.join(['{}:\\t{}'.format(k, self._repr(v)) for k,v in extra.items()])\n",
        "        return _repr\n",
        "TAb = tab = Table\n",
        "\n",
        "def set_repr_to(lines, ratio=.7):\n",
        "    set_repr_to(lines)\n",
        "    Table.ELLIPSIS_AT = int(Table._lines_*(1-ratio))\n",
        "\n",
        "def butter_type(lowcut, highcut, fs, order=5, type='band'):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = butter(order, [low, high], btype=type)\n",
        "    return b, a\n",
        "\n",
        "def butter_filter(data, lowcut, highcut, fs, order=5, type='band'):\n",
        "    b, a = butter_type(lowcut, highcut, fs, order=order, type=type)\n",
        "    y = lfilter(b, a, data)\n",
        "    return y\n",
        "\n",
        "!pip install pyEDFlib # installing the pyEDFlib library in the Google Colab environment\n",
        "\n",
        "import pyedflib as edf\n",
        "\n",
        "class EEG(Table):\n",
        "    LABEL_START = 'EEG '\n",
        "    BAD = ['TTL', 'ECG']\n",
        "    BP_SEP = '-'\n",
        "    class time:\n",
        "        \"\"\" converts time units to seconds by frequency sampling (fs) \"\"\"\n",
        "        unit = 'units'\n",
        "        def __init__(_, units): _.time = units\n",
        "        def __call__(_, fs=None): return _.time/fs\n",
        "        def __repr__(_): return '{} {}'.format(str(_.time),_.unit)\n",
        "    class ms(time):\n",
        "        \"\"\" converts ms to time units by frequency sampling (fs) \"\"\"\n",
        "        unit = 'ms'\n",
        "        def __call__(_, fs=1000): return int(round(_.time*fs/1000))\n",
        "    class secs(time):\n",
        "        \"\"\" converts seconds to time units by frequency sampling (fs) \"\"\"\n",
        "        unit = 's'\n",
        "        def __call__(_, fs=1000): return int(_.time*fs)\n",
        "    def _load(eeg, epoch, n):\n",
        "        data = None\n",
        "        with edf.EdfReader(eeg.file) as file:\n",
        "            data = [file.readSignal(eeg.labels[id], epoch.at, epoch.span) for id in eeg.labels]\n",
        "            file.close()\n",
        "        if data != None:\n",
        "            eeg._set(data=array(data), at_epoch=(n, epoch()))\n",
        "        else:\n",
        "            if 'at_epoch' in eeg.sets: del(eeg.at_epoch)\n",
        "            eeg._set(data=None)\n",
        "    class step(GSet):\n",
        "        START = 0\n",
        "        CENTRE = 1\n",
        "        END = 2\n",
        "        def __init__(step, space, duration):\n",
        "            step._set(at=space, span=duration)\n",
        "        def reset(grid, at=0, root=None):\n",
        "            if root: grid._set(root=root)\n",
        "            else: root = grid.root\n",
        "            all_space, left = root.duration(root.fs), 0\n",
        "            if grid.at.time == 0: epochs = [EEG.step(0, all_space)]\n",
        "            else:\n",
        "                space, span, epochs = grid.at(root.fs), grid.span(root.fs), []\n",
        "                for x in range(at, all_space, space):\n",
        "                    end = x+span\n",
        "                    if end>all_space: left = all_space-x\n",
        "                    else: epochs.append(EEG.step(x, span))\n",
        "            grid._set(_all=epochs, skip=at, out=left)\n",
        "        def __call__(step, _as=None):\n",
        "            if 'root' in meta(step):\n",
        "                if _as == None: return len(step._all)\n",
        "            elif _as is not None: step.id = _as\n",
        "            elif 'id' in meta(step): return step.id\n",
        "        def items(wrapped):\n",
        "            if 'root' in meta(wrapped): return wrapped._all\n",
        "        def __getitem__(by, epoch_n):\n",
        "            if 'root' in meta(by) and epoch_n<len(by._all):\n",
        "                by.root._load(by._all[epoch_n], epoch_n)\n",
        "        def __repr__(_): return '|'.join([repr(_.at),repr(_.span)])\n",
        "    class event(GSet):\n",
        "        def __init__(event, to=None, group=None, _as=0, _from=0):\n",
        "            event._set(mode=_from, note=group, id=_as)\n",
        "            if to is not None: event.link(to)\n",
        "        def link(event, to):\n",
        "            if event.note == None or not 'event' in to.sets:\n",
        "                if event.note == None:\n",
        "                    to.event = event\n",
        "                    event.type = []\n",
        "                    return\n",
        "                else: EEG.event(to)\n",
        "            types = to.event.type\n",
        "            ids = [to.event.id]+[_type.id for _type in types]\n",
        "            while event.id in ids: event.id += 1\n",
        "            if event.note in to.notes:\n",
        "                event.at = to.notes[event.note]\n",
        "                types.append(event)\n",
        "        def __repr__(event):\n",
        "            _repr = str(event.id)\n",
        "            if 'at' in meta(event): _repr += ' at: {}'.format(event.at)\n",
        "            if 'type' in meta(event):\n",
        "                for subev in event.type: _repr += '; '+repr(subev)\n",
        "            return _repr\n",
        "    def _at_last(eeg, sets):\n",
        "        if 'epoch' in meta(eeg):\n",
        "            eeg.epoch.reset(root=eeg)\n",
        "            if len(eeg.axes.time) != eeg.epoch.span(eeg.fs): eeg.axis(eeg, eeg.epoch.span(eeg.fs), 'time', 1)\n",
        "    @staticmethod\n",
        "    def from_file(name, step=None, bad=None):\n",
        "        def correct_(label):\n",
        "            if label.startswith(EEG.LABEL_START): return label[len(EEG.LABEL_START):]\n",
        "            return label\n",
        "        eeg = EEG()\n",
        "        with edf.EdfReader(name) as file:\n",
        "            if bad == None: bad = EEG.BAD\n",
        "            duration = EEG.secs(file.getFileDuration())\n",
        "            fs = file.getSampleFrequencies()[0]\n",
        "            if step == None: step = EEG.step(EEG.secs(0), duration)\n",
        "            raw_notes = file.readAnnotations()\n",
        "            notes = {note:[] for note in set(raw_notes[-1])}\n",
        "            for n, note in enumerate(raw_notes[-1]):\n",
        "                notes[note].append(EEG.secs(raw_notes[0][n]))\n",
        "            labels = [correct_(label) for label in file.getSignalLabels()]\n",
        "            labels = {label:n for n,label in enumerate(labels) if label not in bad}\n",
        "            eeg.set(time=step.span(fs), region=tuple(labels))\n",
        "            eeg(file=name, duration=duration, fs=fs, notes=notes, labels=labels, epoch=step)\n",
        "            file.close()\n",
        "        return eeg\n",
        "    def remap(eeg, at=None, step=None):\n",
        "        sets = eeg.sets\n",
        "        if this(step).inherits(EEG.step): eeg._set(epoch=step)\n",
        "        if at == None:\n",
        "            at = eeg._best_map if '_best_map' in sets else 0\n",
        "        eeg.epoch.reset(at)\n",
        "        if 'event' in sets:\n",
        "            deltas = []\n",
        "            for epoch in eeg.epoch._all: epoch.id = None\n",
        "            for event in eeg.event.type:\n",
        "                for time in event.at:\n",
        "                    at, space, limit = time(eeg.fs), eeg.epoch.at(eeg.fs), len(eeg.epoch.items())-1\n",
        "                    for n,epoch in enumerate(eeg.epoch.items()):\n",
        "                        end = epoch.at+space if n<limit else epoch.span\n",
        "                        if at>=epoch.at and at<end:\n",
        "                            epoch(event.id)\n",
        "                            if event.mode == eeg.step.START: deltas.append(EEG.time(at-epoch.at)(eeg.fs))\n",
        "                            elif event.mode == eeg.step.END: deltas.append(EEG.time(end-at-1)(eeg.fs))\n",
        "                            else:\n",
        "                                centre = epoch.at+int(round(epoch.span/2))\n",
        "                                deltas.append(EEG.time(abs(at-centre))(eeg.fs))\n",
        "                            break\n",
        "            for epoch in eeg.epoch._all:\n",
        "                if epoch() is None: epoch(eeg.event.id)\n",
        "            eeg.deltas = deltas\n",
        "    def optimize(eeg, *events, grid=None):\n",
        "        if events:\n",
        "            for event in events: event.link(eeg)\n",
        "        eeg.remap(0, grid)\n",
        "        gaussian_space = stats.shapiro if len(eeg.deltas)<=5000 else stats.normaltest\n",
        "        def test():\n",
        "            _, p = gaussian_space(eeg.deltas) if len(eeg.deltas)>2 else 0,1\n",
        "            if p<=0.05: return p, median(eeg.deltas)\n",
        "            return p, average(eeg.deltas)\n",
        "        (p, best), at, check = test(), 0, eeg.epoch.span(eeg.fs)\n",
        "        print('optimizing epoch position...', end=' ')\n",
        "        for _try in range(1, check):\n",
        "            eeg.remap(_try)\n",
        "            p, check = test()\n",
        "            if check<best: p, best, at = p, check, _try\n",
        "        _test = 'median' if p<0.05 else 'mean'\n",
        "        print('best frame found at {:.3f}s with a {} delay of {:.3f}s'.format(EEG.time(at)(eeg.fs), _test, EEG.time(best)(eeg.fs)))\n",
        "        eeg._set(_best_map=at)\n",
        "    class sampler(GSet, GRe):\n",
        "        eeg = None\n",
        "        def __init__(map, root, *reserve, **opts):\n",
        "            raw, proc = [step() for step in root.epoch.items()], []\n",
        "            find, key = None, {k:v for k,v in reserve}\n",
        "            for step in raw:\n",
        "                if find==step: find=None\n",
        "                if find == None: proc.append(step)\n",
        "                else: proc.append(None)\n",
        "                if step in key: find = key[step]\n",
        "            key = {k:[] for k in list(set(raw))+[None]}\n",
        "            for n,id in enumerate(proc): key[id].append(n)\n",
        "            map._set(eeg=root, key=key, mask=proc, **opts)\n",
        "        def _at_last(_, sets):\n",
        "            if 'seed' in sets: random.seed(_.seed)\n",
        "        def set(map, **event_range):\n",
        "            prev, key = map.key, {}\n",
        "            for k,deltas in event_range.items():\n",
        "                if k in prev:\n",
        "                    seq, key[k] = prev[k], []\n",
        "                    for item in seq: key[k] += [item+d for d in deltas]\n",
        "                    for o in prev:\n",
        "                        if o != k:\n",
        "                            for e in key[k]:\n",
        "                                if e in prev[o]: prev[o].pop(prev[o].index(e))\n",
        "            for k in prev:\n",
        "                if k not in key: key[k] = prev[k]\n",
        "            map._set(prev=prev, key=key)\n",
        "        def get(map, event, times, random_seed=None):\n",
        "            if random_seed and not 'seed' in meta(map): map._set(seed=random_seed)\n",
        "            if not 'pool' in meta(map): map._set(pool = {k:map.key[k].copy() for k in map.key})\n",
        "            resampled, sequence = [], []\n",
        "            while times:\n",
        "                if len(map.pool[event])==0: map.pool[event] = map.key[event].copy()\n",
        "                at = map.pool[event].pop(random.randint(len(map.pool[event])))\n",
        "                map.eeg.epoch[at]\n",
        "                resampled.append(map.eeg.data)\n",
        "                sequence.append(at)\n",
        "                times -= 1\n",
        "            return resampled, sequence\n",
        "        def __repr__(_): return _._resize('|'.join([str(id) if id!=None else ' ' for id in _.mask]))\n",
        "    def tag(event, *a_b, **event_range):\n",
        "        event._set(sample=event.sampler(event, *a_b))\n",
        "        event.sample.set(**event_range)\n",
        "\n",
        "STEp = epoch = EEG.step\n",
        "TIME = EEG.time\n",
        "SET = EEG.event\n",
        "secs = EEG.secs\n",
        "ms = EEG.ms\n",
        "\n",
        "def preprocess(eeg, epoch, limit=500):\n",
        "    \"\"\"Primary preprocessing. Resamples data to a limit frequency and applies a notch filter.\n",
        "\n",
        "    Args:\n",
        "        eeg (eeg): Wrapper object of the raw EEG data and metadata.\n",
        "        epoch (list): Signal epoch.\n",
        "        limit (int): Target frequency for resampling. Defaults to 500.\n",
        "\n",
        "    Returns:\n",
        "        list: Preprocessed epoch.\n",
        "    \"\"\"\n",
        "    sampling, rse = limit, epoch\n",
        "    if eeg.fs == limit: rse = epoch\n",
        "    elif eeg.fs%limit != 0: rse = upsample(epoch, eeg.fs, limit) if eeg.fs<limit else dwindle(epoch, int(eeg.fs/limit)-1)\n",
        "    else: rse = upsample(epoch, eeg.fs, limit) if eeg.fs<limit else dwindle(epoch, int(eeg.fs/limit)-2)\n",
        "    nse = notch(rse, fs=sampling, order=2)\n",
        "    return nse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the PREP file of a single subject (define subject, WOI and the connectivity measure), containing a dictionary of preprocessed data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HicJ3ouGUfnS"
      },
      "outputs": [],
      "source": [
        "# main_folder = \"/content/gdrive/My Drive/epigame-folder/30sec/\"\n",
        "main_folder = \"/media/kivi/ADATA HV100/epigame-folder/\"\n",
        "\n",
        "cm_folder = main_folder + \"connectivity_matrices/\"\n",
        "# cvs_folder = main_folder + \"results/\"\n",
        "\n",
        "subject = 1\n",
        "woi = \"preseizure1\"\n",
        "measure = \"CC-(0,4)\"\n",
        "\n",
        "cm_filename = cm_folder + f\"{subject}-{woi}-{measure}.prep\"\n",
        "# cvs_filename = cvs_folder + f\"{subject}-{woi}-{measure}.res\"\n",
        "\n",
        "cm_load = REc.load(cm_filename)\n",
        "# cvs_load = REc.load(cvs_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the connectivity matrices (CM)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ki6OqVXaYObH"
      },
      "outputs": [],
      "source": [
        "cm = cm_load.data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a dictionary with epoch index as key and epoch CM as value, sorted sequentially (this is because epochs are saved randomly in the PREP file)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AssrLzP3nCi"
      },
      "outputs": [],
      "source": [
        "n_epochs = int(len(cm.X)/2)\n",
        "\n",
        "idx_cm_tuples = [(idx,cm.X[i]) for i,idx in enumerate(cm.i)]\n",
        "\n",
        "idx_cm_1 = {idx:cm for (idx,cm) in idx_cm_tuples[:n_epochs]}\n",
        "idx_cm_2 = {idx:cm for (idx,cm) in idx_cm_tuples[n_epochs::]}\n",
        "\n",
        "idx_cm_1 = dict(sorted(idx_cm_1.items()))\n",
        "idx_cm_2 = dict(sorted(idx_cm_2.items()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The function get_connectivity returns the connectivity value for a node pair from a CM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gW8VJtRYqhT"
      },
      "outputs": [],
      "source": [
        "def get_connectivity(matrix, row, col):\n",
        "  # Get the connectivity measure for a node pair with indices row and col\n",
        "\n",
        "    if row < 0 or row >= len(matrix) or col < 0 or col >= len(matrix[0]):\n",
        "        return None  # Indices are out of bounds\n",
        "    return matrix[row][col]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test the function above by making lists of connectivity values for node1 and node2 from interictal (conn_1) and WOI (conn_2) epochs. Check means."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-BDMZFAUVwy",
        "outputId": "9fbc5762-93a6-4679-874c-216276af086c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "node1, node2 = 1,2\n",
        "\n",
        "epochs_1 = list(idx_cm_1.values())\n",
        "epochs_2 = list(idx_cm_2.values())\n",
        "# List of connectivity measures for all epochs\n",
        "conn_1 = [get_connectivity(cm, node1, node2) for cm in epochs_1]\n",
        "conn_2 = [get_connectivity(cm, node1, node2) for cm in epochs_2]\n",
        "\n",
        "print(np.mean(conn_1), np.mean(conn_2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the interictal (non-seizure) and WOI connectivity values (\"connectivity evolution\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "4IlKlD_WYH0E",
        "outputId": "e82a838e-5290-4048-aded-df506a8b6fc0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.plot(conn_1, label=\"non-seizure\")\n",
        "plt.plot(conn_2, label=\"preseizure\")\n",
        "plt.title(\"Connectivity evolution\")\n",
        "plt.xlabel(\"Epoch index\")\n",
        "plt.ylabel(f\"Connectiviy measure ({measure})\")\n",
        "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculate the Shannon entropy of the series using the scipy.stats entropy function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4S0tDWB8xTr",
        "outputId": "788c679d-5c06-4516-feca-d34d3b8dc537"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import entropy\n",
        "\n",
        "# Calculate Shannon entropy\n",
        "# Shannon entropy measures the uncertainty or randomness in a time series. Higher entropy could indicate higher dynamics or volatility.\n",
        "entropy_1 = entropy(conn_1, base=2)  # Set base to 2 for log base 2\n",
        "entropy_2 = entropy(conn_2, base=2)  # Set base to 2 for log base 2\n",
        "\n",
        "print(f\"Shannon Entropy of non-seizure: {entropy_1:.4f}\")\n",
        "print(f\"Shannon Entropy of preseizure: {entropy_2:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we analyze the whole cohort. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLGi4wDkQ3aM"
      },
      "outputs": [],
      "source": [
        "NODES = {\n",
        "\"1\":['P1-P2', 'P4-P5', 'P8-P9', 'P9-P10', 'P10-P11', 'G1-G2', 'G8-G9', 'G9-G10', 'G10-G11', 'G11-G12', 'M1-M2', 'M8-M9', 'M9-M10', 'M10-M11', 'M11-M12', 'O1-O2', 'O2-O3', 'O5-O6', 'O6-O7', 'F1-F2', 'F7-F8', 'F8-F9', 'F9-F10', 'F10-F11', 'F11-F12', 'F12-F13', 'A1-A2', 'A2-A3', 'A3-A4', 'A7-A8', 'A8-A9', 'A9-A10', 'A10-A11', 'B1-B2', 'B2-B3', 'B5-B6', 'B6-B7', 'B7-B8', 'B8-B9', 'C1-C2', 'C5-C6', 'C6-C7', 'C7-C8', 'C8-C9', 'C9-C10', 'Q1-Q2', 'Q2-Q3', 'Q3-Q4', 'Q4-Q5', 'Q8-Q9', 'Q9-Q10', 'Q10-Q11', 'Q11-Q12', 'T1-T2', 'T2-T3', 'T3-T4', 'T4-T5', 'T5-T6', 'T6-T7', 'T7-T8', 'T8-T9', 'T9-T10', 'T10-T11', 'T11-T12', 'D1-D2', 'D2-D3', 'D3-D4', 'D4-D5', 'D5-D6', 'D6-D7', 'D7-D8', 'E1-E2', 'E2-E3', 'E3-E4', 'E4-E5', 'E5-E6', 'E6-E7', 'E7-E8', 'E8-E9', 'E9-E10', 'E10-E11', 'L1-L2', 'L2-L3', 'L5-L6', 'L6-L7', 'L7-L8', 'U1-U2', 'U2-U3', 'U3-U4', 'U4-U5', 'U5-U6', 'U6-U7', 'J1-J2', 'J9-J10', 'J10-J11', 'J11-J12', 'J12-J13', 'J13-J14', 'J14-J15'],\n",
        "\"2\":['A1-A2', 'A2-A3', 'A3-A4', 'A4-A5', 'B1-B2', 'B2-B3', 'B3-B4', 'B4-B5', 'B5-B6', 'B6-B7', 'B7-B8', 'B8-B9', 'B9-B10', 'C1-C2', 'C2-C3', 'C3-C4', 'C4-C5', 'C5-C6', 'C6-C7', 'C7-C8', 'C8-C9', 'C9-C10', 'C10-C11', 'C11-C12', 'D1-D2', 'D2-D3', 'D3-D4', 'D4-D5', 'E1-E2', 'E2-E3', 'E3-E4', 'E4-E5', 'E5-E6', 'E6-E7', 'E7-E8', 'F1-F2', 'F2-F3', 'F3-F4', 'F4-F5', 'F5-F6', 'F6-F7', 'F7-F8'],\n",
        "\"3\":[\"T'1-T'2\", \"T'2-T'3\", \"T'3-T'4\", \"T'4-T'5\", \"T'5-T'6\", \"T'6-T'7\", \"T'7-T'8\", \"A'1-A'2\", \"A'2-A'3\", \"A'3-A'4\", \"A'4-A'5\", \"A'5-A'6\", \"A'6-A'7\", \"A'7-A'8\", \"A'8-A'9\", \"A'9-A'10\", \"B'1-B'2\", \"B'2-B'3\", \"B'3-B'4\", \"B'4-B'5\", \"B'5-B'6\", \"B'6-B'7\", \"B'7-B'8\", \"B'8-B'9\", \"B'9-B'10\", \"C'1-C'2\", \"C'2-C'3\", \"C'3-C'4\", \"C'4-C'5\", \"C'5-C'6\", \"C'6-C'7\", \"C'7-C'8\", \"C'8-C'9\", \"C'9-C'10\", \"C'10-C'11\", \"C'11-C'12\", \"E'1-E'2\", \"E'2-E'3\", \"E'3-E'4\", \"E'4-E'5\", \"E'5-E'6\", \"E'6-E'7\", \"E'7-E'8\", \"E'8-E'9\", \"E'9-E'10\", \"D'1-D'2\", \"D'2-D'3\", \"D'3-D'4\", \"D'4-D'5\", \"D'5-D'6\", \"D'6-D'7\", \"D'7-D'8\", \"D'8-D'9\", \"D'9-D'10\", \"D'10-D'11\", \"D'11-D'12\", \"W'1-W'2\", \"W'2-W'3\", \"W'3-W'4\", \"W'4-W'5\", \"W'5-W'6\", \"W'6-W'7\", \"W'7-W'8\", \"W'8-W'9\", \"W'9-W'10\", \"W'10-W'11\", \"W'11-W'12\", \"W'12-W'13\", \"W'13-W'14\", \"W'14-W'15\", \"K'1-K'2\", \"K'2-K'3\", \"K'3-K'4\", \"K'4-K'5\", \"K'5-K'6\", \"K'6-K'7\", \"K'7-K'8\", \"K'8-K'9\", \"K'9-K'10\", \"K'10-K'11\", \"K'11-K'12\", \"K'12-K'13\", \"K'13-K'14\", \"K'14-K'15\", \"P'1-P'2\", \"P'2-P'3\", \"P'3-P'4\", \"P'4-P'5\", \"P'5-P'6\", \"P'6-P'7\", \"P'7-P'8\", \"P'8-P'9\", \"P'9-P'10\", \"P'10-P'11\", \"P'11-P'12\", \"P'12-P'13\", \"P'13-P'14\", \"P'14-P'15\", \"L'1-L'2\", \"L'2-L'3\", \"L'3-L'4\", \"L'4-L'5\", \"L'5-L'6\", \"L'6-L'7\", \"L'7-L'8\", \"L'8-L'9\", \"L'9-L'10\", \"L'10-L'11\", \"L'11-L'12\", \"O'1-O'2\", \"O'2-O'3\", \"O'3-O'4\", \"O'4-O'5\", \"O'5-O'6\", \"O'6-O'7\", \"O'7-O'8\", \"O'8-O'9\", \"O'9-O'10\", \"O'10-O'11\", \"O'11-O'12\", \"X'1-X'2\", \"X'2-X'3\", \"X'3-X'4\", \"X'4-X'5\", \"X'5-X'6\", \"X'6-X'7\", \"X'7-X'8\", \"X'8-X'9\", \"X'9-X'10\", \"X'10-X'11\", \"X'11-X'12\", \"X'12-X'13\", \"X'13-X'14\", \"X'14-X'15\"],\n",
        "\"4\":['A1-A2', 'A2-A3', 'A3-A4', 'A4-A5', 'A5-A6', 'A6-A7', 'A7-A8', 'A8-A9', 'A9-A10', 'A10-A11', 'B1-B2', 'B2-B3', 'B3-B4', 'B4-B5', 'B5-B6', 'B6-B7', 'B7-B8', 'B11-B12', 'C1-C2', 'C2-C4', 'C4-C5', 'C5-C6', 'C6-C7', 'C7-C8', 'C8-C9', 'C9-C10', 'C10-C11', 'C11-C12', 'R1-R2', 'R2-R3', 'R3-R4', 'R4-R5', 'R5-R6', 'R6-R7', 'R7-R8', 'R8-R9', 'R9-R10', 'R10-R11', 'R11-R12', 'R12-R13', 'L1-L2', 'L2-L3', 'L3-L4', 'L4-L5', 'L5-L6', 'L6-L7', 'L7-L8', 'L8-L9', 'L9-L10', 'O1-O2', 'O2-O3', 'O3-O4', 'O4-O5', 'O5-O6', 'O6-O7', 'O7-O8', 'Q1-Q2', 'Q2-Q3', 'Q3-Q4', 'Q4-Q5', 'Q5-Q6', 'Q6-Q7', 'Q7-Q8', 'Q8-Q9', 'Q9-Q10', 'T1-T2', 'T2-T3', 'T3-T4', 'T4-T5', 'T5-T6', 'T6-T7', 'T7-T8', 'T8-T9', 'T9-T10', 'D1-D2', 'D2-D3', 'D3-D4', 'D4-D6', 'D6-D7', 'D7-D8', 'D8-D9', 'E1-E2', 'E2-E3', 'E3-E4', 'E4-E5', 'E5-E6', 'E6-E8', 'E8-E9', 'E9-E10', 'E10-E11', 'J1-J2', 'J2-J3', 'J6-J7', 'J7-J8', 'J8-J9', 'J9-J10', 'J10-J11', 'J11-J12', 'I1-I2', 'I2-I3', 'I3-I4', 'I4-I5', 'I5-I6', 'I6-I7', 'I7-I8', 'P4-P5', 'P5-P6', 'P6-P7', 'P7-P8', 'P8-P9', 'P9-P10'],\n",
        "\"5\":[\"A'1-A'2\", \"A'2-A'3\", \"A'3-A'4\", \"A'4-A'5\", \"A'5-A'6\", \"A'6-A'7\", \"A'7-A'8\", \"A'8-A'9\", \"A'9-A'10\", \"B'1-B'2\", \"B'2-B'3\", \"B'3-B'4\", \"B'4-B'5\", \"B'5-B'6\", \"B'6-B'7\", \"B'7-B'8\", \"B'8-B'9\", \"B'9-B'10\", \"B'10-B'11\", \"B'11-B'12\", \"C'1-C'2\", \"C'2-C'3\", \"C'3-C'4\", \"C'4-C'5\", \"C'5-C'6\", \"C'6-C'7\", \"C'7-C'8\", \"C'8-C'9\", \"C'9-C'10\", \"C'10-C'11\", \"C'11-C'12\", \"D'1-D'2\", \"D'2-D'3\", \"D'3-D'4\", \"D'4-D'5\", \"D'5-D'6\", \"D'6-D'7\", \"D'7-D'8\", \"F'1-F'2\", \"F'2-F'3\", \"F'3-F'4\", \"F'4-F'5\", \"F'5-F'6\", \"F'6-F'7\", \"F'7-F'8\", \"H'1-H'2\", \"H'2-H'3\", \"H'3-H'4\", \"H'4-H'5\", \"G'1-G'2\", \"G'2-G'3\", \"G'3-G'4\", \"G'4-G'5\", \"G'5-G'6\", \"G'6-G'7\", \"G'7-G'8\", \"G'8-G'9\", \"G'9-G'10\", \"G'10-G'11\", \"G'11-G'12\"],\n",
        "\"6\":['A1-A2', 'A2-A3', 'A3-A4', 'A4-A5', 'A5-A6', 'A6-A7', 'A7-A8', 'B1-B2', 'B2-B3', 'B3-B4', 'B4-B5', 'B5-B6', 'B6-B7', 'B7-B8', 'B8-B9', 'B9-B10', 'B10-B11', 'B11-B12', 'C1-C2', 'C2-C3', 'C3-C4', 'C4-C5', 'C5-C6', 'C6-C7', 'C7-C8', 'C8-C9', 'C9-C10', 'C10-C11', 'C11-C12', 'D1-D2', 'D2-D3', 'D3-D4', 'D4-D5', 'D5-D6', 'D6-D7', 'D7-D8', 'D8-D9', 'D9-D10', 'D10-D11', 'D11-D12', 'E1-E2', 'E2-E3', 'E3-E4', 'E4-E5', 'E5-E6', 'E6-E7', 'E7-E8', 'E8-E9', 'E9-E10', 'E10-E11', 'E11-E12', 'F1-F2', 'F2-F3', 'F3-F4', 'F4-F5', 'F5-F6', 'F6-F7', 'F7-F8', 'F8-F9', 'F9-F10', 'F10-F11', 'F11-F12', 'G1-G2', 'G2-G3', 'G3-G4', 'G4-G5', 'H1-H2', 'H2-H3', 'H3-H4', 'H4-H5', 'H5-H6', 'H6-H7', 'H7-H8', 'H8-H9', 'H9-H10', 'H10-H11', 'H11-H12', 'I1-I2', 'I2-I3', 'I3-I4', 'I4-I5', 'I5-I6', 'I6-I7', 'I7-I8', 'I8-I9', 'I9-I10', 'I10-I11', 'I11-I12', \"C'1-C'2\", \"C'2-C'3\", \"C'3-C'4\", \"C'4-C'5\", \"C'5-C'6\", \"C'6-C'7\", \"C'7-C'8\", \"C'8-C'9\", \"C'9-C'10\"],\n",
        "\"7\":['A1-A2', 'A2-A3', 'A3-A4', 'A4-A5', 'A5-A6', 'A6-A7', 'A7-A8', 'B1-B2', 'B2-B3', 'B3-B4', 'B4-B5', 'B5-B6', 'B6-B7', 'B7-B8', 'B8-B9', 'B9-B10', 'B10-B11', 'B11-B12', 'C1-C2', 'C2-C3', 'C3-C4', 'C4-C5', 'C5-C6', 'C6-C7', 'C7-C8', 'C8-C9', 'C9-C10', \"C'1-C'2\", \"C'2-C'3\", \"C'3-C'4\", \"C'4-C'5\", \"C'5-C'6\", \"C'6-C'7\", \"C'7-C'8\", \"C'8-C'9\", \"C'9-C'10\", \"C'10-C'11\", \"C'11-C'12\", 'D1-D2', 'D2-D3', 'D3-D4', 'D4-D5', 'D5-D6', 'D6-D7', 'D7-D8', 'E1-E2', 'E2-E3', 'E3-E4', 'E4-E5', 'E5-E6', 'E6-E7', 'E7-E8', 'E8-E9', 'E9-E10', 'F1-F2', 'F2-F3', 'F3-F4', 'F4-F5', 'F5-F6', 'F6-F7', 'F7-F8', 'F8-F9', 'F9-F10', 'F10-F11', 'F11-F12', 'G1-G2', 'G2-G3', 'G3-G4', 'G4-G5', 'G5-G6', 'G6-G7', 'G7-G8', 'G8-G9', 'G9-G10', 'H1-H2', 'H2-H3', 'H3-H4', 'H4-H5', 'H5-H6', 'H6-H7', 'H7-H8', 'I1-I2', 'I2-I3', 'I3-I4', 'I4-I5', 'I5-I6', 'I6-I7', 'I7-I8', 'I8-I9', 'I9-I10', 'I10-I11', 'I11-I12', 'J1-J2', 'J2-J3', 'J3-J4', 'J4-J5', 'J5-J6', 'J6-J7', 'J7-J8'],\n",
        "\"8\":[\"F'1-F'2\", \"F'2-F'3\", \"F'8-F'9\", \"F'9-F'10\", \"F'10-F'11\", \"F'11-F'12\", \"T'1-T'2\", \"T'2-T'3\", \"T'3-T'4\", \"T'4-T'5\", \"T'5-T'6\", \"T'6-T'7\", \"T'7-T'8\", \"T'8-T'9\", \"T'9-T'10\", \"A'1-A'2\", \"A'2-A'3\", \"A'3-A'4\", \"A'4-A'5\", \"A'5-A'6\", \"A'6-A'7\", \"A'7-A'8\", \"A'8-A'9\", \"A'9-A'10\", \"A'10-A'11\", \"A'11-A'12\", \"B'1-B'2\", \"B'2-B'3\", \"B'3-B'4\", \"B'4-B'5\", \"B'8-B'9\", \"B'9-B'10\", \"B'10-B'11\", \"B'11-B'12\", \"C'1-C'2\", \"C'2-C'3\", \"C'3-C'4\", \"C'4-C'5\", \"C'5-C'6\", \"C'6-C'7\", \"C'7-C'8\", \"C'8-C'9\", \"C'9-C'10\", \"C'10-C'11\", \"D'1-D'2\", \"D'2-D'3\", \"D'3-D'4\", \"D'4-D'5\", \"D'5-D'6\", \"D'6-D'7\", \"D'7-D'8\", \"D'8-D'9\", \"D'9-D'10\", \"S'1-S'2\", \"S'2-S'3\", \"S'5-S'6\", \"S'6-S'7\", \"S'7-S'8\", \"S'8-S'9\", \"S'9-S'10\", \"S'10-S'11\", \"S'11-S'12\", \"S'12-S'13\", \"S'13-S'14\", \"I'1-I'2\", \"I'2-I'3\", \"I'3-I'4\", \"I'4-I'5\", \"I'5-I'6\", \"I'6-I'7\", \"I'7-I'8\", \"I'8-I'9\", \"I'9-I'10\", \"W'1-W'2\", \"W'2-W'3\", \"W'10-W'11\", \"W'11-W'12\", \"W'12-W'13\", \"W'13-W'14\", \"W'14-W'15\", \"W'15-W'16\", \"W'16-W'17\", \"W'17-W'18\", \"U'1-U'2\", \"U'2-U'3\", \"U'3-U'4\", \"U'4-U'5\", \"U'8-U'9\", \"U'9-U'10\", \"U'10-U'11\", \"U'11-U'12\", \"U'12-U'13\", \"U'13-U'14\", \"U'14-U'15\", \"O'1-O'2\", \"O'2-O'3\", \"O'3-O'4\", \"O'4-O'5\", \"O'5-O'6\", \"O'6-O'7\", \"O'7-O'8\", \"O'8-O'9\", \"O'9-O'10\", \"O'10-O'11\", \"O'11-O'12\", \"O'12-O'13\", \"O'13-O'14\", \"O'14-O'15\"],\n",
        "\"9\":['FC1-FC2', 'FC2-FC3', 'FC3-FC4', 'FC4-FC5', 'FC5-FC6', 'FC6-FC7', 'FC7-FC8', 'FC8-FC9', 'FC9-FC10', 'A1-A2', 'A2-A3', 'A3-A4', 'A4-A5', 'A5-A6', 'A6-A7', 'A7-A8', 'A8-A9', 'A9-A10', 'HAn1-HAn2', 'HAn2-HAnt3', 'HAnt3-HAnt4', 'HAnt4-HAnt5', 'HAnt5-HAnt6', 'HAnt6-HAnt7', 'HAnt7-HAnt8', 'HAnt8-HAnt9', 'HAnt9-HAnt10', 'HAnt10-Ref', 'HAnt11-Ref', 'HP1-HP2', 'HP2-HP3', 'HP3-HP4', 'HP4-HP5', 'HP5-HP6', 'HP6-HP7', 'HP7-HP8', 'HP8-HP9', 'HP9-HP10', 'TB1-TB2', 'TB2-TB3', 'TB3-TB4', 'TB4-TB5', 'TB5-TB6', 'TB6-TB7', 'TB7-TB8', 'TB8-TB9', 'TB9-TB10', 'TB10-TB11', 'TB11-TB12'],\n",
        "\"10\":['F1-F2', 'F2-F3', 'F3-F4', 'F4-F5', 'F5-F6', 'F6-F7', 'F7-F8', 'F8-F9', 'F9-F10', 'F10-F11', 'F11-F12', 'F12-F13', 'F13-F14', 'F14-F15', 'F15-F16', 'F16-F17', 'A1-A3', 'A3-A5', 'A5-A7', 'A7-A9', 'A9-A11', 'A11-A13', 'A13-A15', 'B1-B3', 'B3-B5', 'B5-B7', 'B7-B9', 'C1-C3', 'C3-C5', 'C5-C7', 'C7-C9', 'C9-C11', 'D1-D3', 'D3-D5', 'D5-D7', 'D7-D9', 'D9-D11', 'D11-D13', 'D13-D15', 'E1-E3', 'E3-E5', 'E5-E7', 'E7-E9', 'E9-E11', 'J1-J3', 'J3-J5', 'J5-J7', 'J7-J9', 'J9-J11', 'J11-J13', 'J13-J15', 'J15-J17', 'K1-K3', 'K3-K5', 'K5-K7', 'K7-K9', 'K9-K11', 'K11-K13', 'K13-K15', 'L1-L3', 'L3-L5', 'L5-L7', 'L7-L9', 'L9-L11', 'L11-L13', 'L13-L15', 'M1-M3', 'M3-M5', 'M5-M7', 'M7-M9', 'O1-O3', 'O3-O5', 'O5-O7', 'P1-P3', 'P3-P5', 'P5-P7', 'P7-P9', 'R1-R3', 'R3-R5', 'R5-R7', 'R7-R9', 'R9-R11', 'R11-R13', 'R13-R15', 'S1-S3', 'S3-S5', 'S5-S7', 'S7-S9', 'S9-S11', 'S11-S13', 'S13-S15', 'T1-T3', 'T3-T5', 'T5-T9', 'T9-T11'],\n",
        "\"11\":[\"R'1-R'2\", \"R'2-R'3\", \"R'9-R'10\", \"R'10-R'11\", \"R'11-R'12\", \"R'12-R'13\", \"R'13-R'14\", \"S'1-S'2\", \"S'2-S'3\", \"S'6-S'7\", \"P'1-P'2\", \"P'2-P'3\", \"P'3-P'4\", \"P'4-P'5\", \"P'7-P'8\", \"P'8-P'9\", \"M'1-M'2\", \"M'8-M'9\", \"M'9-M'10\", \"M'10-M'11\", \"M'13-M'14\", \"J'1-J'2\", \"J'7-J'8\", \"J'8-J'9\", \"J'9-J'10\", \"K'1-K'2\", \"K'2-K'3\", \"K'7-K'8\", \"K'8-K'9\", \"K'9-K'10\", \"L'2-L'3\", \"L'3-L'4\", \"L'4-L'5\", \"L'5-L'6\", \"B'1-B'2\", \"B'5-B'6\", \"B'6-B'7\", \"B'7-B'8\", \"B'8-B'9\", \"C'1-C'2\", \"C'2-C'3\", \"C'9-C'10\", \"C'10-C'11\", \"C'11-C'12\", \"O'4-O'5\", \"O'5-O'6\", \"O'6-O'7\", \"O'10-O'11\", \"O'11-O'12\", \"O'12-O'13\", \"Q'3-Q'4\", \"Q'4-Q'5\", \"Q'5-Q'6\", \"Q'6-Q'7\", \"Q'10-Q'11\", \"Q'11-Q'12\", 'S1-S2', 'S8-S9', 'S9-S10', 'S10-S11', 'P1-P2', 'P2-P3', 'P6-P7', 'P7-P8', 'K2-K3', 'K3-K4', 'K4-K5', 'L1-L2', 'L2-L3', 'L3-L4', 'L7-L8', 'J1-J2', 'J11-J12', 'J12-J13', 'J13-J14', 'C1-C2', 'C2-C3', 'C8-C9', 'C9-C10', 'Q1-Q2', 'Q2-Q3', 'Q3-Q4', 'Q12-Q13', 'Q13-Q14', 'O1-O2', 'O2-O3', 'O3-O4', 'O4-O5', 'O9-O10'],\n",
        "\"12\":[\"E'1-E'2\", \"E'2-E'3\", \"E'3-E'4\", \"E'4-E'5\", \"E'5-E'6\", \"E'6-E'7\", \"E'7-E'8\", \"E'10-E'11\", \"O'1-O'2\", \"O'2-O'3\", \"O'3-O'4\", \"O'4-O'5\", \"O'5-O'6\", \"O'6-O'7\", \"O'7-O'8\", \"O'8-O'9\", \"O'9-O'10\", \"O'10-O'11\", \"C'1-C'2\", \"C'2-C'3\", \"C'6-C'7\", \"C'7-C'8\", \"C'8-C'9\", \"C'9-C'10\", \"C'10-C'11\", \"T'1-T'2\", \"T'2-T'3\", \"T'8-T'9\", \"T'9-T'10\", \"T'10-T'11\", \"U'1-U'2\", \"U'4-U'5\", \"U'5-U'6\", \"U'8-U'9\", \"Q'1-Q'2\", \"Q'10-Q'11\", \"Q'11-Q'12\", \"Q'12-Q'13\", \"Q'13-Q'14\", \"Q'14-Q'15\", \"W'1-W'2\", \"W'6-W'7\", \"W'7-W'8\", \"W'8-W'9\", \"W'12-W'13\", \"W'13-W'14\", \"S'1-S'2\", \"S'8-S'9\", \"S'9-S'10\", \"S'10-S'11\", \"S'11-S'12\", \"P'1-P'2\", \"P'2-P'3\", \"P'3-P'4\", \"P'4-P'5\", \"P'5-P'6\", \"P'6-P'7\", \"P'7-P'8\", \"P'8-P'9\", 'T1-T2', 'T2-T3', 'T3-T4', 'T4-T5', 'T5-T6', 'T11-T12', 'O1-O2', 'O2-O3', 'O5-O6', 'O6-O7', 'O7-O8', 'O8-O9', 'O9-O10', 'O10-O11', 'U1-U2', 'U5-U6', 'U6-U7', 'W1-W2', 'W2-W3', 'W3-W4', 'W9-W10', 'W10-W11', 'W11-W12', 'W12-W13', 'Q1-Q2', 'Q2-Q3', 'Q6-Q7', 'Q7-Q8', 'Q13-Q14', 'Q14-Q15', 'P1-P2', 'P4-P5', 'P5-P6', 'P6-P7', 'P10-P11'],\n",
        "\"13\":['A1-A2', 'A2-A3', 'A3-A4', 'A4-A5', 'A5-A6', 'A6-A7', 'A7-A8', 'A8-A9', 'A9-A10', 'A10-A11', 'A11-A12', 'B1-B2', 'B2-B3', 'B3-B4', 'B4-B5', 'B5-B6', 'B6-B7', 'B7-B8', 'B8-B9', 'B9-B10', 'B10-B11', 'B11-B12', 'B12-B13', 'B13-B14', 'B14-B15', 'C1-C2', 'C2-C3', 'C3-C4', 'C4-C5', 'C5-C6', 'C6-C7', 'C7-C8', 'C8-C9', 'C9-C10', 'C10-C11', 'C11-C12', 'D1-D2', 'D2-D3', 'D3-D4', 'D4-D5', 'E1-E2', 'E2-E3', 'E3-E4', 'E4-E5', 'F1-F2', 'F2-F3', 'F3-F4', 'F4-F5', 'F5-F6', 'F6-F7', 'F7-F8', 'F8-F9', 'F9-F10', 'F10-F11', 'F11-F12', 'G1-G2', 'G2-G3', 'G3-G4', 'G4-G5', 'G5-G6', 'G6-G7', 'G7-G8', 'H1-H2', 'H2-H3', 'H3-H4', 'H4-H5', 'H5-H6', 'H6-H7', 'H7-H8', 'H8-H9', 'H9-H10', 'H10-H11', 'H11-H12', 'I1-I2', 'I2-I3', 'I3-I4', 'I4-I5', 'I5-I6', 'I6-I7', 'I7-I8', 'I8-I9', 'I9-I10', 'L1-L2', 'L2-L3', 'L3-L4', 'L4-L5', 'L5-L6', 'L6-L7', 'L7-L8'],\n",
        "\"14\":['A1-A2', 'A2-A3', 'A3-A4', 'A4-A5', 'A5-A6', 'A6-A7', 'B1-B2', 'B2-B3', 'B3-B4', 'B4-B5', 'B5-B6', 'B6-B7', 'B7-B8', 'B8-B9', 'B9-B10', 'B10-B11', 'C1-C2', 'C2-C3', 'C3-C4', 'C4-C5', 'C5-C6', 'C6-C7', 'C7-C8', 'C8-C9', 'C9-C10', 'D1-D2', 'D2-D3', 'D3-D4', 'D4-D5', 'D5-D6', 'D6-D7', 'D7-D8', 'D8-D9', 'D9-D10', 'D10-D11', 'E1-E2', 'E2-E3', 'E3-E4', 'E4-E5', 'E5-E6', 'E6-E7', 'E7-E8', 'E8-E9', 'F1-F2', 'F2-F3', 'F3-F4', 'F4-F5', 'F5-F6', 'F6-F7', 'F7-F8', 'F8-F9', 'F9-F10', 'G1-G2', 'G2-G3', 'G3-G4', 'G4-G5', 'H1-H2', 'H2-H3', 'H3-H4', 'H4-H5', 'H5-H6', 'H6-H7', 'H7-H8', \"G'1-G'2\", \"G'2-G'3\", \"G'3-G'4\", \"G'4-G'5\", \"C'1-C'2\", \"C'2-C'3\", \"C'3-C'4\", \"C'4-C'5\", \"C'5-C'6\", \"C'6-C'7\", \"C'7-C'8\", \"C'8-C'9\", \"C'9-C'10\"],\n",
        "\"15\":[\"A'1-A'2\", \"A'2-A'3\", \"A'6-A'7\", \"A'7-A'8\", \"B'1-B'2\", \"B'2-B'3\", \"B'3-B'4\", \"B'4-B'5\", \"B'5-B'6\", \"B'6-B'7\", \"B'7-B'8\", \"C'1-C'2\", \"C'6-C'7\", \"C'7-C'8\", \"C'8-C'9\", \"C'9-C'10\", \"E'1-E'2\", \"E'2-E'3\", \"E'5-E'6\", \"E'6-E'7\", \"E'7-E'8\", \"E'8-E'9\", \"T'1-T'2\", \"T'2-T'3\", \"T'3-T'4\", \"T'8-T'9\", \"T'9-T'10\", \"T'10-T'11\", \"T'11-T'12\", \"T'12-T'13\", \"T'13-T'14\", \"T'14-T'15\", \"J'1-J'2\", \"J'2-J'3\", \"J'3-J'4\", \"J'4-J'5\", \"J'12-J'13\", \"J'13-J'14\", \"J'14-J'15\", \"I'1-I'2\", \"I'2-I'3\", \"I'3-I'4\", \"I'8-I'9\", \"I'9-I'10\", \"I'10-I'11\", \"O'1-O'2\", \"O'6-O'7\", \"O'7-O'8\", \"O'8-O'9\", \"O'9-O'10\", \"O'10-O'11\", \"G'1-G'2\", \"G'2-G'3\", \"G'10-G'11\", \"G'11-G'12\", \"G'12-G'13\", \"G'13-G'14\", \"G'14-G'15\", \"Q'1-Q'2\", \"Q'6-Q'7\", \"Q'7-Q'8\", \"Q'8-Q'9\", \"Q'11-Q'12\", \"Q'12-Q'13\", \"P'1-P'2\", \"P'2-P'3\", \"P'3-P'4\", \"P'4-P'5\", \"P'5-P'6\", \"P'6-P'7\", \"P'7-P'8\", \"P'8-P'9\", \"P'9-P'10\", \"U'1-U'2\", \"U'5-U'6\", \"U'6-U'7\", \"U'7-U'8\", \"U'8-U'9\", \"M'1-M'2\", \"M'2-M'3\", \"M'3-M'4\", \"M'4-M'5\", \"M'7-M'8\", \"M'8-M'9\", \"M'9-M'10\", \"M'10-M'11\", \"M'11-M'12\", 'F1-F2', 'F7-F8', 'F8-F9', 'F9-F10', 'F10-F11', 'M1-M2', 'M2-M3', 'M3-M4', 'M12-M13', 'M13-M14'],\n",
        "\"16\":[\"F'1-F'2\", \"F'5-F'6\", \"F'8-F'9\", \"F'9-F'10\", \"M'1-M'2\", \"M'8-M'9\", \"M'9-M'10\", \"M'10-M'11\", \"M'11-M'12\", \"M'12-M'13\", \"M'13-M'14\", \"O'4-O'5\", \"O'5-O'6\", \"O'6-O'7\", \"Y'1-Y'2\", \"Y'2-Y'3\", \"Y'3-Y'4\", \"Y'4-Y'5\", \"Y'5-Y'6\", \"Y'6-Y'7\", \"Y'13-Y'14\", \"Y'14-Y'15\", \"Y'15-Y'16\", \"Y'16-Y'17\", \"I'1-I'2\", \"I'5-I'6\", \"I'6-I'7\", \"I'7-I'8\", \"U'1-U'2\", \"U'2-U'3\", \"U'3-U'4\", \"U'4-U'5\", \"U'5-U'6\", \"A'1-A'2\", \"A'7-A'8\", \"A'8-A'9\", \"A'9-A'10\", \"A'10-A'11\", \"B'1-B'2\", \"B'2-B'3\", \"B'8-B'9\", \"B'9-B'10\", \"B'10-B'11\", \"C'1-C'2\", \"C'2-C'3\", \"C'5-C'6\", \"C'6-C'7\", \"C'7-C'8\", \"C'10-C'11\", \"C'11-C'12\", \"D'1-D'2\", \"D'6-D'7\", \"D'7-D'8\", \"D'8-D'9\", \"E'2-E'3\", \"E'3-E'4\", \"E'4-E'5\", \"E'5-E'6\", \"E'6-E'7\", \"E'7-E'8\", \"E'8-E'9\", \"E'9-E'10\", \"E'10-E'11\", \"J'1-J'2\", \"J'8-J'9\", \"J'9-J'10\", \"J'10-J'11\", \"J'11-J'12\", \"J'12-J'13\", \"J'13-J'14\", \"J'14-J'15\", \"W'1-W'2\", \"W'11-W'12\", \"W'12-W'13\", \"W'13-W'14\", \"Q'1-Q'2\", \"Q'2-Q'3\", \"Q'6-Q'7\", \"Q'7-Q'8\", \"Q'8-Q'9\", \"Q'9-Q'10\", \"Q'10-Q'11\", 'A1-A2', 'A2-A3', 'A8-A9', 'A9-A10', 'A10-A11', 'B1-B2', 'B7-B8', 'B8-B9', 'C1-C2', 'C7-C8', 'C8-C9'],\n",
        "\"17\":['A1-A2', 'A2-A3', 'A3-A4', 'A4-A5', 'B1-B2', 'B2-B3', 'B3-B4', 'B4-B5', 'B5-B6', 'B6-B7', 'B7-B8', 'B8-B9', 'B9-B10', 'B10-B11', 'B11-B12', 'C1-C2', 'C2-C3', 'C3-C4', 'C4-C5', 'C5-C6', 'C6-C7', 'C7-C8', 'C8-C9', 'C9-C10', 'C10-C11', 'C11-C12', 'D1-D2', 'D2-D3', 'D3-D4', 'D4-D5', 'D5-D6', 'D6-D7', 'D7-D8', 'D8-D9', 'D9-D10', 'D10-D11', 'D11-D12', 'G1-G2', 'G2-G3', 'G3-G4', 'G4-G5', 'G5-G6', 'G6-G7', 'G7-G8', 'G8-G9', 'G9-G10', 'E1-E2', 'E2-E3', 'E3-E4', 'E4-E5', 'E5-E6', 'E6-E7', 'E7-E8', 'E8-E9', 'E9-E10', 'F1-F2', 'F2-F3', 'F3-F4', 'F4-F5', 'F5-F6', 'F6-F7', 'F7-F8', 'F8-F9', 'F9-F10', 'F10-F11', 'F11-F12', 'H1-H2', 'H2-H3', 'H3-H4', 'H4-H5', 'H5-H6', 'H6-H7', 'H7-H8', 'H8-H9', 'H9-H10', 'J1-J2', 'J2-J3', 'J3-J4', 'J4-J5', 'J5-J6', 'J6-J7', 'J7-J8', 'J8-J9', 'J9-J10', 'J10-J11', 'J11-J12', 'O1-O2', 'O2-O3', 'O3-O4', 'O4-O5', 'O5-O6', 'O6-O7', 'O7-O8', 'O8-O9', 'O9-O10', 'O10-O11', 'O11-O12', 'I1-I2', 'I2-I3', 'I3-I4', 'I4-I5', 'I5-I6', 'I6-I7', 'I7-I8', 'I8-I9', 'I9-I10', 'I10-I11', 'I11-I12', 'I12-I13', 'I13-I14', 'I14-I15', 'I15-I16', 'I16-I17', 'I17-I18'],\n",
        "\"18\":['F1-F2', 'F2-F3', 'F3-F4', 'F4-F5', 'F5-F6', 'F6-F7', 'F7-F8', 'S1-S2', 'S2-S3', 'S3-S4', 'S4-S5', 'S5-S6', 'S6-S7', 'S7-S8', 'S8-S9', 'S9-S10', 'S10-S11', 'S11-S12', 'H1-H2', 'H2-H3', 'H3-H4', 'H4-H5', 'H5-H6', 'H6-H7', 'H7-H8', 'M1-M2', 'M2-M3', 'M3-M4', 'M4-M5', 'M5-M6', 'M6-M7', 'M7-M8', 'U1-U2', 'U2-U3', 'U3-U4', 'U4-U5', 'U5-U6', 'U6-U7', 'U7-U8', 'A1-A2', 'A2-A3', 'A3-A4', 'A4-A5', 'A5-A6', 'A6-A7', 'A7-A8', 'A8-A9', 'A9-A10', 'A10-A11', 'A11-A12', 'A12-A13', 'A13-A14', 'A14-A15', 'B1-B2', 'B2-B3', 'B3-B4', 'B4-B5', 'B5-B6', 'B6-B7', 'B7-B8', 'B8-B9', 'B9-B10', 'J1-J2', 'J2-J3', 'J3-J4', 'J4-J5', 'J5-J6', 'J6-J7', 'J7-J8', 'J8-J9', 'J9-J10', 'O1-O2', 'O2-O3', 'O3-O4', 'O4-O5', 'O5-O6', 'O6-O7', 'O7-O8', 'O8-O9', 'O9-O10', 'O10-O11', 'O11-O12', 'O12-O13', 'O13-O14', 'O14-O15', 'T1-T2', 'T2-T3', 'T3-T4', 'T4-T5', 'T5-T6', 'T6-T7', 'T7-T8', 'T8-T9', 'T9-T10', 'L1-L2', 'L2-L3', 'L3-L4', 'L4-L5', 'L5-L6', 'L6-L7', 'L7-L8', 'L8-L9', 'L9-L10', 'P1-P2', 'P2-P3', 'P3-P4', 'P4-P5', 'P5-P6', 'P6-P7', 'P7-P8', 'P8-P9', 'P9-P10', 'P10-P11', 'P11-P12', 'P12-P13', 'P13-P14', 'P14-P15', 'Q1-Q2', 'Q2-Q3', 'Q3-Q4', 'Q4-Q5', 'Q5-Q6', 'Q6-Q7', 'Q7-Q8', 'Q8-Q9', 'Q9-Q10', 'Q10-Q11', 'Q11-Q12', 'Q12-Q13', 'Q13-Q14', 'Q14-Q15', 'X1-X2', 'X2-X3', 'X3-X4', 'X4-X5', 'X5-X6', 'X6-X7', 'X7-X8', 'X8-X9', 'X9-X10', 'C1-C2', 'C2-C3', 'C3-C4', 'C4-C5', 'C5-C6', 'C6-C7', 'C7-C8', 'Y1-Y2', 'Y2-Y3', 'Y3-Y4', 'Y4-Y5', 'Y5-Y6', 'Y6-Y7', 'Y7-Y8', 'Z1-Z2', 'Z2-Z3', 'Z3-Z4', 'Z4-Z5', 'Z5-Z6', 'Z6-Z7', 'Z7-Z8', 'Au1-Ref Au', 'Au2-Ref Au'],\n",
        "\"19\":['A1-A2', 'A2-A3', 'A3-A4', 'A4-A5', 'A5-A6', 'A6-A7', 'A7-A8', 'B1-B2', 'B2-B3', 'B3-B4', 'B4-B5', 'B5-B6', 'B6-B7', 'B7-B8', 'B8-B9', 'B9-B10', 'B10-B11', 'B11-B12', 'C1-C2', 'C2-C3', 'C3-C4', 'C4-C5', 'C5-C6', 'C6-C7', 'C7-C8', 'C8-C9', 'C9-C10', 'C10-C11', 'C11-C12', 'D1-D2', 'D2-D3', 'D3-D4', 'D4-D5', 'D5-D6', 'D6-D7', 'D7-D8', 'D8-D9', 'D9-D10', 'E1-E2', 'E2-E3', 'E3-E4', 'E4-E5', 'E5-E6', 'E6-E7', 'E7-E8', 'E8-E9', 'E9-E10', 'E10-E11', 'E11-E12', 'F1-F2', 'F2-F3', 'F3-F4', 'F4-F5'],\n",
        "\"20\":['A1-A2', 'A2-A3', 'A6-A7', 'A7-A8', 'A8-A9', 'A10-A11', 'A11-A12', 'B1-B2', 'B2-B3', 'B3-B4', 'B7-B8', 'B8-B9', 'B9-B10', 'B10-B11', 'C1-C2', 'C2-C3', 'C3-C4', 'C4-C5', 'C5-C6', 'C6-C7', 'C7-C8', 'C8-C9', 'C9-C10', 'C10-C11', 'C11-C12', 'D1-D2', 'D2-D3', 'D3-D4', 'D4-D5', 'D5-D6', 'D6-D7', 'D7-D8', 'D8-D9', 'D9-D10', 'D10-D11', 'D11-D12', 'J1-J2', 'J2-J3', 'J3-J4', 'J8-J9', 'J9-J10', 'J10-J11', 'J11-J12', 'J12-J13', 'J13-J14', 'J14-J15', 'K1-K2', 'K2-K3', 'K3-K4', 'K4-K5', 'K5-K6', 'K6-K7', 'K7-K8', 'K8-K9', 'K9-K10', 'K10-K11', 'K11-K12', 'K12-K13', 'K13-K14', 'P1-P2', 'P2-P3', 'P3-P4', 'P4-P5', 'P5-P6', 'P6-P7', 'P7-P8', 'P8-P9', 'P9-P10', 'P10-P11', 'P11-P12', 'O1-O2', 'O2-O3', 'O3-O4', 'O4-O5', 'O5-O6', 'O6-O7', 'O7-O8', 'F3-F4', 'F4-F5', 'F5-F6', 'F6-F7', 'F9-F10', 'F10-F11', 'F11-F12', \"A'1-A'2\", \"A'2-A'3\", \"A'3-A'4\", \"A'4-A'5\", \"A'8-A'9\", \"A'9-A'10\", \"A'10-A'11\", \"A'11-A'12\", \"B'1-B'2\", \"B'2-B'3\", \"B'3-B'4\", \"B'7-B'8\", \"B'8-B'9\", \"B'9-B'10\", \"C'1-C'2\", \"C'2-C'3\", \"C'3-C'4\", \"C'4-C'5\", \"C'5-C'6\", \"C'6-C'7\", \"C'7-C'8\", \"C'8-C'9\", \"C'9-C'10\"],\n",
        "\"21\":['X1-X2', 'X2-X3', 'X3-X4', 'X4-X5', 'X5-X6', 'X6-X7', 'X7-X8', 'X10-X11', 'X11-X12', 'X12-X13', 'X13-X14', 'X14-X15', 'X15-X16', 'X16-X17', 'X17-X18', 'I1-I2', 'I4-I5', 'I5-I6', 'I8-I9', 'I9-I10', 'A1-A2', 'A2-A3', 'A3-A4', 'A7-A8', 'A8-A9', 'A9-A10', 'A10-A11', 'T1-T2', 'T2-T3', 'T5-T6', 'T6-T7', 'T7-T8', 'T8-T9', 'E1-E2', 'E2-E3', 'E3-E4', 'E4-E5', 'E6-E7', 'E7-E8', 'E8-E9', 'B1-B2', 'B2-B3', 'B3-B4', 'B8-B9', 'B9-B10', 'B10-B11', 'C1-C2', 'C2-C3', 'C3-C4', 'C7-C8', 'C8-C9', 'C9-C10', 'C10-C11', 'D1-D2', 'D2-D3', 'D3-D4', 'D4-D5', 'D8-D9', 'D9-D10', 'D10-D11', 'S1-S2', 'S2-S3', 'S3-S4', 'S4-S5', 'S12-S13', 'S13-S14', 'S14-S15', 'S15-S16', 'S16-S17', 'Q1-Q2', 'Q2-Q3', 'Q3-Q4', 'Q9-Q10', 'Q10-Q11', 'Q11-Q12', 'Q12-Q13', 'Q13-Q14', 'O1-O2', 'O2-O3', 'O3-O4', 'O4-O5', 'O5-O6', 'O6-O7', 'O7-O8', 'O8-O9', 'O9-O10', 'O10-O11', 'O11-O12', 'P1-P2', 'P2-P3', 'P3-P4', 'P4-P5', 'P5-P6', 'P11-P12', 'P12-P13', 'P13-P14', 'P14-P15']\n",
        "}\n",
        "\n",
        "RESECTION = {1: ['T9-T10', 'T4-T5', 'B6-B7', 'B2-B3', 'D4-D5', 'D2-D3', 'T6-T7', 'B1-B2', 'T10-T11', 'T11-T12', 'T3-T4', 'T8-T9', 'T7-T8', 'A3-A4', 'T1-T2', 'T5-T6', 'T2-T3', 'B5-B6', 'D3-D4'], 2: ['A1-A2', 'B2-B3', 'B6-B7', 'A2-A3', 'B1-B2', 'C2-C3', 'C7-C8', 'B8-B9', 'B4-B5', 'C9-C10', 'A3-A4', 'C3-C4', 'B7-B8', 'C1-C2', 'C5-C6', 'C8-C9', 'A4-A5', 'C6-C7', 'C4-C5', 'B5-B6', 'B9-B10', 'B3-B4'], 3: [\"D'8-D'9\", \"D'11-D'12\", \"W'14-W'15\", \"P'12-P'13\", \"L'9-L'10\", \"L'11-L'12\", \"O'9-O'10\", \"O'10-O'11\", \"K'13-K'14\", \"O'11-O'12\", \"D'6-D'7\", \"L'10-L'11\", \"P'13-P'14\", \"D'9-D'10\", \"W'13-W'14\", \"P'14-P'15\", \"K'12-K'13\", \"D'7-D'8\"], 4: ['A10-A11', 'P7-P8', 'P9-P10', 'O4-O5', 'Q8-Q9', 'E2-E3', 'E9-E10', 'P6-P7', 'L1-L2', 'E4-E5', 'L4-L5', 'Q7-Q8', 'J7-J8', 'C9-C10', 'J8-J9', 'O6-O7', 'P8-P9', 'C11-C12', 'E3-E4', 'I6-I7', 'J9-J10', 'B11-B12', 'L2-L3', 'O5-O6', 'O3-O4', 'J10-J11', 'Q3-Q4', 'C10-C11', 'E10-E11', 'P5-P6', 'J6-J7', 'J11-J12', 'J1-J2', 'Q4-Q5', 'I7-I8', 'A9-A10', 'L3-L4', 'Q9-Q10'], 5: [\"D'4-D'5\", \"D'2-D'3\", \"D'3-D'4\", \"D'1-D'2\", \"B'1-B'2\", \"D'6-D'7\", \"D'5-D'6\", \"D'7-D'8\"], 6: ['C1-C2', 'C4-C5', 'C2-C3', 'C3-C4'], 7: ['F11-F12', 'G5-G6', 'E1-E2', 'F8-F9', 'A1-A2', 'B2-B3', 'B6-B7', 'A2-A3', 'D4-D5', 'D2-D3', 'E2-E3', 'E9-E10', 'G7-G8', 'C2-C3', 'G9-G10', 'G2-G3', 'E4-E5', 'G6-G7', 'A5-A6', 'D1-D2', 'G8-G9', 'C7-C8', 'E8-E9', 'I8-I9', 'B8-B9', 'B4-B5', 'F7-F8', 'F9-F10', 'H7-H8', 'J7-J8', 'A7-A8', 'C9-C10', 'D5-D6', 'H4-H5', 'H5-H6', 'D7-D8', 'B10-B11', 'A3-A4', 'E6-E7', 'G1-G2', 'E3-E4', 'A6-A7', 'B7-B8', 'C1-C2', 'C3-C4', 'C5-C6', 'F10-F11', 'G3-G4', 'H2-H3', 'H3-H4', 'B11-B12', 'E5-E6', 'D6-D7', 'C8-C9', 'I11-I12', 'J6-J7', 'F6-F7', 'A4-A5', 'C6-C7', 'C4-C5', 'E7-E8', 'G4-G5', 'I7-I8', 'B5-B6', 'D3-D4', 'B9-B10', 'I9-I10', 'I10-I11', 'B3-B4', 'H6-H7'], 8: [\"T'7-T'8\", \"D'2-D'3\", \"A'3-A'4\", \"T'6-T'7\", \"A'6-A'7\", \"D'4-D'5\", \"A'7-A'8\", \"T'2-T'3\", \"A'1-A'2\", \"A'5-A'6\", \"A'8-A'9\", \"D'5-D'6\", \"A'2-A'3\", \"D'8-D'9\", \"A'4-A'5\", \"T'5-T'6\", \"D'3-D'4\", \"T'4-T'5\", \"D'7-D'8\", \"T'1-T'2\", \"T'3-T'4\", \"D'1-D'2\"], 9: ['A3-A4', 'TB5-TB6', 'A8-A9', 'TB4-TB5', 'A4-A5', 'A6-A7', 'TB6-TB7', 'A1-A2', 'A2-A3', 'A7-A8', 'TB2-TB3', 'A9-A10', 'TB1-TB2', 'A5-A6', 'TB3-TB4'], 10: ['F11-F12', 'F6-F7', 'F4-F5', 'F13-F14', 'F5-F6', 'F12-F13', 'F1-F2', 'F8-F9', 'F7-F8', 'O1-O3', 'F2-F3', 'T3-T5', 'O3-O5', 'F3-F4', 'T5-T9'], 11: [\"O'5-O'6\", \"O'4-O'5\", \"M'1-M'2\", \"J'8-J'9\", \"Q'5-Q'6\", \"P'4-P'5\", \"K'9-K'10\", \"P'7-P'8\", \"P'1-P'2\", \"O'6-O'7\", \"Q'4-Q'5\", \"S'6-S'7\", \"K'8-K'9\", \"R'2-R'3\", \"J'7-J'8\", \"J'9-J'10\", \"P'8-P'9\", \"Q'3-Q'4\", \"K'1-K'2\", \"K'7-K'8\", \"J'1-J'2\"], 12: ['O10-O11'], 13: ['F11-F12', 'F4-F5', 'I3-I4', 'F8-F9', 'B2-B3', 'B6-B7', 'F2-F3', 'I2-I3', 'B1-B2', 'C2-C3', 'C7-C8', 'I8-I9', 'B4-B5', 'F7-F8', 'F9-F10', 'H4-H5', 'H5-H6', 'I4-I5', 'F3-F4', 'I1-I2', 'I5-I6', 'F5-F6', 'C3-C4', 'B7-B8', 'H8-H9', 'I6-I7', 'C5-C6', 'F10-F11', 'H3-H4', 'C8-C9', 'F6-F7', 'C6-C7', 'C4-C5', 'I7-I8', 'B5-B6', 'H7-H8', 'B3-B4', 'H6-H7'], 14: ['E6-E7', 'E8-E9', 'E1-E2', 'E7-E8', 'E5-E6'], 15: [\"J'13-J'14\", \"T'1-T'2\", \"J'2-J'3\", \"J'12-J'13\", \"T'2-T'3\", \"T'14-T'15\", \"T'13-T'14\", \"J'1-J'2\", \"J'14-J'15\", \"J'3-J'4\"], 16: [\"B'9-B'10\", \"D'8-D'9\", \"E'2-E'3\", \"A'7-A'8\", \"D'1-D'2\", \"B'1-B'2\", \"A'1-A'2\", \"A'10-A'11\", \"D'6-D'7\", \"A'8-A'9\", \"B'10-B'11\", \"A'9-A'10\", \"D'7-D'8\"], 17: ['I3-I4', 'E1-E2', 'A1-A2', 'B2-B3', 'B6-B7', 'A2-A3', 'E2-E3', 'I2-I3', 'B1-B2', 'C2-C3', 'E4-E5', 'B8-B9', 'B4-B5', 'I4-I5', 'I1-I2', 'B10-B11', 'A3-A4', 'E6-E7', 'E3-E4', 'C3-C4', 'B7-B8', 'C1-C2', 'C5-C6', 'E5-E6', 'C6-C7', 'C4-C5', 'B5-B6', 'B9-B10', 'B3-B4'], 18: ['A10-A11', 'P7-P8', 'P9-P10', 'P4-P5', 'S3-S4', 'O13-O14', 'Q8-Q9', 'T4-T5', 'P6-P7', 'T6-T7', 'Y6-Y7', 'Q10-Q11', 'X5-X6', 'T7-T8', 'Q7-Q8', 'X4-X5', 'H4-H5', 'A7-A8', 'H5-H6', 'P8-P9', 'A8-A9', 'F5-F6', 'A6-A7', 'H2-H3', 'S9-S10', 'J2-J3', 'S5-S6', 'H3-H4', 'C3-C4', 'C5-C6', 'S10-S11', 'Y3-Y4', 'A11-A12', 'Y4-Y5', 'X3-X4', 'Q12-Q13', 'Q13-Q14', 'T5-T6', 'H1-H2', 'P5-P6', 'Y2-Y3', 'Y7-Y8', 'Y5-Y6', 'F6-F7', 'X6-X7', 'C6-C7', 'C4-C5', 'J1-J2', 'Q9-Q10', 'S4-S5', 'O12-O13', 'H7-H8', 'O11-O12', 'Q11-Q12', 'P3-P4', 'H6-H7'], 19: ['A1-A2', 'B2-B3', 'B6-B7', 'A2-A3', 'B1-B2', 'C2-C3', 'A5-A6', 'D1-D2', 'B8-B9', 'B4-B5', 'B10-B11', 'A3-A4', 'C3-C4', 'B7-B8', 'C1-C2', 'C5-C6', 'B11-B12', 'A4-A5', 'C4-C5', 'B5-B6', 'B9-B10', 'B3-B4'], 20: ['A6-A7', 'A1-A2', 'B2-B3', 'A2-A3', 'A7-A8', 'B1-B2', 'B3-B4'], 21: ['A10-A11', 'E1-E2', 'A1-A2', 'B2-B3', 'A2-A3', 'E2-E3', 'T6-T7', 'E4-E5', 'D1-D2', 'T8-T9', 'E8-E9', 'T7-T8', 'A7-A8', 'B10-B11', 'A3-A4', 'E6-E7', 'A8-A9', 'E3-E4', 'T1-T2', 'T5-T6', 'T2-T3', 'E7-E8', 'A9-A10', 'B3-B4']}\n",
        "\n",
        "subs = list(range(1,22))\n",
        "poor_outcome_subjects = [3,6,10,11,12,17,18]\n",
        "good_outcome_subjects = [s for s in subs if s not in poor_outcome_subjects]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqXyFInlv1GM"
      },
      "source": [
        "Test Shannon entropy of all node pairs, grouped into EN-EN and EN-NN pairs. Define WOI and connectivity measure. We will plot the Shannon entropy for each group, in the non-seizure and WOI time. Keep in mind that, since there is normally (in most patients) more NN nodes than EN nodes, the EN-NN group will have much more data points than EN-EN group."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1teLNg8SQ_W5",
        "outputId": "84b27044-be48-41d9-be98-dac1a85465b1"
      },
      "outputs": [],
      "source": [
        "from itertools import combinations\n",
        "\n",
        "woi = \"preseizure1\"\n",
        "measure = \"CC-(0,4)\"\n",
        "\n",
        "# main_folder = f\"/content/gdrive/My Drive/epigame-folder/{woi}/\"\n",
        "\n",
        "main_folder = \"/media/kivi/ADATA HV100/epigame-folder/\"\n",
        "\n",
        "cm_folder = main_folder + \"connectivity_matrices/\"\n",
        "\n",
        "for sub in good_outcome_subjects:\n",
        "    print(f\"Subject {sub}\")\n",
        "\n",
        "    # Initializing lists for storing Shannon entropy for EN-EN (1-1) and EN-NN (1-0) connectivity\n",
        "    conn_11_1, conn_10_1, conn_11_2, conn_10_2 = [],[],[],[]\n",
        "\n",
        "    cm_filename = cm_folder + f\"{sub}-{woi}-{measure}.prep\"\n",
        "    cm_load = REc.load(cm_filename)\n",
        "\n",
        "    # EN(1) and NN(0) nodes\n",
        "    nodes = NODES[str(sub)]\n",
        "    node_ids = list(range(len(nodes)))\n",
        "    resection_ids = [nodes.index(label) for label in RESECTION[sub]]\n",
        "    non_resection_ids = list(np.setdiff1d(node_ids, resection_ids))\n",
        "\n",
        "    # Iterate the upper triangular matrix without diagonal\n",
        "    cms = cm_load.data.X\n",
        "    rows = cms[0].shape[0]\n",
        "    cols = cms[0].shape[1]\n",
        "\n",
        "    n_epochs = int(len(cms)/2)\n",
        "\n",
        "    # Sort epochs by index\n",
        "    idx_cm_tuples = [(idx,cms[i]) for i,idx in enumerate(cm.i)]\n",
        "\n",
        "    idx_cm_1 = dict(sorted({idx:cm for (idx,cm) in idx_cm_tuples[:n_epochs]}.items()))\n",
        "    idx_cm_2 = dict(sorted({idx:cm for (idx,cm) in idx_cm_tuples[n_epochs::]}.items()))\n",
        "    epochs_1 = list(idx_cm_1.values())\n",
        "    epochs_2 = list(idx_cm_2.values())\n",
        "\n",
        "    for node1 in range(rows):\n",
        "        for node2 in range(node1 + 1, cols):  # Start from node1 + 1 to exclude the diagonal\n",
        "\n",
        "            # List of connectivity measures for all epochs for node1-node2\n",
        "            conn_1 = [get_connectivity(cm, node1, node2) for cm in epochs_1]\n",
        "            conn_2 = [get_connectivity(cm, node1, node2) for cm in epochs_2]\n",
        "\n",
        "            # Shannon entropy of the epochs\n",
        "            entropy_1 = entropy(conn_1, base=2)\n",
        "            entropy_2 = entropy(conn_2, base=2)\n",
        "\n",
        "            if node1 in resection_ids or node2 in resection_ids:\n",
        "              if node1 in resection_ids and node2 in resection_ids:\n",
        "                conn_11_1.append(entropy_1)\n",
        "                conn_11_2.append(entropy_2)\n",
        "              else:\n",
        "                conn_10_1.append(entropy_1)\n",
        "                conn_10_2.append(entropy_2)\n",
        "\n",
        "    # Create paired boxplots for 1-1 and 1-0 at non-seizure and preseizure\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(6,4))\n",
        "\n",
        "    axs[0].boxplot([conn_10_1, conn_10_2], labels=['Non-seizure', 'Preseizure'])\n",
        "    axs[0].set_title(\"1-0\")\n",
        "    axs[0].set_ylabel('Shannon entropy')\n",
        "\n",
        "    axs[1].boxplot([conn_11_1, conn_11_2], labels=['Non-seizure', 'Preseizure'])\n",
        "    axs[1].set_title(\"1-1\")\n",
        "    axs[1].set_ylabel('Shannon entropy')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AA1n9EO4vvCU"
      },
      "source": [
        "Test Shannon entropy of the connectivity evolution, using random groups of EN and NN. This way we will have the same number of data points for both EN-EN group and EN-NN group. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nNE84ol9ZYXJ",
        "outputId": "24513dc4-c6b5-4b98-86a3-5931eb34518f"
      },
      "outputs": [],
      "source": [
        "# main_folder = \"/content/gdrive/My Drive/epigame-folder/30sec/\"\n",
        "main_folder = \"/media/kivi/ADATA HV100/epigame-folder/\"\n",
        "\n",
        "cm_folder = main_folder + \"connectivity_matrices/\"\n",
        "\n",
        "woi = \"preseizure1\"\n",
        "measure = \"CC-(0,4)\"\n",
        "\n",
        "for sub in good_outcome_subjects:\n",
        "  print(f\"Subject {sub}\")\n",
        "\n",
        "  # Initializing lists for storing Shannon entropy for EN-EN (1-1) and EN-NN (1-0) connectivity\n",
        "  conn_11_1, conn_10_1, conn_11_2, conn_10_2 = [],[],[],[]\n",
        "\n",
        "  cm_filename = cm_folder + f\"{sub}-{woi}-{measure}.prep\"\n",
        "  cm_load = REc.load(cm_filename)\n",
        "\n",
        "  # EN(1) and NN(0) nodes\n",
        "  nodes = NODES[str(sub)]\n",
        "  node_ids = list(range(len(nodes)))\n",
        "  resection_ids = [nodes.index(label) for label in RESECTION[sub]]\n",
        "  non_resection_ids = list(np.setdiff1d(node_ids, resection_ids))\n",
        "\n",
        "  r = 0.1\n",
        "  group_size = int(len(NODES[str(sub)])*r) if int(len(NODES[str(sub)])*r) < len(RESECTION[sub]) else len(RESECTION[sub])-1\n",
        "  print(group_size, int(len(NODES[str(sub)])*r) < len(RESECTION[sub]))\n",
        "\n",
        "  # Iterate the upper triangular matrix without diagonal\n",
        "  cms = cm_load.data.X\n",
        "  rows = cms[0].shape[0]\n",
        "  cols = cms[0].shape[1]\n",
        "\n",
        "  n_epochs = int(len(cms)/2)\n",
        "\n",
        "  # Sort epochs by index\n",
        "  idx_cm_tuples = [(idx,cms[i]) for i,idx in enumerate(cm.i)]\n",
        "\n",
        "  idx_cm_1 = dict(sorted({idx:cm for (idx,cm) in idx_cm_tuples[:n_epochs]}.items()))\n",
        "  idx_cm_2 = dict(sorted({idx:cm for (idx,cm) in idx_cm_tuples[n_epochs::]}.items()))\n",
        "  epochs_1 = list(idx_cm_1.values())\n",
        "  epochs_2 = list(idx_cm_2.values())\n",
        "\n",
        "  for i in range(10):\n",
        "\n",
        "    resection_group = list(np.random.choice(resection_ids, size=group_size, replace=False))\n",
        "    non_resection_group = list(np.random.choice(non_resection_ids, size=group_size, replace=False))\n",
        "\n",
        "    for en in resection_ids:\n",
        "      if en not in resection_group:\n",
        "\n",
        "        for node1 in range(rows):\n",
        "            for node2 in range(node1 + 1, cols):  # Start from node1 + 1 to exclude the diagonal\n",
        "\n",
        "              if node1==en or node2==en:\n",
        "\n",
        "                # List of connectivity measures for all epochs for node1-node2\n",
        "                conn_1 = [get_connectivity(cm, node1, node2) for cm in epochs_1]\n",
        "                conn_2 = [get_connectivity(cm, node1, node2) for cm in epochs_2]\n",
        "\n",
        "                # Shannon entropy of the epochs\n",
        "                entropy_1 = entropy(conn_1, base=2)\n",
        "                entropy_2 = entropy(conn_2, base=2)\n",
        "\n",
        "                if node1 in resection_group or node2 in resection_group:\n",
        "                  conn_11_1.append(entropy_1)\n",
        "                  conn_11_2.append(entropy_2)\n",
        "                elif node1 in non_resection_group or node2 in non_resection_group:\n",
        "                  conn_10_1.append(entropy_1)\n",
        "                  conn_10_2.append(entropy_2)\n",
        "\n",
        "  # Create paired boxplots for 1-1 and 1-0 at non-seizure and preseizure\n",
        "  fig, axs = plt.subplots(1, 2, figsize=(6,4))\n",
        "\n",
        "  axs[0].boxplot([conn_10_1, conn_11_1], labels=['1-0', '1-1'])\n",
        "  axs[0].set_title(\"Non-seizure\")\n",
        "  axs[0].set_ylabel('Shannon entropy')\n",
        "\n",
        "  axs[1].boxplot([conn_10_2, conn_11_2], labels=['1-0', '1-1'])\n",
        "  axs[1].set_title(\"Preseizure\")\n",
        "  axs[1].set_ylabel('Shannon entropy')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oYMmDpNvpxS"
      },
      "source": [
        "Test if the connectivity series is a random walk.\n",
        "The function variance_ratio_test returns the Z and P value for a time series."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgEzWZKTuWuJ"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import norm\n",
        "\n",
        "def variance_ratio_test(time_series, num_splits=2):\n",
        "    n = len(time_series)\n",
        "    k = n // num_splits\n",
        "\n",
        "    split_series = np.array_split(time_series, num_splits)\n",
        "\n",
        "    r_s = np.sum([(split_series[i][-1] - split_series[i][0]) ** 2 for i in range(num_splits)])\n",
        "    s_s = np.sum([np.var(split_series[i]) for i in range(num_splits)])\n",
        "\n",
        "    z = (r_s / (s_s * k)) - 1\n",
        "    p_value = 1 - norm.cdf(z)\n",
        "\n",
        "    return z, p_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we will add more measures of randomness of a time series to analyze our cohort. We include the random walk test, ADF, kurtosis and skewness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy import stats\n",
        "\n",
        "def stat_test(sample1, sample2):\n",
        "    # Perform a normality test (e.g., Shapiro-Wilk)\n",
        "    normality_test_statistic1, normality_p_value1 = stats.shapiro(sample1)\n",
        "    normality_test_statistic2, normality_p_value2 = stats.shapiro(sample2)\n",
        "\n",
        "    # Define significance level\n",
        "    alpha = 0.05\n",
        "\n",
        "    # Check if data is Gaussian\n",
        "    if normality_p_value1 > alpha and normality_p_value2 > alpha:\n",
        "        # Data is approximately Gaussian, apply Z-test (assuming known population std. dev.)\n",
        "        \n",
        "        # Calculate the mean and standard deviation of the samples\n",
        "        mean1 = np.mean(sample1)\n",
        "        mean2 = np.mean(sample2)\n",
        "        std1 = np.std(sample1)\n",
        "        std2 = np.std(sample2)\n",
        "        \n",
        "        # Calculate the Z-score\n",
        "        z_score = (mean1 - mean2) / ((std1**2 / len(sample1)) + (std2**2 / len(sample2)))**0.5\n",
        "        \n",
        "        # Calculate the two-tailed p-value\n",
        "        p_value = 2 * (1 - stats.norm.cdf(abs(z_score)))\n",
        "        \n",
        "    else:\n",
        "        # Data is not Gaussian, apply Mann-Whitney U test\n",
        "        U_statistic, p_value = stats.mannwhitneyu(sample1, sample2)\n",
        "    return p_value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pyO11Pnluc8c",
        "outputId": "6c7f9c73-1c56-4585-ed35-f89bf0412d44"
      },
      "outputs": [],
      "source": [
        "from statsmodels.tsa.stattools import adfuller\n",
        "from scipy.stats import skew, kurtosis\n",
        "\n",
        "# main_folder = \"/content/gdrive/My Drive/epigame-folder/30sec/\"\n",
        "main_folder = \"/media/kivi/ADATA HV100/epigame-folder/\"\n",
        "\n",
        "cm_folder = main_folder + \"connectivity_matrices/\"\n",
        "\n",
        "woi = \"preseizure1\"\n",
        "measure = \"CC-(0,4)\"\n",
        "print(measure)\n",
        "\n",
        "for sub in good_outcome_subjects:\n",
        "  print(f\"Subject {sub}\")\n",
        "\n",
        "  # Initializing lists for storing timeseries analyses for EN-EN (1-1) and EN-NN (1-0) connectivity\n",
        "  conn_11_1_rwalk, conn_10_1_rwalk, conn_11_2_rwalk, conn_10_2_rwalk = [],[],[],[]\n",
        "  conn_11_1_adf, conn_10_1_adf, conn_11_2_adf, conn_10_2_adf = [],[],[],[]\n",
        "  # conn_11_1_lb, conn_10_1_lb, conn_11_2_lb, conn_10_2_lb = [],[],[],[]\n",
        "  conn_11_1_kurt, conn_10_1_kurt, conn_11_2_kurt, conn_10_2_kurt = [],[],[],[]\n",
        "  conn_11_1_skew, conn_10_1_skew, conn_11_2_skew, conn_10_2_skew = [],[],[],[]\n",
        "\n",
        "  cm_filename = cm_folder + f\"{sub}-{woi}-{measure}.prep\"\n",
        "  cm_load = REc.load(cm_filename)\n",
        "\n",
        "  # EN(1) and NN(0) nodes\n",
        "  nodes = NODES[str(sub)]\n",
        "  node_ids = list(range(len(nodes)))\n",
        "  resection_ids = [nodes.index(label) for label in RESECTION[sub]]\n",
        "  non_resection_ids = list(np.setdiff1d(node_ids, resection_ids))\n",
        "\n",
        "  r = 0.1\n",
        "  group_size = int(len(NODES[str(sub)])*r) if int(len(NODES[str(sub)])*r) < len(RESECTION[sub]) else len(RESECTION[sub])-1\n",
        "  print(group_size, int(len(NODES[str(sub)])*r) < len(RESECTION[sub]))\n",
        "\n",
        "  # Iterate the upper triangular matrix without diagonal\n",
        "  cms = cm_load.data.X\n",
        "  rows = cms[0].shape[0]\n",
        "  cols = cms[0].shape[1]\n",
        "\n",
        "  n_epochs = int(len(cms)/2)\n",
        "\n",
        "  # Sort epochs by index\n",
        "  idx_cm_tuples = [(idx,cms[i]) for i,idx in enumerate(cm.i)]\n",
        "\n",
        "  idx_cm_1 = dict(sorted({idx:cm for (idx,cm) in idx_cm_tuples[:n_epochs]}.items()))\n",
        "  idx_cm_2 = dict(sorted({idx:cm for (idx,cm) in idx_cm_tuples[n_epochs::]}.items()))\n",
        "  epochs_1 = list(idx_cm_1.values())\n",
        "  epochs_2 = list(idx_cm_2.values())\n",
        "\n",
        "  for i in range(10):\n",
        "\n",
        "    resection_group = list(np.random.choice(resection_ids, size=group_size, replace=False))\n",
        "    non_resection_group = list(np.random.choice(non_resection_ids, size=group_size, replace=False))\n",
        "\n",
        "    for en in resection_ids:\n",
        "      if en not in resection_group:\n",
        "\n",
        "        for node1 in range(rows):\n",
        "            for node2 in range(node1 + 1, cols):  # Start from node1 + 1 to exclude the diagonal\n",
        "\n",
        "              if node1==en or node2==en:\n",
        "\n",
        "                # List of connectivity measures for all epochs for node1-node2\n",
        "                conn_1 = [get_connectivity(cm, node1, node2) for cm in epochs_1]\n",
        "                conn_2 = [get_connectivity(cm, node1, node2) for cm in epochs_2]\n",
        "\n",
        "                # Check random walk of the epochs\n",
        "                z_score_1, p_value_1 = variance_ratio_test(conn_1)\n",
        "                z_score_2, p_value_2 = variance_ratio_test(conn_2)\n",
        "\n",
        "                # Stationarity Analysis\n",
        "\n",
        "                # Augmented Dickey-Fuller (ADF) Test is used to check for the presence of a unit root in a time series,\n",
        "                # which indicates non-stationarity. If the time series is non-stationary, # it might have trends or other\n",
        "                # patterns that make statistical analysis challenging.\n",
        "                # A lower p-value indicates a stronger rejection of the null hypothesis of non-stationarity\n",
        "                adfuller_1, adfuller_2 = adfuller(conn_1), adfuller(conn_2)\n",
        "                adf_tstat_1, adf_p_1 = adfuller_1[0], adfuller_1[1]\n",
        "                adf_tstat_2, adf_p_2 = adfuller_2[0], adfuller_2[1]\n",
        "\n",
        "                # Skewness and Kurtosis\n",
        "                skewness_1 = skew(conn_1)\n",
        "                kurt_1 = kurtosis(conn_1)\n",
        "                skewness_2 = skew(conn_2)\n",
        "                kurt_2 = kurtosis(conn_2)\n",
        "\n",
        "                if node1 in resection_group or node2 in resection_group:\n",
        "                  conn_11_1_rwalk.append(p_value_1)\n",
        "                  conn_11_2_rwalk.append(p_value_2)\n",
        "\n",
        "                  conn_11_1_adf.append(adf_p_1)\n",
        "                  conn_11_2_adf.append(adf_p_2)\n",
        "\n",
        "                  conn_11_1_kurt.append(kurt_1)\n",
        "                  conn_11_2_kurt.append(kurt_2)\n",
        "\n",
        "                  conn_11_1_skew.append(skewness_1)\n",
        "                  conn_11_2_skew.append(skewness_2)\n",
        "\n",
        "                elif node1 in non_resection_group or node2 in non_resection_group:\n",
        "                  conn_10_1_rwalk.append(p_value_1)\n",
        "                  conn_10_2_rwalk.append(p_value_2)\n",
        "\n",
        "                  conn_10_1_adf.append(adf_p_1)\n",
        "                  conn_10_2_adf.append(adf_p_2)\n",
        "\n",
        "                  conn_10_1_kurt.append(kurt_1)\n",
        "                  conn_10_2_kurt.append(kurt_2)\n",
        "\n",
        "                  conn_10_1_skew.append(skewness_1)\n",
        "                  conn_10_2_skew.append(skewness_2)\n",
        "\n",
        "  # Create paired boxplots for 1-1 and 1-0 at non-seizure and preseizure\n",
        "\n",
        "  # Random Walk\n",
        "  fig, axs = plt.subplots(1, 2, figsize=(6,4))\n",
        "  axs[0].boxplot([conn_10_1_rwalk, conn_11_1_rwalk], labels=['1-0', '1-1'])\n",
        "  axs[0].set_title(f\"Non-seizure\")#, {stat_test(conn_10_1_rwalk, conn_11_1_rwalk)}\")\n",
        "  axs[0].set_ylabel('Random Walk Test P-value')\n",
        "  axs[1].boxplot([conn_10_2_rwalk, conn_11_2_rwalk], labels=['1-0', '1-1'])\n",
        "  axs[1].set_title(f\"Preseizure\")# {stat_test(conn_10_2_rwalk, conn_11_2_rwalk)}\")\n",
        "  axs[1].set_ylabel('Random Walk Test P-value')\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "  # Augmented Dickey-Fuller (ADF) Test\n",
        "  fig, axs = plt.subplots(1, 2, figsize=(6,4))\n",
        "  axs[0].boxplot([conn_10_1_adf, conn_11_1_adf], labels=['1-0', '1-1'])\n",
        "  axs[0].set_title(f\"Non-seizure\")#, {stat_test(conn_10_1_adf, conn_11_1_adf)}\")\n",
        "  axs[0].set_ylabel('ADF Test P-value')\n",
        "  axs[1].boxplot([conn_10_2_adf, conn_11_2_adf], labels=['1-0', '1-1'])\n",
        "  axs[1].set_title(f\"Preseizure\")# {stat_test(conn_10_2_adf, conn_11_2_adf)}\")\n",
        "  axs[1].set_ylabel('ADF Test P-value')\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "  # Kurtosis\n",
        "  fig, axs = plt.subplots(1, 2, figsize=(6,4))\n",
        "  axs[0].boxplot([conn_10_1_kurt, conn_11_1_kurt], labels=['1-0', '1-1'])\n",
        "  axs[0].set_title(f\"Non-seizure\")#, {stat_test(conn_10_1_kurt, conn_11_1_kurt)}\")\n",
        "  axs[0].set_ylabel('Kurtosis')\n",
        "  axs[1].boxplot([conn_10_2_kurt, conn_11_2_kurt], labels=['1-0', '1-1'])\n",
        "  axs[1].set_title(f\"Preseizure\")# {stat_test(conn_10_2_kurt, conn_11_2_kurt)}\")\n",
        "  axs[1].set_ylabel('Kurtosis')\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "  # Skeweness\n",
        "  fig, axs = plt.subplots(1, 2, figsize=(6,4))\n",
        "  axs[0].boxplot([conn_10_1_skew, conn_11_1_skew], labels=['1-0', '1-1'])\n",
        "  axs[0].set_title(f\"Non-seizure\")#, {stat_test(conn_10_1_skew, conn_11_1_skew)}\")\n",
        "  axs[0].set_ylabel('Skeweness')\n",
        "  axs[1].boxplot([conn_10_2_skew, conn_11_2_skew], labels=['1-0', '1-1'])\n",
        "  axs[1].set_title(f\"Preseizure\")# {stat_test(conn_10_2_skew, conn_11_2_skew)}\")\n",
        "  axs[1].set_ylabel('Skeweness')\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "  break"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMob2l4m3avVKoIn4UXdH6l",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
