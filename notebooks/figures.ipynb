{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook produces figures from Ivankovic *et al.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect import isfunction, ismethod, isgeneratorfunction, isgenerator, isroutine\n",
    "from inspect import isabstract, isclass, ismodule, istraceback, isframe, iscode, isbuiltin\n",
    "from inspect import ismethoddescriptor, isdatadescriptor, isgetsetdescriptor, ismemberdescriptor\n",
    "from inspect import isawaitable, iscoroutinefunction, iscoroutine\n",
    "\n",
    "from collections.abc import Iterable as iterable\n",
    "\n",
    "import pickle\n",
    "\n",
    "def isfx(field): return ismethod(field) or isfunction(field)\n",
    "\n",
    "class GhostSet:\n",
    "    \"\"\" enhanced interface (ghost) to retrieve class fields \"\"\"\n",
    "    def _meta(data): return {k:v for k,v in data.__dict__.items() if not isfx(v)}\n",
    "    def _at_last(_, sets): pass\n",
    "    def _set(object, **sets):\n",
    "        ''' use to fast initialize fields | needed to avoid initialization problems at copy by value '''\n",
    "        for field in sets: setattr(object, field, sets[field])\n",
    "        object._at_last(sets)\n",
    "GSet = GhostSet\n",
    "\n",
    "def meta(object):\n",
    "    ''' retrieves clonable object metadata (__dict__) as a copy '''\n",
    "    if isinstance(object, GSet): return object._meta()\n",
    "    return {}\n",
    "\n",
    "class ClonableObjectGhost:\n",
    "    \"\"\" enhanced interface (ghost) for clonable objects \"\"\"\n",
    "    def _by_val(_, depth=-1, _layer=0): pass\n",
    "GCo = ClonableObjectGhost\n",
    "\n",
    "class ClonableObject(GSet, GCo):\n",
    "    \"\"\" base clonable object \"\"\"\n",
    "    def __init__(this, **data): this._set(**data)\n",
    "    def __call__(_, **options): _._set(**options)\n",
    "    def _by_val(_, depth=-1, _layer=0):\n",
    "        copy = type(_)()\n",
    "        copy._set(**_._meta())\n",
    "        if depth<0 or depth>_layer:\n",
    "            for field in copy.__dict__:\n",
    "                if isinstance(copy.__dict__[field], ClonableObjectGhost):\n",
    "                    copy.__dict__[field] = copy.__dict__[field]._by_val(depth,_layer+1)\n",
    "        return copy\n",
    "COb = ClonableObject\n",
    "\n",
    "def copy_by_val(object, depth=-1, _layer=0):\n",
    "    if isinstance(object, GCo): return object._by_val(depth,_layer)\n",
    "    return object\n",
    "copy = by_val = vof = copy_by_val\n",
    "\n",
    "class ComparableGhost:\n",
    "    \"\"\" enhanced interface (ghost) for comparing instances \"\"\"\n",
    "    def _compare(a, b):\n",
    "        if type(a) != type(b): return False\n",
    "        if a.__dict__ == b.__dict__: return True\n",
    "        return False\n",
    "    def __eq__(a, b): return a._compare(b)\n",
    "GEq = ComparableGhost\n",
    "\n",
    "class IterableObjectGhost(GSet):\n",
    "    \"\"\" enhanced interface (ghost) for iterables: exposes __dict__,\n",
    "        therefore Iterable Objects are like lua dictionaries \"\"\"\n",
    "    def __contains__(this, key): return key in this.__dict__\n",
    "    def __iter__(this): return iter(this.__dict__)\n",
    "    def items(my): return my.__dict__.items()\n",
    "    def __getitem__(by, field): return by.__dict__[field]\n",
    "    def __setitem__(by, field, value): by.__dict__[field] = value\n",
    "    def pop(by, field): return by.__dict__.pop(field)\n",
    "GIo = IterableObjectGhost\n",
    "\n",
    "class ReprGhost:\n",
    "    \"\"\" enhanced interface (ghost) for the skeleton method _repr,\n",
    "        see implementation of Struct for a working example;\n",
    "        Record __repr__ override uses _lines_ for max lines display \"\"\"\n",
    "    _lines_ = 31\n",
    "    _chars_ = 13\n",
    "    _msgsz_ = 62\n",
    "    _ellipsis_ = ' ... '\n",
    "    def _repr(my, value):\n",
    "        _type = ''.join(''.join(str(type(value)).split('class ')).split(\"'\"))\n",
    "        _value = '{}'.format(value)\n",
    "        if len(_value)>my._chars_:\n",
    "            show = int(my._chars_/2)\n",
    "            _value = _value[:show]+my._ellipsis_+_value[-show:]\n",
    "        return '{} {}'.format(_type, _value)\n",
    "    def _resize(this, message, at=.7):\n",
    "        if len(message)>this._msgsz_:\n",
    "            start = int(at*this._msgsz_)\n",
    "            end = this._msgsz_-start\n",
    "            return message[:start]+this._ellipsis_+message[-end:]\n",
    "        return message\n",
    "GRe = ReprGhost\n",
    "\n",
    "def set_repr_to(lines): GRe._lines_ = lines\n",
    "\n",
    "class Struct(COb, GEq, GIo, GRe):\n",
    "    \"\"\" structured autoprintable object, behaves like a lua dictionary \"\"\"\n",
    "    def __repr__(_):\n",
    "        return '\\n'.join(['{}:\\t{}'.format(k, _._repr(v)) for k,v in _.items()])\n",
    "struct = Struct\n",
    "\n",
    "class RecordableGhost:\n",
    "    \"\"\" enhanced interface (ghost) for type recording,\n",
    "        see Record for a working example \"\"\"\n",
    "    @staticmethod\n",
    "    def load(filename):\n",
    "        with open(filename, 'rb') as file: return pickle.load(file)\n",
    "    def save(data, filename):\n",
    "        with open(filename, 'wb') as file: pickle.dump(data, file)\n",
    "        \n",
    "GRec = RecordableGhost\n",
    "\n",
    "class Record(GSet, GCo, GRec, GEq, GRe):\n",
    "    \"\"\" wrapper for any object or value, auto-inspects and provides load/save type structure \"\"\"\n",
    "    data = None\n",
    "    _check = dict(\n",
    "            isfunction=isfunction, ismethod=ismethod, isgeneratorfunction=isgeneratorfunction, isgenerator=isgenerator, isroutine=isroutine,\n",
    "            isabstract=isabstract, isclass=isclass, ismodule=ismodule, istraceback=istraceback, isframe=isframe, iscode=iscode, isbuiltin=isbuiltin,\n",
    "            ismethoddescriptor=ismethoddescriptor, isdatadescriptor=isdatadescriptor, isgetsetdescriptor=isgetsetdescriptor, ismemberdescriptor=ismemberdescriptor,\n",
    "            isawaitable=isawaitable, iscoroutinefunction=iscoroutinefunction, iscoroutine=iscoroutine\n",
    "                   )\n",
    "    def __init__(this, token, **meta):\n",
    "        this.data = token\n",
    "        this.__dict__.update({k:v(token) for k,v in this._check.items()})\n",
    "        super()._set(**meta)\n",
    "    @property\n",
    "    def type(_): return type(_.data)\n",
    "    def inherits(_, *types): return issubclass(_.type, types)\n",
    "    @property\n",
    "    def isbaseiterable(_): return _.inherits(tuple, list, dict, set) or _.isgenerator or _.isgeneratorfunction\n",
    "    @property\n",
    "    def isiterable(_): return isinstance(_.data, iterable) and _.type is not str\n",
    "    def _clone_iterable(_):\n",
    "        if _.inherits(dict): return _.data.copy()\n",
    "        elif _.isgenerator or _.isgeneratorfunction: return (i for i in list(_.data))\n",
    "        else: return type(_.data)(list(_.data)[:])\n",
    "    def _meta(data): return {k:v for k,v in data.__dict__.items() if k != 'data' and not isfx(v)}\n",
    "    def _by_val(_, depth=-1, layer=0):\n",
    "        data = _.data\n",
    "        if _.isiterable: data = _._clone_iterable()\n",
    "        elif _.inherits(ClonableObjectGhost): data = by_val(data, depth, layer)\n",
    "        return type(_)(data, **meta(_))\n",
    "    def __enter__(self): self._instance = self; return self\n",
    "    def __exit__(self, type, value, traceback): self._instance = None\n",
    "    def __repr__(self):\n",
    "        if not hasattr(self, '_preprint'): return Record(self.data, _preprint='', _lines=Record(Record._lines_)).__repr__()\n",
    "        if self.isbaseiterable:\n",
    "            pre, repr = self._preprint, ''\n",
    "            for n,i in enumerate(self.data):\n",
    "                if self._lines.data == 0: break\n",
    "                else: self._lines.data -= 1\n",
    "                index, item = str(n), i\n",
    "                if self.inherits(dict): index += ' ({})'.format(str(i)); item = self.data[i]\n",
    "                repr += pre+'{}: '.format(index)\n",
    "                next = Record(item, _preprint=pre+'\\t', _lines=self._lines)\n",
    "                if next.isiterable: repr += '\\n'\n",
    "                repr += next.__repr__()\n",
    "                repr += '\\n'\n",
    "            return repr\n",
    "        elif self.inherits(GCo): return Record(self.data._meta(), _preprint=self._preprint, _lines=self._lines).__repr__()\n",
    "        else: return self._repr(self.data)\n",
    "REc = Record\n",
    "\n",
    "class Bisect(list, COb):\n",
    "    \"\"\" bisect implementation using clonable objects \"\"\"\n",
    "    def __init__(set, *items, key=None, reverse=False):\n",
    "        if not key: key = lambda  x:x\n",
    "        super().__init__(sorted(items, reverse=reverse, key=key))\n",
    "    def _bisect(set, item, key, reverse, bottom, top):\n",
    "        def _(check):\n",
    "            if key: return key(check)\n",
    "            return check\n",
    "        at = int((top-bottom)/2)+bottom\n",
    "        if len(set)==0: return (0,-1)\n",
    "        if item==_(set[at]): return (at,0)\n",
    "        bigger = item<_(set[at])\n",
    "        if bigger != reverse:\n",
    "            if at-bottom>0: return set._bisect(item, key, reverse, bottom, at)\n",
    "            return (at,-1)\n",
    "        elif top-at>1: return set._bisect(item, key, reverse, at, top)\n",
    "        return (at,1)\n",
    "    def search(_, item, key=None, reverse=False):\n",
    "        if not key: key = lambda x:x\n",
    "        return _._bisect(item, key, reverse, 0, len(_))\n",
    "    def _by_val(_, depth=-1, _layer=0):\n",
    "        copy = super()._by_val(depth, _layer)\n",
    "        copy += _[:]\n",
    "        return copy\n",
    "BSx = Bisect"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Below we define the validation function to score networks relative to the resection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_validation_score(net, resection, all_nodes):\n",
    "    \n",
    "    not_resection = len(all_nodes)-len(resection)\n",
    "    tp = len(list(set(net)&set(resection)))\n",
    "    fp = len(net) - tp\n",
    "    tn = not_resection - fp\n",
    "    \n",
    "    return (tp / len(resection)) * (tn / not_resection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_subject = \"ASJ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_folder = \"/home/kivi/gdrive/epigame-folder/\"\n",
    "path_net = main_folder + \"selected_network/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "woi_code = {'1':\"baseline\", '2':\"preseizure5\", '3':\"preseizure4\", '4':\"preseizure3\", '5':\"preseizure2\", '6':\"preseizure1\", '7':\"transition1\", '8':\"transition2\", '9':\"transition60\", '10':\"seizure\"}\n",
    "woi_code_inverted = {y:x for x,y in woi_code.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes = ['P1-P2', 'P4-P5', 'P8-P9', 'P9-P10', 'P10-P11', 'G1-G2', 'G8-G9', 'G9-G10', 'G10-G11', 'G11-G12', 'M1-M2', 'M8-M9', 'M9-M10', 'M10-M11', 'M11-M12', 'O1-O2', 'O2-O3', 'O5-O6', 'O6-O7', 'F1-F2', 'F7-F8', 'F8-F9', 'F9-F10', 'F10-F11', 'F11-F12', 'F12-F13', 'A1-A2', 'A2-A3', 'A3-A4', 'A7-A8', 'A8-A9', 'A9-A10', 'A10-A11', 'B1-B2', 'B2-B3', 'B5-B6', 'B6-B7', 'B7-B8', 'B8-B9', 'C1-C2', 'C5-C6', 'C6-C7', 'C7-C8', 'C8-C9', 'C9-C10', 'Q1-Q2', 'Q2-Q3', 'Q3-Q4', 'Q4-Q5', 'Q8-Q9', 'Q9-Q10', 'Q10-Q11', 'Q11-Q12', 'T1-T2', 'T2-T3', 'T3-T4', 'T4-T5', 'T5-T6', 'T6-T7', 'T7-T8', 'T8-T9', 'T9-T10', 'T10-T11', 'T11-T12', 'D1-D2', 'D2-D3', 'D3-D4', 'D4-D5', 'D5-D6', 'D6-D7', 'D7-D8', 'E1-E2', 'E2-E3', 'E3-E4', 'E4-E5', 'E5-E6', 'E6-E7', 'E7-E8', 'E8-E9', 'E9-E10', 'E10-E11', 'L1-L2', 'L2-L3', 'L5-L6', 'L6-L7', 'L7-L8', 'U1-U2', 'U2-U3', 'U3-U4', 'U4-U5', 'U5-U6', 'U6-U7', 'J1-J2', 'J10-J11', 'J11-J12', 'J12-J13', 'J13-J14', 'J14-J15']\n",
    "resection = ['A3-A4', 'B2-B3', 'B5-B6', 'B6-B7', 'D3-D4', 'D4-D5', 'T1-T2', 'T2-T3', 'T3-T4', 'T4-T5', 'T5-T6', 'T6-T7', 'T7-T8', 'T8-T9', 'T9-T10', 'T10-T11', 'T11-T12']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rows = []\n",
    "\n",
    "for file_net in os.listdir(path_net):\n",
    "\n",
    "  filename = file_net.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "  subject_id = filename[0:3]\n",
    "\n",
    "  if subject_id == selected_subject:\n",
    "    # print(\"\\nSelected network of\", file_net)\n",
    "\n",
    "    # Load attributes from filename \"SUB-woi-connectivitymeasure-(fmin,fmax).res\"\n",
    "    woi = filename.split(\"-\")[1]\n",
    "    woi_id = woi_code_inverted[woi]\n",
    "    conn_measure = filename.split(\"-\")[2]\n",
    "\n",
    "    # If the signal was not filtered in any specific frequency band, mark it as \"NA\"\n",
    "    # If the signal was filtered, mark the frequency band minimum and maximum\n",
    "    f_min = None if len(filename.split(\".\")[0].split(\"-\")) == 3 else int(filename.split(\".\")[0].split(\"-\")[3][1:-1].split(\",\")[0])\n",
    "    f_max = None if len(filename.split(\".\")[0].split(\"-\")) == 3 else int(filename.split(\".\")[0].split(\"-\")[3][1:-1].split(\",\")[1])\n",
    "\n",
    "    net = REc.load(path_net + file_net).data\n",
    "\n",
    "    last_tested_n = list(net.test_nets.keys())[-1]\n",
    "    # print(last_tested_n)\n",
    "    # if last_tested_n>2: print(f\"Evaluation score decreased at {last_tested_n} nodes:\", net.test_nets[last_tested_n-1][0][-1], \">=\", net.test_nets[last_tested_n][0][-1])\n",
    "\n",
    "    eval_progression = [net.test_nets[n][0][-1] for n in net.test_nets.keys()]\n",
    "    # print(\"Evaluation score progression:\", eval_progression)\n",
    "    eval_score = net.test_nets[last_tested_n-1][0][-1]\n",
    "\n",
    "    selected = list(net.nodes)\n",
    "    net_size = len(selected)\n",
    "    best_pair = sorted(set([node for node in net.test_nets[2][0][1].split(\"<->\")]))\n",
    "\n",
    "    # print(\"\\nOriginal resection:\", resection)\n",
    "    # print(\"Selected net:\", selected)\n",
    "\n",
    "    intersection = list(set(selected)&set(resection))\n",
    "    # print(\"\\nNet-Resection intersection:\", intersection)\n",
    "    S = network_validation_score(selected, resection, all_nodes)\n",
    "    # print(\"Validation score =\", val_score)\n",
    "\n",
    "    df_rows.append([woi, woi_id, conn_measure, f_min, f_max, resection, \n",
    "                    best_pair, selected, net_size, \n",
    "                    intersection, eval_progression, eval_score, \n",
    "                    S])\n",
    "\n",
    "  dataframe = pd.DataFrame(df_rows, columns=[\"WOI\",\"WOI_ID\",\"CM\",\"F_MIN\",\"F_MAX\",\"RESECTION\",\n",
    "                                            \"BEST_PAIR\",\"NET\",\"NET_SIZE\",\n",
    "                                            \"INTERSECTION\",\"EVAL_PROGRESSION\",\"EVAL_SCORE\",\n",
    "                                            \"VAL_SCORE\"]) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add a sheet to an existing XLSX file, for the new subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter(main_folder + \"RESULTS.xlsx\", engine='openpyxl')\n",
    "\n",
    "if os.path.exists(main_folder + \"RESULTS.xlsx\"):\n",
    "    book = openpyxl.load_workbook(main_folder + \"RESULTS.xlsx\")\n",
    "    writer.book = book\n",
    "\n",
    "dataframe.sort_values(\"WOI\").to_excel(writer, sheet_name=selected_subject)\n",
    "writer.save()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_excel(main_folder + \"RESULTS.xlsx\", sheet_name=selected_subject)\n",
    "\n",
    "conn_measures = set(dataframe[\"CM\"])\n",
    "print(\"Connectivity measures:\", conn_measures)\n",
    "\n",
    "for cm in conn_measures:\n",
    "\n",
    "    cm_dataframe = dataframe.groupby(\"CM\").get_group(cm)\n",
    "\n",
    "    df_val_score = cm_dataframe[[\"F_MIN\", \"F_MAX\", \"WOI\", \"WOI_ID\", \"VAL_SCORE\"]]\n",
    "\n",
    "    if cm not in [\"PAC\", \"PEC\"]:\n",
    "        df_val_score = df_val_score.pivot(\"F_MIN\", \"WOI_ID\", \"VAL_SCORE\").sort_values(\"F_MIN\")\n",
    "    else: \n",
    "        df_val_score = df_val_score.pivot(\"F_MIN\", \"WOI_ID\", \"VAL_SCORE\")\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    ax = sns.heatmap(df_val_score, vmin=0, vmax=1, cbar_kws={\"label\": \"Network validation score\"})\n",
    "    ax.figure.axes[-1].yaxis.label.set_size(16)\n",
    "    ax.figure.axes[-1].tick_params(labelsize=16)\n",
    "    plt.yticks(ticks=list(x+0.5 for x in range(6)), \n",
    "               labels=[\"δ\", \"ϑ\", \"α\", \"β\", \"low γ\", \"high γ\"], \n",
    "               rotation=0, size=16)\n",
    "    plt.ylabel(\"Frequency band\", size=16)\n",
    "    if cm==\"CC\": plt.xticks(ticks=list(x+0.5 for x in range(7)), \n",
    "                            labels=[\"[-5 min, -4 min]\", \"[-4 min, -3 min]\", \"[-3 min, -2 min]\", \"[-2 min, -1 min]\", \"[-1 min, 0]\", \"[-30 s, +30 s]\", \"[-30% L, +30% L]\"], \n",
    "                            rotation=90, size=16)\n",
    "    else: plt.xticks(ticks=list(x+0.5 for x in range(8)), \n",
    "                    labels=[\"[-5 min, -4 min]\", \"[-4 min, -3 min]\", \"[-3 min, -2 min]\", \"[-2 min, -1 min]\", \"[-1 min, 0]\", \"[-30 s, +30 s]\", \"[-1 min, +1 min]\", \"[-30% L, +30% L]\"], \n",
    "                    rotation=90, size=16)\n",
    "    plt.xlabel(\"WOI\", size=16)\n",
    "    plt.title(cm, size=16)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_woi = \"preseizure5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_combs = []\n",
    "\n",
    "woi_nets, woi_cms = {},{}\n",
    "\n",
    "woi_group = dataframe.groupby(\"WOI\").get_group(selected_woi)\n",
    "for idx_row,row in woi_group.iterrows():\n",
    "    woi_nets[row[0]] = list(node[1:-1] for node in row[8][1:-1].split(\", \")) # reformat string to list of nodes\n",
    "    woi_cms[row[0]] = row[3]+\"(\"+str(row[4]).split(\".\")[0]+\",\"+str(row[5]).split(\".\")[0]+\")-\"\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "n_max = 9\n",
    "all_combs = {n:{} for n in range(2,n_max+1)}\n",
    "\n",
    "N, Combination, Score = [],[],[]\n",
    "\n",
    "for n in all_combs:\n",
    "    combs = combinations(woi_nets.keys(), n)\n",
    "    combs_val_scores = {}\n",
    "    max_score, best_combs_n = 0,[]\n",
    "    for comb in combs:\n",
    "        agregate = []\n",
    "        comb_label = \"\"\n",
    "        for c in comb:\n",
    "            agregate += woi_nets[c]\n",
    "            comb_label += woi_cms[c]\n",
    "        union = set(agregate)\n",
    "        score = network_validation_score(union, resection, all_nodes)\n",
    "\n",
    "        if score>max_score:\n",
    "            max_score = score\n",
    "            best_combs_n = [comb]\n",
    "        elif score==max_score: best_combs_n.append(comb)\n",
    "\n",
    "        N.append(n)\n",
    "        Combination.append(comb_label[:-1])\n",
    "        Score.append(score)\n",
    "\n",
    "        combs_val_scores[comb_label[:-1]] = score\n",
    "    \n",
    "    best_combs+=best_combs_n\n",
    "\n",
    "    print(f\"\\nNumber of combinations at {n} = {len(combs_val_scores)}\")\n",
    "    print(f\"Best net by the combination {max(combs_val_scores, key=combs_val_scores.get)}\")\n",
    "    print(f\"Score = {max(combs_val_scores.values())}\")\n",
    "    all_combs[n] = combs_val_scores\n",
    "\n",
    "\n",
    "print(\"\\nBest combinations:\")\n",
    "for n in all_combs:\n",
    "    print(max(all_combs[n], key=all_combs[n].get))\n",
    "\n",
    "dataframe_combs = pd.DataFrame({\"N\":N, \"Combination\":Combination, \"Score\":Score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=dataframe_combs, x=\"N\", y=\"Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, Best_comb, Best_score, Net_union, Net_size, TP, FP, TN = [],[],[],[],[],[],[],[] \n",
    "\n",
    "for comb in best_combs:\n",
    "    n = 0\n",
    "    agregate = []\n",
    "    comb_label = \"\"\n",
    "    for c in comb:\n",
    "        n += 1\n",
    "        agregate += woi_nets[c]\n",
    "        comb_label += woi_cms[c]\n",
    "\n",
    "    union = set(agregate)\n",
    "    score = network_validation_score(union, resection, all_nodes)\n",
    "    \n",
    "    print(comb_label[:-1], score)\n",
    "    print(sorted([n for n in union if n in resection]) + sorted([n for n in union if n not in resection]))\n",
    "    print(len(union))\n",
    "    \n",
    "    not_resection = len(all_nodes)-len(resection)\n",
    "    tp = len(list(set(union)&set(resection)))\n",
    "    fp = len(union) - tp\n",
    "    tn = not_resection - fp\n",
    "\n",
    "    print(\"TP:\", tp)\n",
    "    print(\"FP:\", fp)\n",
    "    print(\"TN:\", tn)\n",
    "\n",
    "    N.append(n)\n",
    "    Best_comb.append(comb_label[:-1])\n",
    "    Best_score.append(score)\n",
    "    Net_union.append(sorted([n for n in union if n in resection]) + sorted([n for n in union if n not in resection]))\n",
    "    Net_size.append(len(union))\n",
    "    TP.append(tp)\n",
    "    FP.append(fp)\n",
    "    TN.append(tn)\n",
    "\n",
    "dataframe_best_combs = pd.DataFrame({\"N\":N, \"BEST_COMB\":Best_comb, \"SCORE\":Best_score, \"NET_UNION\":Net_union, \"NET_SIZE\":Net_size, \"TP\":TP, \"FP\":FP, \"TN\":TN})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add a sheet to an existing XLSX file, for the new WOI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(main_folder + f\"{selected_subject}_BEST_COMB_RESULTS.xlsx\"):\n",
    "    book = openpyxl.load_workbook(main_folder + f\"{selected_subject}_BEST_COMB_RESULTS.xlsx\")\n",
    "    writer.book = book\n",
    "\n",
    "dataframe_best_combs.to_excel(writer, sheet_name=selected_woi)\n",
    "writer.save()\n",
    "writer.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize Venn diagrams of nets found based on top connectivity measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_best_combs = pd.read_excel(main_folder + f\"{selected_subject}_BEST_COMB_RESULTS.xlsx\", sheet_name=\"preseizure5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_methods = {}\n",
    "\n",
    "max_score = max(dataframe_best_combs[\"SCORE\"])\n",
    "max_score_idx = [i for i in range(len(dataframe_best_combs[\"SCORE\"])) if dataframe_best_combs[\"SCORE\"][i] == max_score]\n",
    "\n",
    "for idx in max_score_idx:\n",
    "    cms = dataframe_best_combs[\"BEST_COMB\"][idx].split(\"-\")\n",
    "    for cm in cms:\n",
    "\n",
    "        top_methods[cm] = 1 if cm not in top_methods else top_methods[cm]+1\n",
    "\n",
    "        acronym_len = len(cm.split(\"(\")[0])\n",
    "        \n",
    "        if cm[0:acronym_len]!=\"PAC\":\n",
    "            cm_net = [node_label[1:-1] for node_label in dataframe.groupby([\"WOI\"]).get_group(\"preseizure5\").groupby([\"CM\"]).get_group(cm[0:acronym_len]).groupby([\"F_MIN\"]).get_group(float(int(cm[acronym_len+1:-1].split(\",\")[0])))[\"NET\"][1].split(\", \")]\n",
    "        else: \n",
    "            cm_net = [node_label[1:-1] for node_label in dataframe.groupby([\"WOI\"]).get_group(\"preseizure5\").groupby([\"CM\"]).get_group(cm[0:acronym_len])[\"NET\"][1].split(\", \")]\n",
    "        print(cm, cm_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(top_methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
