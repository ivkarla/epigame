{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### __Guidelines for using this notebook__\n",
        "#### The notebook identifies the hypothesized epileptogenic network (EN).\n",
        "##### The notebook should be run in the Google Colab platform.\n",
        "##### The results will be saved in the main folder, in a subfolder called '*selected_network*'."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### The following five cells define objects for loading and handling the EDF data.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from inspect import ismethod\n",
        "from datetime import timedelta as _time\n",
        "from datetime import datetime\n",
        "from collections.abc import Iterable as iterable\n",
        "\n",
        "def some(field): return (field is not None and field != [] and field != {} and field != ()) or field == True\n",
        "def no(field): return not some(field) or field==False or field==''\n",
        "\n",
        "class class_of:\n",
        "    _instance = None\n",
        "    def __init__(_, object):\n",
        "        _._is = type(object)\n",
        "    def inherits(_, *types):\n",
        "        return issubclass(_._is, types)\n",
        "    def has(_, *types): return _.inherits(*types)\n",
        "    def __enter__(self):\n",
        "        self._instance = self\n",
        "        return self\n",
        "    def __exit__(self, type, value, traceback): self._instance = None\n",
        "    @staticmethod\n",
        "    def each_in(list):\n",
        "        if isiterable(list):\n",
        "            return [type(item) for item in list]\n",
        "\n",
        "class struct:\n",
        "    def __init__(table, **sets): table.__dict__.update(sets)\n",
        "    @property\n",
        "    def sets(this): return set(dir(this)) - set(dir(type(this)))\n",
        "    def set(object, **fields):\n",
        "        for field in fields: setattr(object, field, fields[field])\n",
        "    def get(object, *fields): return [getattr(object, field) for field in fields if field in object.__dict__]\n",
        "    def _clonable(set, mask=None):\n",
        "        check = set.__dict__.copy()\n",
        "        clonable = check.copy()\n",
        "        if some(mask): pass\n",
        "#            for field in check:\n",
        "#                if sum([int(_(check[field])) for _ in mask])+sum([int(_(field)) for _ in mask]): clonable.pop(field)\n",
        "        return clonable\n",
        "    @staticmethod\n",
        "    def _from(type):\n",
        "        if hasattr(type, '__dict__'): return struct(**type.__dict__.copy())\n",
        "        return struct()\n",
        "\n",
        "def meta(data, *mask): return struct._from(data)._clonable(mask)\n",
        "def get(data, *fields):\n",
        "    if not issubclass(type(data), dict): data=struct._from(data)._clonable()\n",
        "    return struct(**data).get(*fields)\n",
        "\n",
        "class table(struct):\n",
        "    def _default(field, name, value):\n",
        "        try: return getattr(field, name)\n",
        "        except: setattr(field, name, value)\n",
        "        return value\n",
        "    def clear(this, *fields):\n",
        "        sets = this.sets\n",
        "        if not fields: fields = sets\n",
        "        if fields:\n",
        "            set = [field for field in fields if hasattr(this,field) and not ismethod(getattr(this, field))]\n",
        "            for field in set: delattr(this, field)\n",
        "    def has(this, *fields):\n",
        "        return all([hasattr(this, field) for field in fields])\n",
        "    def has_not(this, *fields): return not this.has(*fields)\n",
        "    def check(this, **KV):\n",
        "        try: check = [KV[key]==this.__dict__[key] for key in KV]\n",
        "        except: return False\n",
        "        return all(check)\n",
        "    def find(this, _type):\n",
        "        return [value for value in this.sets if class_of(get(this,value)[0]).inherits(_type)]\n",
        "    def clone(this): \n",
        "        clone = type(this)()\n",
        "        sets = this._clonable()\n",
        "        clone.set(**sets)\n",
        "        return clone\n",
        "\n",
        "def isiterable(this): return isinstance(this, iterable) and type(this) is not str\n",
        "def default(field, name, value): return table(**field)._default(name, value)\n",
        "\n",
        "def ni(list):\n",
        "    if isiterable(list):\n",
        "        for n,i in enumerate(list): yield n,i\n",
        "    else:\n",
        "        for n,i in enumerate(list.__dict__.keys()): yield n,i\n",
        "\n",
        "class at(table):\n",
        "    DAY, HOUR, MIN = 86400, 3600, 60\n",
        "    def __init__(_, dtime=None, **sets):\n",
        "        _.set(**sets)\n",
        "        if some(dtime) and issubclass(type(dtime), _time): _._time = dtime\n",
        "        else:\n",
        "            d,h,m,s,ms = _._default('d',0), _._default('h',0), _._default('m',0), _._default('s',0), _._default('ms',0)\n",
        "            if not any([d,h,m,s,ms]): now=datetime.now(); _._time = now-datetime(now.year, now.month, now.day)\n",
        "            else: _._time = _time(days=d, hours=h, minutes=m, seconds=s, milliseconds=ms)\n",
        "        _.clear('d','h','m','s','ms')\n",
        "    def __sub__(_, dtime):\n",
        "        of=type(dtime); sets=_._clonable()\n",
        "        if issubclass(of, _time): return at(_._time-dtime, **sets)\n",
        "        elif issubclass(of, at): sets.update(dtime._clonable()); return at(_._time-dtime._time, **sets)\n",
        "    def __add__(_, dtime):\n",
        "        of=type(dtime); sets=_._clonable()\n",
        "        if issubclass(of, _time): return at(_._time+dtime, **sets)\n",
        "        elif issubclass(of, at): sets.update(dtime._clonable()); return at(_._time+dtime._time, **sets)\n",
        "    def __str__(_): return str(_._time)\n",
        "    @property\n",
        "    def seconds(_): return _._time.seconds\n",
        "    @property\n",
        "    def S(_): return _.seconds\n",
        "    @property\n",
        "    def minutes(_): return _._time.seconds/60\n",
        "    @property\n",
        "    def M(_): return _.minutes\n",
        "    @property\n",
        "    def hours(_): return _.minutes/60\n",
        "    @property\n",
        "    def H(_): return _.hours\n",
        "    @property\n",
        "    def days(_): return _._time.days\n",
        "    @property\n",
        "    def D(_): return _.days\n",
        "    @staticmethod\n",
        "    def zero(): return at(_time())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from inspect import isfunction, ismethod, isgeneratorfunction, isgenerator, isroutine\n",
        "from inspect import isabstract, isclass, ismodule, istraceback, isframe, iscode, isbuiltin\n",
        "from inspect import ismethoddescriptor, isdatadescriptor, isgetsetdescriptor, ismemberdescriptor\n",
        "from inspect import isawaitable, iscoroutinefunction, iscoroutine\n",
        "\n",
        "from collections.abc import Iterable as iterable\n",
        "\n",
        "import pickle\n",
        "\n",
        "def isfx(field): return ismethod(field) or isfunction(field)\n",
        "\n",
        "class GhostSet:\n",
        "    \"\"\" enhanced interface (ghost) to retrieve class fields \"\"\"\n",
        "    def _meta(data): return {k:v for k,v in data.__dict__.items() if not isfx(v)}\n",
        "    def _at_last(_, sets): pass\n",
        "    def _set(object, **sets):\n",
        "        ''' use to fast initialize fields | needed to avoid initialization problems at copy by value '''\n",
        "        for field in sets: setattr(object, field, sets[field])\n",
        "        object._at_last(sets)\n",
        "GSet = GhostSet\n",
        "\n",
        "def meta(object):\n",
        "    ''' retrieves clonable object metadata (__dict__) as a copy '''\n",
        "    if isinstance(object, GSet): return object._meta()\n",
        "    return {}\n",
        "\n",
        "class ClonableObjectGhost:\n",
        "    \"\"\" enhanced interface (ghost) for clonable objects \"\"\"\n",
        "    def _by_val(_, depth=-1, _layer=0): pass\n",
        "GCo = ClonableObjectGhost\n",
        "\n",
        "class ClonableObject(GSet, GCo):\n",
        "    \"\"\" base clonable object \"\"\"\n",
        "    def __init__(this, **data): this._set(**data)\n",
        "    def __call__(_, **options): _._set(**options)\n",
        "    def _by_val(_, depth=-1, _layer=0):\n",
        "        copy = type(_)()\n",
        "        copy._set(**_._meta())\n",
        "        if depth<0 or depth>_layer:\n",
        "            for field in copy.__dict__:\n",
        "                if isinstance(copy.__dict__[field], ClonableObjectGhost):\n",
        "                    copy.__dict__[field] = copy.__dict__[field]._by_val(depth,_layer+1)\n",
        "        return copy\n",
        "COb = ClonableObject\n",
        "\n",
        "def copy_by_val(object, depth=-1, _layer=0):\n",
        "    if isinstance(object, GCo): return object._by_val(depth,_layer)\n",
        "    return object\n",
        "copy = by_val = vof = copy_by_val\n",
        "\n",
        "class ComparableGhost:\n",
        "    \"\"\" enhanced interface (ghost) for comparing instances \"\"\"\n",
        "    def _compare(a, b):\n",
        "        if type(a) != type(b): return False\n",
        "        if a.__dict__ == b.__dict__: return True\n",
        "        return False\n",
        "    def __eq__(a, b): return a._compare(b)\n",
        "GEq = ComparableGhost\n",
        "\n",
        "class IterableObjectGhost(GSet):\n",
        "    \"\"\" enhanced interface (ghost) for iterables: exposes __dict__,\n",
        "        therefore Iterable Objects are like lua dictionaries \"\"\"\n",
        "    def __contains__(this, key): return key in this.__dict__\n",
        "    def __iter__(this): return iter(this.__dict__)\n",
        "    def items(my): return my.__dict__.items()\n",
        "    def __getitem__(by, field): return by.__dict__[field]\n",
        "    def __setitem__(by, field, value): by.__dict__[field] = value\n",
        "    def pop(by, field): return by.__dict__.pop(field)\n",
        "GIo = IterableObjectGhost\n",
        "\n",
        "class ReprGhost:\n",
        "    \"\"\" enhanced interface (ghost) for the skeleton method _repr,\n",
        "        see implementation of Struct for a working example;\n",
        "        Record __repr__ override uses _lines_ for max lines display \"\"\"\n",
        "    _lines_ = 31\n",
        "    _chars_ = 13\n",
        "    _msgsz_ = 62\n",
        "    _ellipsis_ = ' ... '\n",
        "    def _repr(my, value):\n",
        "        _type = ''.join(''.join(str(type(value)).split('class ')).split(\"'\"))\n",
        "        _value = '{}'.format(value)\n",
        "        if len(_value)>my._chars_:\n",
        "            show = int(my._chars_/2)\n",
        "            _value = _value[:show]+my._ellipsis_+_value[-show:]\n",
        "        return '{} {}'.format(_type, _value)\n",
        "    def _resize(this, message, at=.7):\n",
        "        if len(message)>this._msgsz_:\n",
        "            start = int(at*this._msgsz_)\n",
        "            end = this._msgsz_-start\n",
        "            return message[:start]+this._ellipsis_+message[-end:]\n",
        "        return message\n",
        "GRe = ReprGhost\n",
        "\n",
        "def set_repr_to(lines): GRe._lines_ = lines\n",
        "\n",
        "class Struct(COb, GEq, GIo, GRe):\n",
        "    \"\"\" structured autoprintable object, behaves like a lua dictionary \"\"\"\n",
        "    def __repr__(_):\n",
        "        return '\\n'.join(['{}:\\t{}'.format(k, _._repr(v)) for k,v in _.items()])\n",
        "struct = Struct\n",
        "\n",
        "class RecordableGhost:\n",
        "    \"\"\" enhanced interface (ghost) for type recording,\n",
        "        see Record for a working example \"\"\"\n",
        "    @staticmethod\n",
        "    def load(filename):\n",
        "        with open(filename, 'rb') as file: return pickle.load(file)\n",
        "    def save(data, filename):\n",
        "        with open(filename, 'wb') as file: pickle.dump(data, file)\n",
        "        \n",
        "GRec = RecordableGhost\n",
        "\n",
        "class Record(GSet, GCo, GRec, GEq, GRe):\n",
        "    \"\"\" wrapper for any object or value, auto-inspects and provides load/save type structure \"\"\"\n",
        "    data = None\n",
        "    _check = dict(\n",
        "            isfunction=isfunction, ismethod=ismethod, isgeneratorfunction=isgeneratorfunction, isgenerator=isgenerator, isroutine=isroutine,\n",
        "            isabstract=isabstract, isclass=isclass, ismodule=ismodule, istraceback=istraceback, isframe=isframe, iscode=iscode, isbuiltin=isbuiltin,\n",
        "            ismethoddescriptor=ismethoddescriptor, isdatadescriptor=isdatadescriptor, isgetsetdescriptor=isgetsetdescriptor, ismemberdescriptor=ismemberdescriptor,\n",
        "            isawaitable=isawaitable, iscoroutinefunction=iscoroutinefunction, iscoroutine=iscoroutine\n",
        "                   )\n",
        "    def __init__(this, token, **meta):\n",
        "        this.data = token\n",
        "        this.__dict__.update({k:v(token) for k,v in this._check.items()})\n",
        "        super()._set(**meta)\n",
        "    @property\n",
        "    def type(_): return type(_.data)\n",
        "    def inherits(_, *types): return issubclass(_.type, types)\n",
        "    @property\n",
        "    def isbaseiterable(_): return _.inherits(tuple, list, dict, set) or _.isgenerator or _.isgeneratorfunction\n",
        "    @property\n",
        "    def isiterable(_): return isinstance(_.data, iterable) and _.type is not str\n",
        "    def _clone_iterable(_):\n",
        "        if _.inherits(dict): return _.data.copy()\n",
        "        elif _.isgenerator or _.isgeneratorfunction: return (i for i in list(_.data))\n",
        "        else: return type(_.data)(list(_.data)[:])\n",
        "    def _meta(data): return {k:v for k,v in data.__dict__.items() if k != 'data' and not isfx(v)}\n",
        "    def _by_val(_, depth=-1, layer=0):\n",
        "        data = _.data\n",
        "        if _.isiterable: data = _._clone_iterable()\n",
        "        elif _.inherits(ClonableObjectGhost): data = by_val(data, depth, layer)\n",
        "        return type(_)(data, **meta(_))\n",
        "    def __enter__(self): self._instance = self; return self\n",
        "    def __exit__(self, type, value, traceback): self._instance = None\n",
        "    def __repr__(self):\n",
        "        if not hasattr(self, '_preprint'): return Record(self.data, _preprint='', _lines=Record(Record._lines_)).__repr__()\n",
        "        if self.isbaseiterable:\n",
        "            pre, repr = self._preprint, ''\n",
        "            for n,i in enumerate(self.data):\n",
        "                if self._lines.data == 0: break\n",
        "                else: self._lines.data -= 1\n",
        "                index, item = str(n), i\n",
        "                if self.inherits(dict): index += ' ({})'.format(str(i)); item = self.data[i]\n",
        "                repr += pre+'{}: '.format(index)\n",
        "                next = Record(item, _preprint=pre+'\\t', _lines=self._lines)\n",
        "                if next.isiterable: repr += '\\n'\n",
        "                repr += next.__repr__()\n",
        "                repr += '\\n'\n",
        "            return repr\n",
        "        elif self.inherits(GCo): return Record(self.data._meta(), _preprint=self._preprint, _lines=self._lines).__repr__()\n",
        "        else: return self._repr(self.data)\n",
        "REc = Record\n",
        "\n",
        "class Bisect(list, COb):\n",
        "    \"\"\" bisect implementation using clonable objects \"\"\"\n",
        "    def __init__(set, *items, key=None, reverse=False):\n",
        "        if not key: key = lambda  x:x\n",
        "        super().__init__(sorted(items, reverse=reverse, key=key))\n",
        "    def _bisect(set, item, key, reverse, bottom, top):\n",
        "        def _(check):\n",
        "            if key: return key(check)\n",
        "            return check\n",
        "        at = int((top-bottom)/2)+bottom\n",
        "        if len(set)==0: return (0,-1)\n",
        "        if item==_(set[at]): return (at,0)\n",
        "        bigger = item<_(set[at])\n",
        "        if bigger != reverse:\n",
        "            if at-bottom>0: return set._bisect(item, key, reverse, bottom, at)\n",
        "            return (at,-1)\n",
        "        elif top-at>1: return set._bisect(item, key, reverse, at, top)\n",
        "        return (at,1)\n",
        "    def search(_, item, key=None, reverse=False):\n",
        "        if not key: key = lambda x:x\n",
        "        return _._bisect(item, key, reverse, 0, len(_))\n",
        "    def _by_val(_, depth=-1, _layer=0):\n",
        "        copy = super()._by_val(depth, _layer)\n",
        "        copy += _[:]\n",
        "        return copy\n",
        "BSx = Bisect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from numpy import ndarray, resize, linspace, arange\n",
        "from numpy import min, max, average, floor\n",
        "from numpy import ubyte, zeros, array\n",
        "from scipy.signal import lfilter, butter\n",
        "from matplotlib import pylab as lab\n",
        "\n",
        "_NOTCH = _FR = 50\n",
        "_SAMPLING = 500\n",
        "_CONTINUOUS = 1\n",
        "_UNIT = 'ms'\n",
        "\n",
        "class rec(table, ndarray):\n",
        "    @property\n",
        "    def dimensions(of): return len(of.shape)\n",
        "    @property\n",
        "    def is_scalar(this): return this.shape is ()\n",
        "    @property\n",
        "    def is_vector(this): return len(this.shape)==1\n",
        "    @property\n",
        "    def is_matrix(this): return len(this.shape)>1\n",
        "    @property\n",
        "    def is_cube(this): return len(this.shape) == 3\n",
        "    @property\n",
        "    def is_binary(this): return this.dtype == ubyte and max(this) == 1\n",
        "    @property\n",
        "    def serialized(data):\n",
        "        if not data.is_scalar and data.dimensions>1:\n",
        "            return rec.read(data.T.flatten(), _deser=data.T.shape, **meta(data))\n",
        "        return data\n",
        "    @property\n",
        "    def deserialized(data):\n",
        "        if data.has('_deser'):\n",
        "            deser = rec.read(resize(data, data._deser).T, **meta(data))\n",
        "            deser.clear('_deser')\n",
        "            return deser\n",
        "        return data\n",
        "    @property\n",
        "    def as_matrix(data):    #implement numpy matrix\n",
        "        if data.is_vector: return rec.read([data], to=type(data), **meta(data))\n",
        "        return data\n",
        "    @property\n",
        "    def raw(data):\n",
        "        if data.shape[0] == 1: return rec.read(data[0], **meta(data)).raw\n",
        "        return data\n",
        "    def join(base, *parts, **sets):\n",
        "        flip, parts = None, list(parts)\n",
        "        if 'flip' in sets: flip=sets.pop('flip')\n",
        "        next = parts[0]\n",
        "        if len(parts)>1: next = rec.join(parts[0], parts[1:])\n",
        "        congruent = base.dimensions == next.dimensions and base.dimensions < 3\n",
        "        if congruent:\n",
        "            sets.update(base._clonable())\n",
        "            A, B = base, next\n",
        "            if flip: A, B = base.T, next.T\n",
        "            C = record(A.tolist()+B.tolist(), **sets)\n",
        "            if flip: return record(C.T, **sets)\n",
        "            return C\n",
        "    def get_as(this, data, cast=None):\n",
        "        source = this\n",
        "        if no(cast):\n",
        "            if issubclass(type(data), rec): cast = type(data)\n",
        "            else: cast = type(this)\n",
        "        if issubclass(type(data), ndarray): source = resize(this, data.shape)\n",
        "        return rec.read(source, to=cast, **meta(data))\n",
        "    @staticmethod\n",
        "    def read(iterable, to=None, **sets):\n",
        "        if no(to) or not issubclass(to, rec): to = rec\n",
        "        data = array(iterable).view(to)\n",
        "        data.set(**sets)\n",
        "        return data\n",
        "    def clone(this, **sets):\n",
        "        copy = this.copy().view(type(this))\n",
        "        sets.update(this._clonable())\n",
        "        copy.set(**sets)\n",
        "        return copy\n",
        "    def exclude(data, *items, **sets):\n",
        "        if no(items) or data.is_scalar: return data\n",
        "        excluded, items = None, [item for item in range(len(data)) if item not in items]\n",
        "        if data.is_vector: excluded = rec.read([data])[:,items][0]\n",
        "        else: excluded = data[items,:]\n",
        "        return rec.read(excluded, to=type(data), **meta(data), **sets)\n",
        "    def include(data, *items, **sets):\n",
        "        if no(items) or data.is_scalar: return data\n",
        "        included = []\n",
        "        if data.is_vector: included = rec.read([data])[:,items][0]\n",
        "        else: included = data[items,:]\n",
        "        return rec.read(included, to=type(data), **meta(data), **sets)\n",
        "\n",
        "create = record = rec.read\n",
        "line = linspace\n",
        "\n",
        "def series(ori,end=None,by=1):\n",
        "    if no(end): end=ori; ori=0\n",
        "    if not issubclass(type(by), int): return create(arange(ori,end,by))\n",
        "    return array(range(ori,end,by))\n",
        "\n",
        "def plot(data, at = 0., spacing = 1., color = 'k', width = 1., offset=0.): #review\n",
        "    draw = record(data, **meta(data)); at = spacing*draw.as_matrix.shape[0]\n",
        "    axes = lab.gca(); axes.set_ylim([at+max(data),0-max(data)]); at=0\n",
        "    for n, row in ni(draw.as_matrix):\n",
        "        if some(offset): row = draw[n]-average(row)+offset\n",
        "        c, w = color, width\n",
        "        if isiterable(color): c = color[n]\n",
        "        if isiterable(width): w = width[n]\n",
        "        lab.plot(at+row+n*spacing, color = c, linewidth = w)\n",
        "\n",
        "def butter_type(lowcut, highcut, fs, order=5, type='band'):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = butter(order, [low, high], btype=type)\n",
        "    return b, a\n",
        "\n",
        "def butter_filter(data, lowcut, highcut, fs, order=5, type='band'):\n",
        "    b, a = butter_type(lowcut, highcut, fs, order=order, type=type)\n",
        "    y = lfilter(b, a, data)\n",
        "    return y\n",
        "\n",
        "def _to_rec(this):\n",
        "    if not issubclass(type(this), rec): return rec.read(this, **meta(this)), rec\n",
        "    return this, type(this)\n",
        "\n",
        "def _prefilt(data, fs):\n",
        "    pre = []\n",
        "    for line in data.as_matrix:\n",
        "        vector = line.tolist()\n",
        "        pre.append(vector[:int(fs)]+vector)\n",
        "    return record(pre)\n",
        "\n",
        "def _postfilt(data, fs):\n",
        "    post = []\n",
        "    for line in data:\n",
        "        vector = line.tolist()\n",
        "        post.append(vector[int(fs):])\n",
        "    return post\n",
        "\n",
        "def notch(this, using=butter_type, fs=_SAMPLING, size=2, at=_NOTCH, order=5):\n",
        "    data, type = _to_rec(this)\n",
        "    if data.has('sampling'): fs=data.sampling\n",
        "    nyq, cutoff = fs / 2., []\n",
        "    for f in range(int(at), int(nyq), int(at)):\n",
        "        cutoff.append((f - size, f + size))\n",
        "    signal = _prefilt(data, fs)\n",
        "    for bs in cutoff:\n",
        "        low,hi = bs\n",
        "        b,a = butter_type(low,hi,fs,order,'bandstop')\n",
        "        signal = lfilter(b,a,signal)\n",
        "    return record(_postfilt(signal, fs), to=type, **meta(data))\n",
        "\n",
        "def band(this, low_high, fs=_SAMPLING, using=butter_filter, order=5):\n",
        "    data, type = _to_rec(this)\n",
        "    if data.has('sampling'): fs=data.sampling\n",
        "    low, high = min(low_high), max(low_high)\n",
        "    if low<1.: low = 1.\n",
        "    tailed = _prefilt(data, fs)\n",
        "    tailed = using(tailed, low, high, fs, order)\n",
        "    return rec.read(_postfilt(tailed, fs), to=type, **meta(data))\n",
        "\n",
        "def binarize(this):\n",
        "    data, type = _to_rec(this)\n",
        "    if data.is_binary: return data\n",
        "    rows = []\n",
        "    for row in data.as_matrix:\n",
        "        d = row - array([row[-1]]+row[:-1].tolist())\n",
        "        d[d>=0] = 1; d[d<0] = 0\n",
        "        rows.append(d.astype(ubyte))\n",
        "    return rec.read(rows, to=type).get_as(data)\n",
        "\n",
        "def halve(matrix):\n",
        "    halved, (data, type) = [], _to_rec(matrix)\n",
        "    for line in data.as_matrix:\n",
        "        h = resize(line, (int(len(line)/2), 2))\n",
        "        halved.append((h[:,0]+h[:,1])/2.)\n",
        "    return rec.read(halved, to=type, **meta(data))\n",
        "\n",
        "def dwindle(matrix, by=1):\n",
        "    if by: return dwindle(halve(matrix), by-1)\n",
        "    return matrix\n",
        "\n",
        "def upsample(matrix, fs1, fs2):\n",
        "    y=zeros((matrix.shape[0],fs2))\n",
        "    if fs1 < fs2:\n",
        "        #upsampling by a factor R\n",
        "        L=matrix.shape[1]\n",
        "        R=int(floor(fs2/fs1)+1)\n",
        "        for i,e in enumerate(matrix):\n",
        "            ups=[]\n",
        "            for j in range(L-1):\n",
        "                if j>0: ups.append(list(linspace(matrix[i][j],matrix[i][j+1],R)[1:3]))\n",
        "                else: ups.append(list(linspace(matrix[i][j],matrix[i][j+1],R)[0:3]))\n",
        "            for k,s in enumerate(sum(ups, [])): y[i][k]=s \n",
        "            y[i][-1]=y[i][-2]\n",
        "        return rec.read(y)\n",
        "    else: print(\"Error: fs1 >= fs2\")\n",
        "\n",
        "def remap(this, axis=None, base=0, top=1., e=0):\n",
        "    def map(x, b, t, e): return ((x-min(x)+e)/(max(x)-min(x)+e)+b)*(t-b)\n",
        "    data, type = _to_rec(this)\n",
        "    if no(axis): return rec.read(map(this, base, top, e), to=type, **meta(data))\n",
        "    rows = data.as_matrix\n",
        "    if axis==0 or axis>1: rows = rows.T\n",
        "    remapped = []\n",
        "    for row in rows: remapped.append(map(row, base, top, e))\n",
        "    if axis==0 or axis>1: rows = rows.T\n",
        "    return rec.read(remapped).get_as(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "this = Record\n",
        "\n",
        "from numpy import array, average, median, random\n",
        "import scipy.stats as stats\n",
        "\n",
        "class Table(COb, GEq, GRe):\n",
        "    data = None\n",
        "    default = None\n",
        "    PAD = 3\n",
        "    ELLIPSIS_AT = int(GRe._lines_*.3)\n",
        "    class _axes(list):\n",
        "        def insert(_from, this, item):\n",
        "            super().insert(this, item)\n",
        "            _from.__dict__[item.name] = item\n",
        "        def __setitem__(_, pos, axis):\n",
        "            super().__setitem__(pos, axis)\n",
        "            _.__dict__[axis.name] = axis\n",
        "    class axis(list, GSet, GRe):\n",
        "        name = None\n",
        "        root = None\n",
        "        _to = 0\n",
        "        def __init__(axis, root, labels, name='ax', force_at=None):\n",
        "            if force_at: root.axes[force_at].name = None\n",
        "            with this(labels) as dim:\n",
        "                if not dim.isiterable and dim.inherits(int): labels = range(labels)\n",
        "            super().__init__(labels)\n",
        "            names = [ax.name for ax in root.axes]\n",
        "            name_, n = name, 1\n",
        "            while name in names: name = name_ + str(n); n+=1\n",
        "            axis._set(root=root, name=name)\n",
        "            if force_at: root.axes[force_at] = axis\n",
        "            else: root.axes.insert(0, axis)\n",
        "        def at(axis, field):\n",
        "            field = int(field) if this(field).inherits(str) and field.isdecimal() else field\n",
        "            found = axis.index(field) if field in axis else None\n",
        "            axis._to = found if found is not None else field\n",
        "        def __repr__(_):\n",
        "            return '{}: {}'.format(_.name, _._resize(' '.join([str(i) for i in _])))\n",
        "    def __init__(this, **table_description):\n",
        "        super().__init__(axes=this._axes())\n",
        "        this.set(**table_description)\n",
        "    def reset(data):\n",
        "        base = None\n",
        "        if len(data.axes)>0:\n",
        "            base = [data.default]*len(data.axes[-1])\n",
        "            for ax in reversed(data.axes[0:-1]): base = [base]*len(ax)\n",
        "        data._set(data=array(base))\n",
        "    @property\n",
        "    def ax_names(_): return [ax.name for ax in _.axes]\n",
        "    def at(data, axis):\n",
        "        with this(axis) as _axis:\n",
        "            if _axis.inherits(int):\n",
        "                if axis>0 and axis<len(data.axes): return data.axes[axis]\n",
        "            elif _axis.inherits(str):\n",
        "                axes = data.ax_names\n",
        "                if axis in axes: return data.axes[axes.index(axis)]\n",
        "        return None\n",
        "    def _check(build):\n",
        "        if build.data is None: build.reset()\n",
        "        return build.data\n",
        "    def _find(_, inverted, ax_field):\n",
        "        _._check()\n",
        "        def index(axis, entry):\n",
        "            fields = _.at(axis)\n",
        "            if fields is not None:\n",
        "                if this(entry).isiterable:\n",
        "                    return tuple([fields.index(field) for field in entry])\n",
        "                else: return ':'\n",
        "            return None\n",
        "        def translate(axis, found):\n",
        "            if axis.name in found:\n",
        "                _range = found[axis.name]\n",
        "                if this(_range).inherits(tuple):\n",
        "                    if inverted: found[axis.name] = tuple([field for field in range(len(axis)) if field not in _range])\n",
        "                    return \"_from['{}']\".format(axis.name)\n",
        "            return ':'\n",
        "        found={field:index(field,entry) for field,entry in ax_field.items()}\n",
        "        found={field:value for field,value in found.items() if value is not None}\n",
        "        reshape='M['+','.join([translate(axis,found) for axis in _.axes])+']'\n",
        "        _._set(_reshape_ = (reshape, found))\n",
        "    def _by_val(_, depth=-1, _layer=0):\n",
        "        M, axes = _._check(), _.axes\n",
        "        do, _from = _.__dict__.pop('_reshape_') if '_reshape_' in _._meta() else (None, {})\n",
        "        copy = super()._by_val(depth, _layer)\n",
        "        copy.axes = []\n",
        "        for ax in reversed(axes):\n",
        "            fields = [field for n,field in enumerate(ax) if n in _from[ax.name]] if ax.name in _from else ax\n",
        "            copy.set(**{ax.name:fields})\n",
        "        copy.data = eval(do) if do else M.copy()\n",
        "        return copy\n",
        "    def _translate(_, directions):\n",
        "        axes = directions.split(',')\n",
        "        for ax_dir in axes:\n",
        "            ax, field = [token.strip() for token in ax_dir.split(':')]\n",
        "            axis = _.at(ax)\n",
        "            if axis: axis.at(field)\n",
        "        return '['+','.join([str(ax._to) for ax in _.axes])+']'\n",
        "    def _get_set(_, directions, mode='get', value=None):\n",
        "        if mode == 'get' and not '_MGET' in _.sets: _._MGET = []\n",
        "        if len(directions) == len(_.axes):\n",
        "            resolve, message = True, []\n",
        "            for n,part in enumerate(directions):\n",
        "                _part = this(part)\n",
        "                if _part.inherits(int, str) or _part.isiterable and len(part)==1:\n",
        "                    token = part if _part.inherits(str, int) else part[0]\n",
        "                    message.append(':'.join([str(_.axes[n].name),str(part)]))\n",
        "                else:\n",
        "                    resolve = False\n",
        "                    for token in part:\n",
        "                        redirection = list(directions)\n",
        "                        redirection[n] = token\n",
        "                        _._get_set(tuple(redirection), mode, value)\n",
        "            if resolve:\n",
        "                message = ','.join(message)\n",
        "                if mode=='get': _._MGET.append(_[message])\n",
        "                else: _[message] = value\n",
        "    def __getitem__(by, field_directions):\n",
        "        M = by._check()\n",
        "        if this(field_directions).inherits(tuple):\n",
        "            by._get_set(field_directions)\n",
        "            result = by.__dict__.pop('_MGET')\n",
        "            return result\n",
        "        else: return eval('M'+by._translate(field_directions))\n",
        "    def __setitem__(by, field_directions, value):\n",
        "        M = by._check()\n",
        "        if this(field_directions).inherits(tuple): by._get_set(field_directions, 'set', value)\n",
        "        else: exec('M'+by._translate(field_directions)+'=value')\n",
        "    def set(data, **ax_field):\n",
        "        for name, fields in ax_field.items(): data.axis(data, fields, name)\n",
        "    def get(data, **ax_field):\n",
        "        data._find(0, ax_field)\n",
        "        return data._by_val()\n",
        "    def let(data, **ax_field):\n",
        "        data._find(1, ax_field)\n",
        "        return data._by_val()\n",
        "    @property\n",
        "    def sets(tree): return set(meta(tree))\n",
        "    def __repr__(self):\n",
        "        M = self._check()\n",
        "        _repr, dimensions = '', len(self.axes)\n",
        "        if not dimensions: _repr += 'void table\\n'\n",
        "        else:\n",
        "            dimensions = len(self.axes)\n",
        "            y = self.axes[-2] if dimensions >= 2 else None\n",
        "            if dimensions>2:\n",
        "                y = self.axes[-2]\n",
        "                for n,ax in enumerate(self.axes[:-2]): _repr += '{}{}: {}/{}\\n'.format('\\t'*n, ax.name, ax._to, len(ax))\n",
        "            mr = eval('M'+str([ax.index(ax._to) for ax in self.axes][:-2])) if dimensions>2 else M\n",
        "            pad = max([len(y.name)]+[len(str(field)) for field in y]+[len(str(value)) for line in mr for value in line])+self.PAD if dimensions>1 else 0\n",
        "            _repr, x, spaces = _repr+y.name+'\\n' if y else '', self.axes[-1], ' '*pad if pad>0 else '\\t'\n",
        "            header = spaces+''.join([str(field).ljust(pad) for field in x])\n",
        "            _repr += self._resize(header) + '\\n'\n",
        "            ellipsis_at = self._lines_-self.ELLIPSIS_AT-1\n",
        "            last_values_from = len(mr)-self.ELLIPSIS_AT\n",
        "            if last_values_from<=ellipsis_at: last_values_from = ellipsis_at+1\n",
        "            for n, line in enumerate(mr):\n",
        "                if n<ellipsis_at or n>last_values_from:\n",
        "                    values = str(y[n]).ljust(pad) if y else ''\n",
        "                    values += ''.join([str(value).ljust(pad) for value in line])\n",
        "                    _repr += self._resize(values) + '\\n'\n",
        "                elif n==ellipsis_at:\n",
        "                    _repr += self._ellipsis_ + '\\n'\n",
        "        extra = {k:v for k,v in meta(self).items() if k != 'data' and k != 'axes'}\n",
        "        _repr += self._resize(spaces*len(x)+x.name)+'\\n'+'\\n'.join(['{}:\\t{}'.format(k, self._repr(v)) for k,v in extra.items()])\n",
        "        return _repr\n",
        "TAb = tab = Table\n",
        "\n",
        "def set_repr_to(lines, ratio=.7):\n",
        "    set_repr_to(lines)\n",
        "    Table.ELLIPSIS_AT = int(Table._lines_*(1-ratio))\n",
        "\n",
        "def butter_type(lowcut, highcut, fs, order=5, type='band'):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = butter(order, [low, high], btype=type)\n",
        "    return b, a\n",
        "\n",
        "def butter_filter(data, lowcut, highcut, fs, order=5, type='band'):\n",
        "    b, a = butter_type(lowcut, highcut, fs, order=order, type=type)\n",
        "    y = lfilter(b, a, data)\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install pyEDFlib # installing the pyEDFlib library in the Google Colab environment\n",
        "\n",
        "import pyedflib as edf\n",
        "\n",
        "class EEG(Table):\n",
        "    LABEL_START = 'EEG '\n",
        "    BAD = ['TTL', 'ECG']\n",
        "    BP_SEP = '-'\n",
        "    class time:\n",
        "        \"\"\" converts time units to seconds by frequency sampling (fs) \"\"\"\n",
        "        unit = 'units'\n",
        "        def __init__(_, units): _.time = units\n",
        "        def __call__(_, fs=None): return _.time/fs\n",
        "        def __repr__(_): return '{} {}'.format(str(_.time),_.unit)\n",
        "    class ms(time):\n",
        "        \"\"\" converts ms to time units by frequency sampling (fs) \"\"\"\n",
        "        unit = 'ms'\n",
        "        def __call__(_, fs=1000): return int(round(_.time*fs/1000))\n",
        "    class secs(time):\n",
        "        \"\"\" converts seconds to time units by frequency sampling (fs) \"\"\"\n",
        "        unit = 's'\n",
        "        def __call__(_, fs=1000): return int(_.time*fs)\n",
        "    def _load(eeg, epoch, n):\n",
        "        data = None\n",
        "        with edf.EdfReader(eeg.file) as file:\n",
        "            data = [file.readSignal(eeg.labels[id], epoch.at, epoch.span) for id in eeg.labels]\n",
        "            file.close()\n",
        "        if data is not None:\n",
        "            eeg._set(data=array(data), at_epoch=(n, epoch()))\n",
        "        else:\n",
        "            if 'at_epoch' in eeg.sets: del(eeg.at_epoch)\n",
        "            eeg._set(data=None)\n",
        "    class step(GSet):\n",
        "        START = 0\n",
        "        CENTRE = 1\n",
        "        END = 2\n",
        "        def __init__(step, space, duration):\n",
        "            step._set(at=space, span=duration)\n",
        "        def reset(grid, at=0, root=None):\n",
        "            if root: grid._set(root=root)\n",
        "            else: root = grid.root\n",
        "            all_space, left = root.duration(root.fs), 0\n",
        "            if grid.at.time == 0: epochs = [EEG.step(0, all_space)]\n",
        "            else:\n",
        "                space, span, epochs = grid.at(root.fs), grid.span(root.fs), []\n",
        "                for x in range(at, all_space, space):\n",
        "                    end = x+span\n",
        "                    if end>all_space: left = all_space-x\n",
        "                    else: epochs.append(EEG.step(x, span))\n",
        "            grid._set(_all=epochs, skip=at, out=left)\n",
        "        def __call__(step, _as=None):\n",
        "            if 'root' in meta(step):\n",
        "                if _as == None: return len(step._all)\n",
        "            elif _as is not None: step.id = _as\n",
        "            elif 'id' in meta(step): return step.id\n",
        "        def items(wrapped):\n",
        "            if 'root' in meta(wrapped): return wrapped._all\n",
        "        def __getitem__(by, epoch_n):\n",
        "            if 'root' in meta(by) and epoch_n<len(by._all):\n",
        "                by.root._load(by._all[epoch_n], epoch_n)\n",
        "        def __repr__(_): return '|'.join([repr(_.at),repr(_.span)])\n",
        "    class event(GSet):\n",
        "        def __init__(event, to=None, group=None, _as=0, _from=0):\n",
        "            event._set(mode=_from, note=group, id=_as)\n",
        "            if to is not None: event.link(to)\n",
        "        def link(event, to):\n",
        "            if event.note is None or not 'event' in to.sets:\n",
        "                if event.note is None:\n",
        "                    to.event = event\n",
        "                    event.type = []\n",
        "                    return\n",
        "                else: EEG.event(to)\n",
        "            types = to.event.type\n",
        "            ids = [to.event.id]+[_type.id for _type in types]\n",
        "            while event.id in ids: event.id += 1\n",
        "            if event.note in to.notes:\n",
        "                event.at = to.notes[event.note]\n",
        "                types.append(event)\n",
        "        def __repr__(event):\n",
        "            _repr = str(event.id)\n",
        "            if 'at' in meta(event): _repr += ' at: {}'.format(event.at)\n",
        "            if 'type' in meta(event):\n",
        "                for subev in event.type: _repr += '; '+repr(subev)\n",
        "            return _repr\n",
        "    def _at_last(eeg, sets):\n",
        "        if 'epoch' in meta(eeg):\n",
        "            eeg.epoch.reset(root=eeg)\n",
        "            if len(eeg.axes.time) != eeg.epoch.span(eeg.fs): eeg.axis(eeg, eeg.epoch.span(eeg.fs), 'time', 1)\n",
        "    @staticmethod\n",
        "    def from_file(name, step=None, bad=None):\n",
        "        def correct_(label):\n",
        "            if label.startswith(EEG.LABEL_START): return label[len(EEG.LABEL_START):]\n",
        "            return label\n",
        "        eeg = EEG()\n",
        "        with edf.EdfReader(name) as file:\n",
        "            if bad is None: bad = EEG.BAD\n",
        "            duration = EEG.secs(file.getFileDuration())\n",
        "            fs = file.getSampleFrequencies()[0]\n",
        "            if step is None: step = EEG.step(EEG.secs(0), duration)\n",
        "            raw_notes = file.readAnnotations()\n",
        "            notes = {note:[] for note in set(raw_notes[-1])}\n",
        "            for n, note in enumerate(raw_notes[-1]):\n",
        "                notes[note].append(EEG.secs(raw_notes[0][n]))\n",
        "            labels = [correct_(label) for label in file.getSignalLabels()]\n",
        "            labels = {label:n for n,label in enumerate(labels) if label not in bad}\n",
        "            eeg.set(time=step.span(fs), region=tuple(labels))\n",
        "            eeg(file=name, duration=duration, fs=fs, notes=notes, labels=labels, epoch=step)\n",
        "            file.close()\n",
        "        return eeg\n",
        "    def remap(eeg, at=None, step=None):\n",
        "        sets = eeg.sets\n",
        "        if this(step).inherits(EEG.step): eeg._set(epoch=step)\n",
        "        if at is None:\n",
        "            at = eeg._best_map if '_best_map' in sets else 0\n",
        "        eeg.epoch.reset(at)\n",
        "        if 'event' in sets:\n",
        "            deltas = []\n",
        "            for epoch in eeg.epoch._all: epoch.id = None\n",
        "            for event in eeg.event.type:\n",
        "                for time in event.at:\n",
        "                    at, space, limit = time(eeg.fs), eeg.epoch.at(eeg.fs), len(eeg.epoch.items())-1\n",
        "                    for n,epoch in enumerate(eeg.epoch.items()):\n",
        "                        end = epoch.at+space if n<limit else epoch.span\n",
        "                        if at>=epoch.at and at<end:\n",
        "                            epoch(event.id)\n",
        "                            if event.mode == eeg.step.START: deltas.append(EEG.time(at-epoch.at)(eeg.fs))\n",
        "                            elif event.mode == eeg.step.END: deltas.append(EEG.time(end-at-1)(eeg.fs))\n",
        "                            else: \n",
        "                                centre = epoch.at+int(round(epoch.span/2))\n",
        "                                deltas.append(EEG.time(abs(at-centre))(eeg.fs))\n",
        "                            break\n",
        "            for epoch in eeg.epoch._all:\n",
        "                if epoch() is None: epoch(eeg.event.id)\n",
        "            eeg.deltas = deltas\n",
        "    def optimize(eeg, *events, grid=None):\n",
        "        if events:\n",
        "            for event in events: event.link(eeg)\n",
        "        eeg.remap(0, grid)\n",
        "        gaussian_space = stats.shapiro if len(eeg.deltas)<=5000 else stats.normaltest\n",
        "        def test():\n",
        "            _, p = gaussian_space(eeg.deltas) if len(eeg.deltas)>2 else 0,1\n",
        "            if p<=0.05: return p, median(eeg.deltas)\n",
        "            return p, average(eeg.deltas)    \n",
        "        (p, best), at, check = test(), 0, eeg.epoch.span(eeg.fs)\n",
        "        print('optimizing epoch position...', end=' ')\n",
        "        for _try in range(1, check):\n",
        "            eeg.remap(_try)\n",
        "            p, check = test()\n",
        "            if check<best: p, best, at = p, check, _try\n",
        "        _test = 'median' if p<0.05 else 'mean'\n",
        "        print('best frame found at {:.3f}s with a {} delay of {:.3f}s'.format(EEG.time(at)(eeg.fs), _test, EEG.time(best)(eeg.fs)))\n",
        "        eeg._set(_best_map=at)\n",
        "    class sampler(GSet, GRe):\n",
        "        eeg = None\n",
        "        def __init__(map, root, *reserve, **opts):\n",
        "            raw, proc = [step() for step in root.epoch.items()], []\n",
        "            find, key = None, {k:v for k,v in reserve}\n",
        "            for step in raw:\n",
        "                if find==step: find=None\n",
        "                if find is None: proc.append(step)\n",
        "                else: proc.append(None)\n",
        "                if step in key: find = key[step]\n",
        "            key = {k:[] for k in list(set(raw))+[None]}\n",
        "            for n,id in enumerate(proc): key[id].append(n)\n",
        "            map._set(eeg=root, key=key, mask=proc, **opts)\n",
        "        def _at_last(_, sets):\n",
        "            if 'seed' in sets: random.seed(_.seed)\n",
        "        def set(map, **event_range):\n",
        "            prev, key = map.key, {}\n",
        "            for k,deltas in event_range.items():\n",
        "                if k in prev:\n",
        "                    seq, key[k] = prev[k], []\n",
        "                    for item in seq: key[k] += [item+d for d in deltas]\n",
        "                    for o in prev:\n",
        "                        if o != k:\n",
        "                            for e in key[k]:\n",
        "                                if e in prev[o]: prev[o].pop(prev[o].index(e))\n",
        "            for k in prev:\n",
        "                if k not in key: key[k] = prev[k]\n",
        "            map._set(prev=prev, key=key)\n",
        "        def get(map, event, times, random_seed=None):\n",
        "            if random_seed and not 'seed' in meta(map): map._set(seed=random_seed)\n",
        "            if not 'pool' in meta(map): map._set(pool = {k:map.key[k].copy() for k in map.key})\n",
        "            resampled, sequence = [], []\n",
        "            while times:\n",
        "                if len(map.pool[event])==0: map.pool[event] = map.key[event].copy()\n",
        "                at = map.pool[event].pop(random.randint(len(map.pool[event])))\n",
        "                map.eeg.epoch[at]\n",
        "                resampled.append(map.eeg.data)\n",
        "                sequence.append(at)\n",
        "                times -= 1\n",
        "            return resampled, sequence\n",
        "        def __repr__(_): return _._resize('|'.join([str(id) if id!=None else ' ' for id in _.mask]))\n",
        "    def tag(event, *a_b, **event_range):\n",
        "        event._set(sample=event.sampler(event, *a_b))\n",
        "        event.sample.set(**event_range)\n",
        "        \n",
        "STEp = epoch = EEG.step\n",
        "TIME = EEG.time\n",
        "SET = EEG.event\n",
        "secs = EEG.secs\n",
        "ms = EEG.ms"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### The code below implements the functions used in the analytical framework.\n",
        "##### *classify_epochs()* defines the support vector machine classifier, and the K-fold method, for quantifying the connectivity change between baseline and a WOI. The connectivity change is quantified as cross-validation scores.\n",
        "##### *evaluation_function()* defines a heuristic used to evaluate the \"game state\", using the connectivity change scores (cross-validation scores from the K-fold classification). \n",
        "##### *check_until()* defines the node iteration process, limited by the game state evaluation (*evaluation_function()* output)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1NBS5bx764R"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "def classify_epochs(set, node_group, kratio=.1, random_state=31, **mopts):\n",
        "    \"\"\"Quantifies connectivity change of specified node group.\n",
        "    The support vector machine classifies time frame epochs using the connectivity measures as features. \n",
        "    K-fold cross validation scores measure the connectivity change.\n",
        "\n",
        "    Args:\n",
        "        set (core.rec): Preprocessed data - connectivity matrices per epoch.\n",
        "        nodes (set): Node group for which connectivity change is quantified. \n",
        "        kratio (float): Ratio of epochs considered in a fold. Defaults to 0.1, which is ~10s of data if epoch span is 1s.\n",
        "        random_state (int): KFold argument; Controls randomness of each fold. Defaults to 31.\n",
        "\n",
        "    Returns:\n",
        "        ndarray of float: Array of scores for each fold.\n",
        "    \"\"\"\n",
        "    X, Y = array([array((record(x).include(*node_group).T).include(*node_group)).flatten() for x in set.X]), set.y\n",
        "    model, scaler, k = SVC, StandardScaler, int(round(len(Y)*kratio))\n",
        "    if random_state == None: random_state = random.randint(0xFFFFFFFF)\n",
        "    C = Pipeline([('scaler', scaler()), ('model', model(**mopts))])\n",
        "    cv = KFold(k, shuffle=True, random_state=random_state)\n",
        "    cvs = cross_val_score(C, X, Y, cv=cv)\n",
        "    return cvs\n",
        "\n",
        "def evaluation_function(results):\n",
        "    \"\"\"Heuristic evaluation function.\n",
        "\n",
        "    Args:\n",
        "        results (ndarray): Cross validation score array.\n",
        "\n",
        "    Returns:\n",
        "        float: evaluation of results.\n",
        "    \"\"\"\n",
        "    return max(results)*(min(results)/average(results)) \n",
        "\n",
        "def evaluate_nodes(nodes, labels, results, symbol='<->', f=evaluation_function):\n",
        "    \"\"\"Service function. \n",
        "    Creates sets with node indices, labels, cross validation scores and a reference function of the scores.\n",
        "\n",
        "    Args:\n",
        "        nodes ([type]): [description]\n",
        "        labels ([type]): [description]\n",
        "        results (ndarray): Cross validation score array.\n",
        "        symbol (str): Symbol used to connect node labels within a network. Defaults to '<->'.\n",
        "        f (function): Function of cross validation scores used to rank the networks based on their connectivity change. Defaults to evaluation_function.\n",
        "\n",
        "    Returns:\n",
        "        set: A set of all the arguments.\n",
        "    \"\"\"\n",
        "    tag = symbol.join([labels[n] for n in nodes])\n",
        "    return (nodes, tag, results, f(results))\n",
        "\n",
        "def check_until(net, set=1, fall=0, at=-1, limit=None):\n",
        "    \"\"\"This function takes a list of analyzed node combinations sorted by their evaluation score.\n",
        "    The function defines an index of the list until which the node combinations are considered epileptogenic.\n",
        "\n",
        "    Node combination are iterated and the average evaluation score for each combination is computed. \n",
        "    The score is used as a threshold, above which epileptogenic nodes are selected.\n",
        "    When the score decreases the iteration stops.\n",
        "\n",
        "    Args:\n",
        "        net (list): A list of sets returned by evaluate_nodes().\n",
        "        set (int): Defaults to 1.\n",
        "        fall (int): Define score threshold (should be the highest score). Defaults to 0.\n",
        "        at (int): Denotes the index of the set, containing the results for a node group. Defaults to -1, as this is the current saving format.\n",
        "        limit (int): Define limit until which the nodes are checked. This could be useful for checking node pairs (if many node pairs have the highest score, the WOI could be uninformative).\n",
        "\n",
        "    Returns:\n",
        "        int: Index of net list.\n",
        "    \"\"\"\n",
        "    print(\"Searching for the index of the last best network...\")\n",
        "    l = len(net)\n",
        "    if limit: l = limit\n",
        "    while set < l:\n",
        "        print(\"Net =\", net[:set])\n",
        "        print(\"Score =\", [n[at] for n in net[:set]][-1])\n",
        "        score = [n[at] for n in net[:set]][-1]\n",
        "        if score>=fall:\n",
        "            fall=score\n",
        "            set+=1\n",
        "        else: break\n",
        "    print(f\"Last best network at index {set-2}\")\n",
        "    return set-1"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Define the path to your main folder with data in your Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PY6Qicmo7cik",
        "outputId": "62c6551f-0d69-495a-895a-8aadc1ce9dcb"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "main_folder = \"/content/gdrive/My Drive/epigame-folder/\"\n",
        "\n",
        "path_cm = main_folder + \"connectivity_matrices/\" \n",
        "path_net = main_folder + \"selected_network/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "borp9PXS7cik",
        "outputId": "97e2dacb-4efb-482b-8a06-463143dd38e8"
      },
      "outputs": [],
      "source": [
        "woi = input(\"Time window:\\n 1. Non-seizure (baseline)\\n 2. Pre-seizure (5 min prior to seizure)\\n 3. Pre-seizure (4 min prior to seizure)\\n 4. Pre-seizure (3 min prior to seizure)\\n 5. Pre-seizure (2 min prior to seizure)\\n 6. Pre-seizure (1 min prior to seizure)\\n 7. Transition to seizure (1 min interval)\\n 8. Transition to seizure (2 min interval)\\n 9. Transition to seizure (60% seizure length interval)\\n 10. Seizure\\n Indicate a number: \")\n",
        "\n",
        "woi_code = {'1':\"baseline\", '2':\"preseizure5\", '3':\"preseizure4\", '4':\"preseizure3\", '5':\"preseizure2\", '6':\"preseizure1\", '7':\"transition1\", '8':\"transition2\", '9':\"transition60\", '10':\"seizure\"}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "T7MD6TRL7cik"
      },
      "source": [
        "#### Next, we introduce a limitation to the maximum network size (node number) with the purpose of avoiding long processing times. The selected networks are considered too large if the node number is higher than the number of nodes within the surgical resection in patients with good surgical outcome. In patients with poor surgical outcome, the networks are considered too large if the number of nodes exceeds an arbitrary number, which we defined to be 10.\n",
        "##### The propagatory networks contain nodes which hypersynchronize secondarily when a seizure starts, but are not part of the epileptogenic network. For generalized seizures, e.g., the classification between baseline and WOI of seizure propagation would be highly accurate for all possible nodes.\n",
        "##### In this case, the algorithm will keep on adding nodes and the resulting network will ultimately contain all possible nodes. This justifies introducing a limitation to the network size, as finding such a network is not useful for improving identification of surgical targets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1jskIoe7cil"
      },
      "outputs": [],
      "source": [
        "max_net_size = 18 "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### The analysis pipeline.\n",
        "##### The following code iterates through the files of the \"*connectivity_matrices*\" folder and loads the connectivity matrices of the WOI specified by the user. \n",
        "###### The selected networks, based on the available connectivity measures, are saved as *rec* class objects in a subfolder called *selected_network*, as files with a custom extension (*RES*). \n",
        "###### The filenames indicate the preprocessing parameters, matching the connectivity matrix filenames, as: \n",
        "###### \"*SUB-preseizure5-CC-(70,180).res*\" if the signal was filtered, and\n",
        "###### \"*SUB-preseizure5-PEC.res*\" if the signal was not filtered.\n",
        "#### The saved file contains a dictionary, and under the key *data*, it saved *test_nets* and *nodes*, where\n",
        "#### - *test_nets* contains a dictionary in which keys represent all tested network sized, and values contain a lists of sets, containing node group, node indices, CVSs and evaluation score (same format as in base); \n",
        "#### - *nodes* contains a list of nodes representing the selected network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from numpy import delete\n",
        "\n",
        "def exclude_from_cm(cm, channel_id):\n",
        "    \"\"\"Excludes a node from a connectivity matrix.\n",
        "    Arguments:\n",
        "        cm (numpy array): 2D matrix with connectivity measures between all node pairs\n",
        "        channel_id (int or list): index of the node to be removed\n",
        "    Returns the updated matrix.\n",
        "    \"\"\"\n",
        "    cm = delete(cm, channel_id, 0)\n",
        "    cm = delete(cm, channel_id, 1)\n",
        "    return cm\n",
        "\n",
        "def exclude_from_sz_cm(cm_list, channel_id):\n",
        "    \"\"\"Excludes a node from connectivity matrices of seizure epochs.\n",
        "    Arguments:\n",
        "        cm_list (list of numpy arrays): list of connectivity matrices saved in the PREP file under X;\n",
        "                                        First half of the matrices are from seizure epochs, second half form basleine epochs.\n",
        "        channel_id (int or list): index of the node to be removed\n",
        "    Returns the updated list of matrices.\n",
        "    \"\"\"\n",
        "    result = []\n",
        "    for i,cm in enumerate(cm_list):\n",
        "      if i<(len(cm_list)/2):\n",
        "        # Exclude the node only from seizure matrices, which are set as the first half in the list\n",
        "        cm = exclude_from_cm(cm, channel_id)\n",
        "        result.append(cm)\n",
        "      else:\n",
        "        result.append(cm)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 741
        },
        "id": "jdk2kGSD7cim",
        "outputId": "0e0ed945-389a-4ee0-bfaa-089bd236432b"
      },
      "outputs": [],
      "source": [
        "#TODO: Explain the rec object, how to access the results and the pipeline in the markdown above.\n",
        "\n",
        "from itertools import combinations\n",
        "from os import listdir\n",
        "import matplotlib.pyplot as plt\n",
        "from joblib import Parallel, delayed\n",
        "import random as rd\n",
        "\n",
        "for file_cm in listdir(path_cm):\n",
        "  if file_cm.split(\"-\")[1]==woi_code[woi]:\n",
        "\n",
        "    print(\"\\n--------------------------------------------------------------\")\n",
        "    print(\"\\nProcessing...\")\n",
        "\n",
        "    subject_id = file_cm.split(\"/\")[-1][0:3]\n",
        "    print(\"Connectivity matrices of\", file_cm)\n",
        "\n",
        "    cm = REc.load(path_cm + file_cm).data\n",
        "\n",
        "    nodes = cm.nodes\n",
        "    node_ids = list(range(len(nodes))) \n",
        "    print(\"Number of nodes =\",len(nodes))\n",
        "    print(\"\\nNodes:\", nodes)\n",
        "\n",
        "    # For subject ASJ, for connectivity analysis using CC and SCR delta (WOI 2-5),\n",
        "    # the seizure connectivity matrices have an extra channel that was not excluded;\n",
        "    # The node was excluded from the labels list (nodes), therefore:\n",
        "    # This block of code excludes the channel \"J9-J10\", with the index of the channel \"J10-J11\" in the current labels list.\n",
        "    if subject_id==\"ASJ\" and (file_cm.split(\"-\")[2]==\"CC\" or (file_cm.split(\"-\")[2]==\"SCR\" and int(file_cm.split(\"-\")[1][-1]) in [2,3,4,5])):\n",
        "      mismatch_channel_id = cm.nodes.index(\"J10-J11\")\n",
        "      print(\"Mismatched node index:\", mismatch_channel_id)\n",
        "      cm.X = exclude_from_sz_cm(cm.X, mismatch_channel_id)\n",
        "      print(\"Check PREP file sets:\", list(cm.__dict__))\n",
        "\n",
        "    print(\"\\nTotal number of epochs =\", len(cm.X))\n",
        "    print(\"Connectivity matrix shape =\", cm.X[0].shape)\n",
        "    print(\"All matrices have the same shape:\", all([m.shape==(len(nodes),len(nodes)) for m in cm.X]))\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.imshow(cm.X[-1], cmap='Blues', interpolation='nearest')\n",
        "    plt.show()\n",
        "    print(cm.X[-1])\n",
        "\n",
        "    node_pairs = combinations(node_ids, 2)\n",
        "\n",
        "    print(\"\\nProcessing node combinations...\")\n",
        "\n",
        "    parallelize = Parallel(n_jobs=-1)(delayed(evaluate_nodes)(pair, nodes, classify_epochs(cm, pair)) for pair in node_pairs)\n",
        "    base = [p for p in parallelize]\n",
        "\n",
        "    print(f\"{len(base)} finished\")\n",
        "\n",
        "    base.sort(key=lambda x:x[-1], reverse=True)\n",
        "    best_pair = base[0]\n",
        "    best_net = [best_pair]\n",
        "    print(f\"Best node pair: {best_net}\")\n",
        "\n",
        "    best_score, net_size, possible_node_groups, test_nets = base[0][-1], 3, base[:], []\n",
        "    print(\"Best score =\", best_score)\n",
        "\n",
        "    all_node_groups = {} # This dictionary saves all tested node groups, under a key indicating net_size (number of grouped nodes) \n",
        "    all_node_groups[2] = base\n",
        "\n",
        "    while net_size <= max_net_size:\n",
        "\n",
        "      all_node_groups[net_size] = []\n",
        "\n",
        "      print(f\"\\nChecking networks with {net_size} nodes...\")\n",
        "\n",
        "      head = check_until(possible_node_groups, fall=best_score)\n",
        "      \n",
        "      count_node_groups = 0\n",
        "\n",
        "      # The condition below checks if all tested node groups have the same score (the best score);\n",
        "      # if this is the case, we stop the process and save the selected network as all possible nodes.\n",
        "      # We predited that this could occur in the seizure propagation time window, e.g.\n",
        "      if possible_node_groups[:head] == possible_node_groups: \n",
        "\n",
        "        print(\"All possible networks present the best score.\")\n",
        "        selected_net = nodes\n",
        "        print(f\"\\nSelected network: {selected_net} ({len(selected_net)} nodes in total)\")\n",
        "\n",
        "        file_net = file_cm.split(\".\")[0]\n",
        "        REc(struct(test_nets=all_node_groups, nodes=selected_net)).save(path_net + f\"{file_net}.res\")\n",
        "      \n",
        "      else:\n",
        "\n",
        "        # In case there not all, but many network with the best score, the processing time could become impractical;\n",
        "        # to bypass this, we define a limit of maximally considered number of top networks as the *max_net_size* parameter.\n",
        "        # (If the selected network is much larger than the actual resection in good outcome patients, the result is useless.)\n",
        "        # Thus, among the top networks, a number equal to *max_net_size* of randomly picked networks are selected for the next iteration.\n",
        "        possible_node_groups = possible_node_groups[:head if head>0 else 1]\n",
        "        if len(possible_node_groups) >= max_net_size: \n",
        "\n",
        "          print(f\"More than {max_net_size} networks present the best score. Randomly selecting {max_net_size} networks from the pool.\")\n",
        "          possible_node_groups = rd.sample(possible_node_groups, max_net_size)\n",
        "\n",
        "        for node_group in possible_node_groups:\n",
        "            # Here, we iterate through the node groups with the highest score, as possibly there are more than one\n",
        "\n",
        "            for node in node_ids:\n",
        "              # All possible nodes are added to the group and tested\n",
        "\n",
        "              if node not in node_group[0]:\n",
        "                  # Avoiding duplicate nodes\n",
        "\n",
        "                  test_group = node_group[0] + (node,)\n",
        "\n",
        "                  # Perform the classification between baseline and WOI epochs, using the support vector machine\n",
        "                  # Compute the cross-validation scores, using the K-Fold method\n",
        "                  # Apply the evaluation function to the cross-validation scores\n",
        "                  eval = evaluate_nodes(test_group, nodes, classify_epochs(cm, test_group))\n",
        "\n",
        "                  # Store the tested node groups in test_nets list and all_node_groups dictionary, under the net_size key\n",
        "                  test_nets.append(eval)\n",
        "                  all_node_groups[net_size].append(eval)\n",
        "\n",
        "              count_node_groups += 1\n",
        "\n",
        "\n",
        "\n",
        "        print(f\"Tested {count_node_groups} node groups.\")\n",
        "\n",
        "        # Sort the latest networks by their score (indexed -1) and save the best evaluation score\n",
        "        test_nets.sort(key=lambda x:x[-1], reverse=True)\n",
        "        all_node_groups[net_size].sort(key=lambda x:x[-1], reverse=True)\n",
        "\n",
        "        evaluation_score = test_nets[0][-1]\n",
        "\n",
        "        print(f\"Best score for networks of size {net_size} =\", evaluation_score)\n",
        "        print(f\"Best network of size {net_size}: {test_nets[0][1]}\")\n",
        "\n",
        "        if evaluation_score >= best_score:\n",
        "            # If the new score is higher than the previous best score, \n",
        "            # update the best score and the possible node groups for the next iteration\n",
        "            if net_size <= max_net_size:\n",
        "\n",
        "                best_score = evaluation_score\n",
        "                print(\"\\nNew best score =\", best_score)\n",
        "\n",
        "                head_i = check_until(test_nets, fall=best_score)\n",
        "                best_net = test_nets[:head_i if head_i>0 else 1]\n",
        "                print(\"\\nNew best network =\", best_net)\n",
        "\n",
        "                possible_node_groups = best_net\n",
        "                test_nets = []\n",
        "                                \n",
        "            net_size += 1\n",
        "            \n",
        "        else: \n",
        "          print(\"A better network not found.\")\n",
        "          net_size = max_net_size\n",
        "          break\n",
        "\n",
        "      selected_net = sorted(set([t for n in best_net for t in n[1].split('<->')]))\n",
        "\n",
        "\n",
        "      print(f\"\\nSelected network: {selected_net} ({len(selected_net)} nodes in total)\")\n",
        "\n",
        "      file_net = file_cm.split(\".\")[0]\n",
        "      REc(struct(test_nets=all_node_groups, nodes=selected_net)).save(path_net + f\"{file_net}.res\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
