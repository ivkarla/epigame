{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.game import classify_epochs, evaluate_nodes, check_until\n",
    "\n",
    "from itertools import combinations\n",
    "from os import listdir, makedirs\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "import random as rd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect import isfunction, ismethod, isgeneratorfunction, isgenerator, isroutine\n",
    "from inspect import isabstract, isclass, ismodule, istraceback, isframe, iscode, isbuiltin\n",
    "from inspect import ismethoddescriptor, isdatadescriptor, isgetsetdescriptor, ismemberdescriptor\n",
    "from inspect import isawaitable, iscoroutinefunction, iscoroutine\n",
    "\n",
    "from collections.abc import Iterable as iterable\n",
    "\n",
    "from pickle import load, dump\n",
    "\n",
    "def isfx(field): return ismethod(field) or isfunction(field)\n",
    "\n",
    "class GhostSet:\n",
    "    \"\"\" enhanced interface (ghost) to retrieve class fields \"\"\"\n",
    "    def _meta(data): return {k:v for k,v in data.__dict__.items() if not isfx(v)}\n",
    "    def _at_last(_, sets): pass\n",
    "    def _set(object, **sets):\n",
    "        ''' use to fast initialize fields | needed to avoid initialization problems at copy by value '''\n",
    "        for field in sets: setattr(object, field, sets[field])\n",
    "        object._at_last(sets)\n",
    "GSet = GhostSet\n",
    "\n",
    "def meta(object):\n",
    "    ''' retrieves clonable object metadata (__dict__) as a copy '''\n",
    "    if isinstance(object, GSet): return object._meta()\n",
    "    return {}\n",
    "\n",
    "class ClonableObjectGhost:\n",
    "    \"\"\" enhanced interface (ghost) for clonable objects \"\"\"\n",
    "    def _by_val(_, depth=-1, _layer=0): pass\n",
    "GCo = ClonableObjectGhost\n",
    "\n",
    "class ClonableObject(GSet, GCo):\n",
    "    \"\"\" base clonable object \"\"\"\n",
    "    def __init__(this, **data): this._set(**data)\n",
    "    def __call__(_, **options): _._set(**options)\n",
    "    def _by_val(_, depth=-1, _layer=0):\n",
    "        copy = type(_)()\n",
    "        copy._set(**_._meta())\n",
    "        if depth<0 or depth>_layer:\n",
    "            for field in copy.__dict__:\n",
    "                if isinstance(copy.__dict__[field], ClonableObjectGhost):\n",
    "                    copy.__dict__[field] = copy.__dict__[field]._by_val(depth,_layer+1)\n",
    "        return copy\n",
    "COb = ClonableObject\n",
    "\n",
    "def copy_by_val(object, depth=-1, _layer=0):\n",
    "    if isinstance(object, GCo): return object._by_val(depth,_layer)\n",
    "    return object\n",
    "copy = by_val = vof = copy_by_val\n",
    "\n",
    "class ComparableGhost:\n",
    "    \"\"\" enhanced interface (ghost) for comparing instances \"\"\"\n",
    "    def _compare(a, b):\n",
    "        if type(a) != type(b): return False\n",
    "        if a.__dict__ == b.__dict__: return True\n",
    "        return False\n",
    "    def __eq__(a, b): return a._compare(b)\n",
    "GEq = ComparableGhost\n",
    "\n",
    "class IterableObjectGhost(GSet):\n",
    "    \"\"\" enhanced interface (ghost) for iterables: exposes __dict__,\n",
    "        therefore Iterable Objects are like lua dictionaries \"\"\"\n",
    "    def __contains__(this, key): return key in this.__dict__\n",
    "    def __iter__(this): return iter(this.__dict__)\n",
    "    def items(my): return my.__dict__.items()\n",
    "    def __getitem__(by, field): return by.__dict__[field]\n",
    "    def __setitem__(by, field, value): by.__dict__[field] = value\n",
    "    def pop(by, field): return by.__dict__.pop(field)\n",
    "GIo = IterableObjectGhost\n",
    "\n",
    "class ReprGhost:\n",
    "    \"\"\" enhanced interface (ghost) for the skeleton method _repr,\n",
    "        see implementation of Struct for a working example;\n",
    "        Record __repr__ override uses _lines_ for max lines display \"\"\"\n",
    "    _lines_ = 31\n",
    "    _chars_ = 13\n",
    "    _msgsz_ = 62\n",
    "    _ellipsis_ = ' ... '\n",
    "    def _repr(my, value):\n",
    "        _type = ''.join(''.join(str(type(value)).split('class ')).split(\"'\"))\n",
    "        _value = '{}'.format(value)\n",
    "        if len(_value)>my._chars_:\n",
    "            show = int(my._chars_/2)\n",
    "            _value = _value[:show]+my._ellipsis_+_value[-show:]\n",
    "        return '{} {}'.format(_type, _value)\n",
    "    def _resize(this, message, at=.7):\n",
    "        if len(message)>this._msgsz_:\n",
    "            start = int(at*this._msgsz_)\n",
    "            end = this._msgsz_-start\n",
    "            return message[:start]+this._ellipsis_+message[-end:]\n",
    "        return message\n",
    "GRe = ReprGhost\n",
    "\n",
    "def set_repr_to(lines): GRe._lines_ = lines\n",
    "\n",
    "class Struct(COb, GEq, GIo, GRe):\n",
    "    \"\"\" structured autoprintable object, behaves like a lua dictionary \"\"\"\n",
    "    def __repr__(_):\n",
    "        return '\\n'.join(['{}:\\t{}'.format(k, _._repr(v)) for k,v in _.items()])\n",
    "struct = Struct\n",
    "\n",
    "class RecordableGhost:\n",
    "    \"\"\" enhanced interface (ghost) for type recording,\n",
    "        see Record for a working example \"\"\"\n",
    "    @staticmethod\n",
    "    def load(filename):\n",
    "        with open(filename, 'rb') as file: return load(file)\n",
    "    def save(data, filename):\n",
    "        with open(filename, 'wb') as file: dump(data, file)\n",
    "        \n",
    "GRec = RecordableGhost\n",
    "\n",
    "class Record(GSet, GCo, GRec, GEq, GRe):\n",
    "    \"\"\" wrapper for any object or value, auto-inspects and provides load/save type structure \"\"\"\n",
    "    data = None\n",
    "    _check = dict(\n",
    "            isfunction=isfunction, ismethod=ismethod, isgeneratorfunction=isgeneratorfunction, isgenerator=isgenerator, isroutine=isroutine,\n",
    "            isabstract=isabstract, isclass=isclass, ismodule=ismodule, istraceback=istraceback, isframe=isframe, iscode=iscode, isbuiltin=isbuiltin,\n",
    "            ismethoddescriptor=ismethoddescriptor, isdatadescriptor=isdatadescriptor, isgetsetdescriptor=isgetsetdescriptor, ismemberdescriptor=ismemberdescriptor,\n",
    "            isawaitable=isawaitable, iscoroutinefunction=iscoroutinefunction, iscoroutine=iscoroutine\n",
    "                   )\n",
    "    def __init__(this, token, **meta):\n",
    "        this.data = token\n",
    "        this.__dict__.update({k:v(token) for k,v in this._check.items()})\n",
    "        super()._set(**meta)\n",
    "    @property\n",
    "    def type(_): return type(_.data)\n",
    "    def inherits(_, *types): return issubclass(_.type, types)\n",
    "    @property\n",
    "    def isbaseiterable(_): return _.inherits(tuple, list, dict, set) or _.isgenerator or _.isgeneratorfunction\n",
    "    @property\n",
    "    def isiterable(_): return isinstance(_.data, iterable) and _.type is not str\n",
    "    def _clone_iterable(_):\n",
    "        if _.inherits(dict): return _.data.copy()\n",
    "        elif _.isgenerator or _.isgeneratorfunction: return (i for i in list(_.data))\n",
    "        else: return type(_.data)(list(_.data)[:])\n",
    "    def _meta(data): return {k:v for k,v in data.__dict__.items() if k != 'data' and not isfx(v)}\n",
    "    def _by_val(_, depth=-1, layer=0):\n",
    "        data = _.data\n",
    "        if _.isiterable: data = _._clone_iterable()\n",
    "        elif _.inherits(ClonableObjectGhost): data = by_val(data, depth, layer)\n",
    "        return type(_)(data, **meta(_))\n",
    "    def __enter__(self): self._instance = self; return self\n",
    "    def __exit__(self, type, value, traceback): self._instance = None\n",
    "    def __repr__(self):\n",
    "        if not hasattr(self, '_preprint'): return Record(self.data, _preprint='', _lines=Record(Record._lines_)).__repr__()\n",
    "        if self.isbaseiterable:\n",
    "            pre, repr = self._preprint, ''\n",
    "            for n,i in enumerate(self.data):\n",
    "                if self._lines.data == 0: break\n",
    "                else: self._lines.data -= 1\n",
    "                index, item = str(n), i\n",
    "                if self.inherits(dict): index += ' ({})'.format(str(i)); item = self.data[i]\n",
    "                repr += pre+'{}: '.format(index)\n",
    "                next = Record(item, _preprint=pre+'\\t', _lines=self._lines)\n",
    "                if next.isiterable: repr += '\\n'\n",
    "                repr += next.__repr__()\n",
    "                repr += '\\n'\n",
    "            return repr\n",
    "        elif self.inherits(GCo): return Record(self.data._meta(), _preprint=self._preprint, _lines=self._lines).__repr__()\n",
    "        else: return self._repr(self.data)\n",
    "REc = Record\n",
    "\n",
    "class Bisect(list, COb):\n",
    "    \"\"\" bisect implementation using clonable objects \"\"\"\n",
    "    def __init__(set, *items, key=None, reverse=False):\n",
    "        if not key: key = lambda  x:x\n",
    "        super().__init__(sorted(items, reverse=reverse, key=key))\n",
    "    def _bisect(set, item, key, reverse, bottom, top):\n",
    "        def _(check):\n",
    "            if key: return key(check)\n",
    "            return check\n",
    "        at = int((top-bottom)/2)+bottom\n",
    "        if len(set)==0: return (0,-1)\n",
    "        if item==_(set[at]): return (at,0)\n",
    "        bigger = item<_(set[at])\n",
    "        if bigger != reverse:\n",
    "            if at-bottom>0: return set._bisect(item, key, reverse, bottom, at)\n",
    "            return (at,-1)\n",
    "        elif top-at>1: return set._bisect(item, key, reverse, at, top)\n",
    "        return (at,1)\n",
    "    def search(_, item, key=None, reverse=False):\n",
    "        if not key: key = lambda x:x\n",
    "        return _._bisect(item, key, reverse, 0, len(_))\n",
    "    def _by_val(_, depth=-1, _layer=0):\n",
    "        copy = super()._by_val(depth, _layer)\n",
    "        copy += _[:]\n",
    "        return copy\n",
    "BSx = Bisect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "woi = input(\"Time window:\\n 1. Non-seizure (baseline)\\n 2. Pre-seizure (5 min prior to seizure)\\n 3. Pre-seizure (4 min prior to seizure)\\n 4. Pre-seizure (3 min prior to seizure)\\n 5. Pre-seizure (2 min prior to seizure)\\n 6. Pre-seizure (1 min prior to seizure)\\n 7. Transition to seizure (1 min interval)\\n 8. Transition to seizure (2 min interval)\\n 9. Transition to seizure (60% seizure length interval)\\n 10. Seizure\\n Indicate a number: \")\n",
    "\n",
    "woi_code = {'1':\"baseline\", '2':\"preseizure5\", '3':\"preseizure4\", '4':\"preseizure3\", '5':\"preseizure2\", '6':\"preseizure1\", '7':\"transition1\", '8':\"transition2\", '9':\"transition60\", '10':\"seizure\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_folder = \"/home/kivi/gdrive/epigame-folder/\"\n",
    "\n",
    "path_cm = main_folder + \"connectivity_matrices/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_net_size = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_net = main_folder + \"selected_network/\"\n",
    "makedirs(path_net, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------\n",
      "\n",
      "Processing...\n",
      "Connectivity matrices of ASJ-preseizure2-CC-(70,150).prep\n",
      "\n",
      "--------------------------------------------------------------\n",
      "\n",
      "Processing...\n",
      "Connectivity matrices of ASJ-preseizure2-CC-(30,70).prep\n",
      "\n",
      "--------------------------------------------------------------\n",
      "\n",
      "Processing...\n",
      "Connectivity matrices of ASJ-preseizure2-CC-(12,30).prep\n",
      "\n",
      "--------------------------------------------------------------\n",
      "\n",
      "Processing...\n",
      "Connectivity matrices of ASJ-preseizure2-CC-(4,8).prep\n",
      "\n",
      "--------------------------------------------------------------\n",
      "\n",
      "Processing...\n",
      "Connectivity matrices of ASJ-preseizure2-CC-(0,4).prep\n",
      "\n",
      "--------------------------------------------------------------\n",
      "\n",
      "Processing...\n",
      "Connectivity matrices of ASJ-preseizure2-CC-(8,12).prep\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/kivi/Documents/GitHub/epigame/notebooks/find_en_combs.ipynb Cell 7\u001b[0m in \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kivi/Documents/GitHub/epigame/notebooks/find_en_combs.ipynb#W5sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m         \u001b[39melse\u001b[39;00m: X \u001b[39m=\u001b[39m e_base[j]\u001b[39m.\u001b[39mreshape((e_base[j]\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], e_base[j]\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], \u001b[39m1\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kivi/Documents/GitHub/epigame/notebooks/find_en_combs.ipynb#W5sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m         epoch_data_base[eid] \u001b[39m=\u001b[39m X\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/kivi/Documents/GitHub/epigame/notebooks/find_en_combs.ipynb#W5sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m i \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(eid_woi\u001b[39m.\u001b[39mkeys()) \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(eid_base\u001b[39m.\u001b[39mkeys())\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kivi/Documents/GitHub/epigame/notebooks/find_en_combs.ipynb#W5sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m y \u001b[39m=\u001b[39m [\u001b[39m1\u001b[39m]\u001b[39m*\u001b[39m\u001b[39mint\u001b[39m(\u001b[39mlen\u001b[39m(i)\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m) \u001b[39m+\u001b[39m [\u001b[39m0\u001b[39m]\u001b[39m*\u001b[39m\u001b[39mint\u001b[39m(\u001b[39mlen\u001b[39m(i)\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kivi/Documents/GitHub/epigame/notebooks/find_en_combs.ipynb#W5sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m X \u001b[39m=\u001b[39m [epoch_data_woi[e] \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m i]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "cm_combs = []\n",
    "epoch_data_woi, epoch_data_base, X = {},{},None\n",
    "for file_cm in listdir(path_cm):\n",
    "  \n",
    "  if file_cm.split(\"-\")[1]==woi_code[woi] and \"CC\" in file_cm:\n",
    "\n",
    "    print(\"\\n--------------------------------------------------------------\")\n",
    "    print(\"\\nProcessing...\")\n",
    "\n",
    "    subject_id = file_cm.split(\"/\")[-1][0:3]\n",
    "    print(\"Connectivity matrices of\", file_cm)\n",
    "\n",
    "    cm = REc.load(path_cm + file_cm).data\n",
    "\n",
    "    nodes = cm.nodes\n",
    "    node_ids = list(range(len(nodes))) \n",
    "\n",
    "    e_woi = cm.X[0:int(len(cm.X)/2)]\n",
    "    e_base = cm.X[int(len(cm.X)/2)::]\n",
    "\n",
    "    eid_woi = cm.i[0:int(len(cm.i)/2)]\n",
    "    eid_base = cm.i[int(len(cm.i)/2)::]\n",
    "\n",
    "    X =None\n",
    "    for j, eid in enumerate(eid_woi):\n",
    "        if eid in epoch_data_woi: X = np.concatenate((epoch_data_woi[eid], e_woi[j].reshape((e_woi[j].shape[0], e_woi[j].shape[1], 1))), axis=2)\n",
    "        else: X = e_woi[j].reshape((e_woi[j].shape[0], e_woi[j].shape[1], 1))\n",
    "        epoch_data_woi[eid] = X\n",
    "\n",
    "    for j, eid in enumerate(eid_base):\n",
    "        if eid in epoch_data_base: X = np.concatenate((epoch_data_base[eid], e_base[j].reshape((e_base[j].shape[0], e_base[j].shape[1], 1))), axis=2)\n",
    "        else: X = e_base[j].reshape((e_base[j].shape[0], e_base[j].shape[1], 1))\n",
    "        epoch_data_base[eid] = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6), (98, 98, 6)]\n"
     ]
    }
   ],
   "source": [
    "print(list(epoch_data_woi[i].shape for i in epoch_data_woi.keys()))\n",
    "\n",
    "i = list(epoch_data_woi.keys()) + list(epoch_data_base.keys())\n",
    "y = [1]*int(len(i)/2) + [0]*int(len(i)/2)\n",
    "\n",
    "epoch_data_woi.update(epoch_data_base)\n",
    "X = [epoch_data_woi[e] for e in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = struct(y=np.array(y), i=np.array(i))\n",
    "cm._set(X=X)\n",
    "cm._set(nodes=nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing node combinations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kivi/Documents/GitHub/epigame/src/data_legacy.py:27: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  def is_scalar(this): return this.shape is ()\n",
      "/home/kivi/Documents/GitHub/epigame/src/data_legacy.py:27: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  def is_scalar(this): return this.shape is ()\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 6 is out of bounds for axis 0 with size 6",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:862\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 862\u001b[0m     tasks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ready_batches\u001b[39m.\u001b[39;49mget(block\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    863\u001b[0m \u001b[39mexcept\u001b[39;00m queue\u001b[39m.\u001b[39mEmpty:\n\u001b[1;32m    864\u001b[0m     \u001b[39m# slice the iterator n_jobs * batchsize items at a time. If the\u001b[39;00m\n\u001b[1;32m    865\u001b[0m     \u001b[39m# slice returns less than that, then the current batchsize puts\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    868\u001b[0m     \u001b[39m# accordingly to distribute evenly the last items between all\u001b[39;00m\n\u001b[1;32m    869\u001b[0m     \u001b[39m# workers.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/queue.py:168\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_qsize():\n\u001b[0;32m--> 168\u001b[0m         \u001b[39mraise\u001b[39;00m Empty\n\u001b[1;32m    169\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mEmpty\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/kivi/Documents/GitHub/epigame/notebooks/find_en_combs.ipynb Cell 10\u001b[0m in \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kivi/Documents/GitHub/epigame/notebooks/find_en_combs.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m node_pairs \u001b[39m=\u001b[39m combinations(node_ids, \u001b[39m2\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kivi/Documents/GitHub/epigame/notebooks/find_en_combs.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mProcessing node combinations...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/kivi/Documents/GitHub/epigame/notebooks/find_en_combs.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m parallelize \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)(delayed(evaluate_nodes)(pair, nodes, classify_epochs(cm, pair)) \u001b[39mfor\u001b[39;00m pair \u001b[39min\u001b[39;00m node_pairs)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kivi/Documents/GitHub/epigame/notebooks/find_en_combs.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m base \u001b[39m=\u001b[39m [p \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m parallelize]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kivi/Documents/GitHub/epigame/notebooks/find_en_combs.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(base)\u001b[39m}\u001b[39;00m\u001b[39m finished\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1076\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1077\u001b[0m     \u001b[39m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[1;32m   1078\u001b[0m     \u001b[39m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[39m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[1;32m   1083\u001b[0m     \u001b[39m# remaining jobs.\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m-> 1085\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1088\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:873\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    870\u001b[0m n_jobs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached_effective_n_jobs\n\u001b[1;32m    871\u001b[0m big_batch_size \u001b[39m=\u001b[39m batch_size \u001b[39m*\u001b[39m n_jobs\n\u001b[0;32m--> 873\u001b[0m islice \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(itertools\u001b[39m.\u001b[39;49mislice(iterator, big_batch_size))\n\u001b[1;32m    874\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(islice) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    875\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;32m/home/kivi/Documents/GitHub/epigame/notebooks/find_en_combs.ipynb Cell 10\u001b[0m in \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kivi/Documents/GitHub/epigame/notebooks/find_en_combs.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m node_pairs \u001b[39m=\u001b[39m combinations(node_ids, \u001b[39m2\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kivi/Documents/GitHub/epigame/notebooks/find_en_combs.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mProcessing node combinations...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/kivi/Documents/GitHub/epigame/notebooks/find_en_combs.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m parallelize \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)(delayed(evaluate_nodes)(pair, nodes, classify_epochs(cm, pair)) \u001b[39mfor\u001b[39;00m pair \u001b[39min\u001b[39;00m node_pairs)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kivi/Documents/GitHub/epigame/notebooks/find_en_combs.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m base \u001b[39m=\u001b[39m [p \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m parallelize]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kivi/Documents/GitHub/epigame/notebooks/find_en_combs.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(base)\u001b[39m}\u001b[39;00m\u001b[39m finished\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/GitHub/epigame/src/game.py:26\u001b[0m, in \u001b[0;36mclassify_epochs\u001b[0;34m(set, node_group, kratio, random_state, **mopts)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclassify_epochs\u001b[39m(\u001b[39mset\u001b[39m, node_group, kratio\u001b[39m=\u001b[39m\u001b[39m.1\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m31\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmopts):\n\u001b[1;32m     13\u001b[0m     \u001b[39m\"\"\"Quantifies connectivity change of specified node group.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39m    Classifies time frame epochs using the connectivity measures as features. \u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m    K-fold cross validation scores measure the connectivity change.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m        ndarray of float: Array of scores for each fold.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m     X, Y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([np\u001b[39m.\u001b[39marray((record(x)\u001b[39m.\u001b[39minclude(\u001b[39m*\u001b[39mnode_group)\u001b[39m.\u001b[39mT)\u001b[39m.\u001b[39minclude(\u001b[39m*\u001b[39mnode_group))\u001b[39m.\u001b[39mflatten() \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mset\u001b[39m\u001b[39m.\u001b[39mX]), \u001b[39mset\u001b[39m\u001b[39m.\u001b[39my\n\u001b[1;32m     27\u001b[0m     model, scaler, k \u001b[39m=\u001b[39m SVC, StandardScaler, \u001b[39mint\u001b[39m(\u001b[39mround\u001b[39m(\u001b[39mlen\u001b[39m(Y)\u001b[39m*\u001b[39mkratio))\n\u001b[1;32m     28\u001b[0m     \u001b[39mif\u001b[39;00m random_state \u001b[39m==\u001b[39m \u001b[39mNone\u001b[39;00m: random_state \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39m0xFFFFFFFF\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/GitHub/epigame/src/game.py:26\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclassify_epochs\u001b[39m(\u001b[39mset\u001b[39m, node_group, kratio\u001b[39m=\u001b[39m\u001b[39m.1\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m31\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmopts):\n\u001b[1;32m     13\u001b[0m     \u001b[39m\"\"\"Quantifies connectivity change of specified node group.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39m    Classifies time frame epochs using the connectivity measures as features. \u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m    K-fold cross validation scores measure the connectivity change.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m        ndarray of float: Array of scores for each fold.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m     X, Y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([np\u001b[39m.\u001b[39marray((record(x)\u001b[39m.\u001b[39;49minclude(\u001b[39m*\u001b[39;49mnode_group)\u001b[39m.\u001b[39;49mT)\u001b[39m.\u001b[39;49minclude(\u001b[39m*\u001b[39;49mnode_group))\u001b[39m.\u001b[39mflatten() \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mset\u001b[39m\u001b[39m.\u001b[39mX]), \u001b[39mset\u001b[39m\u001b[39m.\u001b[39my\n\u001b[1;32m     27\u001b[0m     model, scaler, k \u001b[39m=\u001b[39m SVC, StandardScaler, \u001b[39mint\u001b[39m(\u001b[39mround\u001b[39m(\u001b[39mlen\u001b[39m(Y)\u001b[39m*\u001b[39mkratio))\n\u001b[1;32m     28\u001b[0m     \u001b[39mif\u001b[39;00m random_state \u001b[39m==\u001b[39m \u001b[39mNone\u001b[39;00m: random_state \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39m0xFFFFFFFF\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/GitHub/epigame/src/data_legacy.py:97\u001b[0m, in \u001b[0;36mrec.include\u001b[0;34m(data, *items, **sets)\u001b[0m\n\u001b[1;32m     95\u001b[0m included \u001b[39m=\u001b[39m []\n\u001b[1;32m     96\u001b[0m \u001b[39mif\u001b[39;00m data\u001b[39m.\u001b[39mis_vector: included \u001b[39m=\u001b[39m rec\u001b[39m.\u001b[39mread([data])[:,items][\u001b[39m0\u001b[39m]\n\u001b[0;32m---> 97\u001b[0m \u001b[39melse\u001b[39;00m: included \u001b[39m=\u001b[39m data[items,:]\n\u001b[1;32m     98\u001b[0m \u001b[39mreturn\u001b[39;00m rec\u001b[39m.\u001b[39mread(included, to\u001b[39m=\u001b[39m\u001b[39mtype\u001b[39m(data), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmeta(data), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msets)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 6 is out of bounds for axis 0 with size 6"
     ]
    }
   ],
   "source": [
    "node_pairs = combinations(node_ids, 2)\n",
    "\n",
    "print(\"\\nProcessing node combinations...\")\n",
    "\n",
    "parallelize = Parallel(n_jobs=-1)(delayed(evaluate_nodes)(pair, nodes, classify_epochs(cm, pair)) for pair in node_pairs)\n",
    "base = [p for p in parallelize]\n",
    "\n",
    "print(f\"{len(base)} finished\")\n",
    "\n",
    "base.sort(key=lambda x:x[-1], reverse=True)\n",
    "best_pair = base[0]\n",
    "best_net = [best_pair]\n",
    "print(f\"Best node pair: {best_net}\")\n",
    "\n",
    "best_score, net_size, possible_node_groups, test_nets = base[0][-1], 3, base[:], []\n",
    "print(\"Best score =\", best_score)\n",
    "\n",
    "all_node_groups = {} # This dictionary saves all tested node groups, under a key indicating net_size (number of grouped nodes) \n",
    "all_node_groups[2] = base\n",
    "\n",
    "while net_size <= max_net_size:\n",
    "\n",
    "  all_node_groups[net_size] = []\n",
    "\n",
    "  print(f\"\\nChecking networks with {net_size} nodes...\")\n",
    "\n",
    "  head = check_until(possible_node_groups, fall=best_score)\n",
    "  \n",
    "  count_node_groups = 0\n",
    "\n",
    "  # The condition below checks if all tested node groups have the same score (the best score);\n",
    "  # if this is the case, we stop the process and save the selected network as all possible nodes.\n",
    "  # We predited that this could occur in the seizure propagation time window, e.g.\n",
    "  if possible_node_groups[:head] == possible_node_groups: \n",
    "\n",
    "    print(\"All possible networks present the best score.\")\n",
    "    selected_net = nodes\n",
    "    print(f\"\\nSelected network: {selected_net} ({len(selected_net)} nodes in total)\")\n",
    "\n",
    "    file_net = file_cm.split(\".\")[0]\n",
    "    REc(struct(test_nets=all_node_groups, nodes=selected_net)).save(path_net + f\"{file_net}.res\")\n",
    "    break\n",
    "\n",
    "  else:\n",
    "\n",
    "    # In case there not all, but many network with the best score, the processing time could become impractical;\n",
    "    # to bypass this, we define a limit of maximally considered number of top networks as the *max_net_size* parameter.\n",
    "    # (If the selected network is much larger than the actual resection in good outcome patients, the result is useless.)\n",
    "    # Thus, among the top networks, a number equal to *max_net_size* of randomly picked networks are selected for the next iteration.\n",
    "    possible_node_groups = possible_node_groups[:head if head>0 else 1]\n",
    "    if len(possible_node_groups) >= max_net_size: \n",
    "\n",
    "      print(f\"More than {max_net_size} networks present the best score. Randomly selecting {max_net_size} networks from the pool.\")\n",
    "      possible_node_groups = rd.sample(possible_node_groups, max_net_size)\n",
    "\n",
    "    for node_group in possible_node_groups:\n",
    "        # Here, we iterate through the node groups with the highest score, as possibly there are more than one\n",
    "\n",
    "        for node in node_ids:\n",
    "          # All possible nodes are added to the group and tested\n",
    "\n",
    "          if node not in node_group[0]:\n",
    "              # Avoiding duplicate nodes\n",
    "\n",
    "              test_group = node_group[0] + (node,)\n",
    "\n",
    "              # Perform the classification between baseline and WOI epochs, using the support vector machine\n",
    "              # Compute the cross-validation scores, using the K-Fold method\n",
    "              # Apply the evaluation function to the cross-validation scores\n",
    "              eval = evaluate_nodes(test_group, nodes, classify_epochs(cm, test_group))\n",
    "\n",
    "              # Store the tested node groups in test_nets list and all_node_groups dictionary, under the net_size key\n",
    "              test_nets.append(eval)\n",
    "              all_node_groups[net_size].append(eval)\n",
    "\n",
    "          count_node_groups += 1\n",
    "\n",
    "    print(f\"Tested {count_node_groups} node groups.\")\n",
    "\n",
    "    # Sort the latest networks by their score (indexed -1) and save the best evaluation score\n",
    "    test_nets.sort(key=lambda x:x[-1], reverse=True)\n",
    "    all_node_groups[net_size].sort(key=lambda x:x[-1], reverse=True)\n",
    "\n",
    "    evaluation_score = test_nets[0][-1]\n",
    "\n",
    "    print(f\"Best score for networks of size {net_size} =\", evaluation_score)\n",
    "    print(f\"Best network of size {net_size}: {test_nets[0][1]}\")\n",
    "\n",
    "    if evaluation_score >= best_score:\n",
    "        # If the new score is higher than the previous best score, \n",
    "        # update the best score and the possible node groups for the next iteration\n",
    "        if net_size <= max_net_size:\n",
    "\n",
    "            best_score = evaluation_score\n",
    "            print(\"\\nNew best score =\", best_score)\n",
    "\n",
    "            head_i = check_until(test_nets, fall=best_score)\n",
    "            best_net = test_nets[:head_i if head_i>0 else 1]\n",
    "            print(\"\\nNew best network =\", best_net)\n",
    "\n",
    "            possible_node_groups = best_net\n",
    "            test_nets = []\n",
    "                            \n",
    "        net_size += 1\n",
    "        \n",
    "    else: \n",
    "      print(\"A better network not found.\")\n",
    "\n",
    "      selected_net = sorted(set([t for n in best_net for t in n[1].split('<->')]))\n",
    "\n",
    "      print(f\"\\nSelected network: {selected_net} ({len(selected_net)} nodes in total)\")\n",
    "\n",
    "      file_net = file_cm.split(\".\")[0]\n",
    "      REc(struct(test_nets=all_node_groups, nodes=selected_net)).save(path_net + f\"{file_net}.res\")\n",
    "      break\n",
    "\n",
    "  if net_size==max_net_size+1:\n",
    "    print(\"Reached the maximum network size.\")\n",
    "\n",
    "    selected_net = sorted(set([t for n in best_net for t in n[1].split('<->')]))\n",
    "\n",
    "    print(f\"\\nSelected network: {selected_net} ({len(selected_net)} nodes in total)\")\n",
    "\n",
    "    file_net = file_cm.split(\".\")[0]\n",
    "    REc(struct(test_nets=all_node_groups, nodes=selected_net)).save(path_net + f\"{file_net}.res\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
