{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### __Guidelines for using this notebook__\n",
        "##### The notebook generates connectivity matrices used as input for the analytical framework (*gcolab_find_en.ipynb*).\n",
        "###### Input for this notebook are two separate EDF files:\n",
        "###### 1) Seizure file - a recording of at least 5 min of pre-seizure time and the whole seizure\n",
        "###### 2) Baseline file - non-seizure recording of 10 min.\n",
        "###### The seizure file should include clinical annotations of seizure start and end. The baseline file should include the annotation at the middle of the recording (at 5 min) and at the end of the recording. These can be customized and changed defined in the variables: *sz_start_note*, *sz_end_note*, *base_center_note*, *base_end_note*.\n",
        "###### The EDF files should be placed in the main folder, in a subfolder called '*data*'.\n",
        "###### EDF filenames should be formatted as: *patientacronym\\*-seizure.EDF* or *patientacronym\\*-baseline.EDF* (asterix signifies a regular expression for any string of choice).\n",
        "###### Once the notebook is run, the user will be asked to choose from a number of parameters:\n",
        "###### 1) Time interval of interest\n",
        "###### 2) Connectivity measure\n",
        "###### 3) If the signal should be filtered in a specific frequency band\n",
        "###### 4) If yes, define the minimum frequency limit\n",
        "###### 5) and the maximum frequency limit.\n",
        "###### The results will be saved as a rec object (defined in data_legacy module), containing the connectivity matrices as '*X*', epoch labels (used as ground truth by the classifier) as '*y*' and node labels as '*nodes*', in a subfolder called '*connectivity_matrices*'."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### The following five cells define objects for loading and handling the EDF data.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5oyIq3JJO0Z"
      },
      "outputs": [],
      "source": [
        "from inspect import ismethod\n",
        "from datetime import timedelta as _time\n",
        "from datetime import datetime\n",
        "from collections.abc import Iterable as iterable\n",
        "\n",
        "def some(field): return (field is not None and field != [] and field != {} and field != ()) or field == True\n",
        "def no(field): return not some(field) or field==False or field==''\n",
        "\n",
        "class class_of:\n",
        "    _instance = None\n",
        "    def __init__(_, object):\n",
        "        _._is = type(object)\n",
        "    def inherits(_, *types):\n",
        "        return issubclass(_._is, types)\n",
        "    def has(_, *types): return _.inherits(*types)\n",
        "    def __enter__(self):\n",
        "        self._instance = self\n",
        "        return self\n",
        "    def __exit__(self, type, value, traceback): self._instance = None\n",
        "    @staticmethod\n",
        "    def each_in(list):\n",
        "        if isiterable(list):\n",
        "            return [type(item) for item in list]\n",
        "\n",
        "class struct:\n",
        "    def __init__(table, **sets): table.__dict__.update(sets)\n",
        "    @property\n",
        "    def sets(this): return set(dir(this)) - set(dir(type(this)))\n",
        "    def set(object, **fields):\n",
        "        for field in fields: setattr(object, field, fields[field])\n",
        "    def get(object, *fields): return [getattr(object, field) for field in fields if field in object.__dict__]\n",
        "    def _clonable(set, mask=None):\n",
        "        check = set.__dict__.copy()\n",
        "        clonable = check.copy()\n",
        "        if some(mask): pass\n",
        "#            for field in check:\n",
        "#                if sum([int(_(check[field])) for _ in mask])+sum([int(_(field)) for _ in mask]): clonable.pop(field)\n",
        "        return clonable\n",
        "    @staticmethod\n",
        "    def _from(type):\n",
        "        if hasattr(type, '__dict__'): return struct(**type.__dict__.copy())\n",
        "        return struct()\n",
        "\n",
        "def meta(data, *mask): return struct._from(data)._clonable(mask)\n",
        "def get(data, *fields):\n",
        "    if not issubclass(type(data), dict): data=struct._from(data)._clonable()\n",
        "    return struct(**data).get(*fields)\n",
        "\n",
        "class table(struct):\n",
        "    def _default(field, name, value):\n",
        "        try: return getattr(field, name)\n",
        "        except: setattr(field, name, value)\n",
        "        return value\n",
        "    def clear(this, *fields):\n",
        "        sets = this.sets\n",
        "        if not fields: fields = sets\n",
        "        if fields:\n",
        "            set = [field for field in fields if hasattr(this,field) and not ismethod(getattr(this, field))]\n",
        "            for field in set: delattr(this, field)\n",
        "    def has(this, *fields):\n",
        "        return all([hasattr(this, field) for field in fields])\n",
        "    def has_not(this, *fields): return not this.has(*fields)\n",
        "    def check(this, **KV):\n",
        "        try: check = [KV[key]==this.__dict__[key] for key in KV]\n",
        "        except: return False\n",
        "        return all(check)\n",
        "    def find(this, _type):\n",
        "        return [value for value in this.sets if class_of(get(this,value)[0]).inherits(_type)]\n",
        "    def clone(this): \n",
        "        clone = type(this)()\n",
        "        sets = this._clonable()\n",
        "        clone.set(**sets)\n",
        "        return clone\n",
        "\n",
        "def isiterable(this): return isinstance(this, iterable) and type(this) is not str\n",
        "def default(field, name, value): return table(**field)._default(name, value)\n",
        "\n",
        "def ni(list):\n",
        "    if isiterable(list):\n",
        "        for n,i in enumerate(list): yield n,i\n",
        "    else:\n",
        "        for n,i in enumerate(list.__dict__.keys()): yield n,i\n",
        "\n",
        "class at(table):\n",
        "    DAY, HOUR, MIN = 86400, 3600, 60\n",
        "    def __init__(_, dtime=None, **sets):\n",
        "        _.set(**sets)\n",
        "        if some(dtime) and issubclass(type(dtime), _time): _._time = dtime\n",
        "        else:\n",
        "            d,h,m,s,ms = _._default('d',0), _._default('h',0), _._default('m',0), _._default('s',0), _._default('ms',0)\n",
        "            if not any([d,h,m,s,ms]): now=datetime.now(); _._time = now-datetime(now.year, now.month, now.day)\n",
        "            else: _._time = _time(days=d, hours=h, minutes=m, seconds=s, milliseconds=ms)\n",
        "        _.clear('d','h','m','s','ms')\n",
        "    def __sub__(_, dtime):\n",
        "        of=type(dtime); sets=_._clonable()\n",
        "        if issubclass(of, _time): return at(_._time-dtime, **sets)\n",
        "        elif issubclass(of, at): sets.update(dtime._clonable()); return at(_._time-dtime._time, **sets)\n",
        "    def __add__(_, dtime):\n",
        "        of=type(dtime); sets=_._clonable()\n",
        "        if issubclass(of, _time): return at(_._time+dtime, **sets)\n",
        "        elif issubclass(of, at): sets.update(dtime._clonable()); return at(_._time+dtime._time, **sets)\n",
        "    def __str__(_): return str(_._time)\n",
        "    @property\n",
        "    def seconds(_): return _._time.seconds\n",
        "    @property\n",
        "    def S(_): return _.seconds\n",
        "    @property\n",
        "    def minutes(_): return _._time.seconds/60\n",
        "    @property\n",
        "    def M(_): return _.minutes\n",
        "    @property\n",
        "    def hours(_): return _.minutes/60\n",
        "    @property\n",
        "    def H(_): return _.hours\n",
        "    @property\n",
        "    def days(_): return _._time.days\n",
        "    @property\n",
        "    def D(_): return _.days\n",
        "    @staticmethod\n",
        "    def zero(): return at(_time())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ix5yCn87JLRI"
      },
      "outputs": [],
      "source": [
        "from inspect import isfunction, ismethod, isgeneratorfunction, isgenerator, isroutine\n",
        "from inspect import isabstract, isclass, ismodule, istraceback, isframe, iscode, isbuiltin\n",
        "from inspect import ismethoddescriptor, isdatadescriptor, isgetsetdescriptor, ismemberdescriptor\n",
        "from inspect import isawaitable, iscoroutinefunction, iscoroutine\n",
        "\n",
        "from collections.abc import Iterable as iterable\n",
        "\n",
        "import pickle\n",
        "\n",
        "def isfx(field): return ismethod(field) or isfunction(field)\n",
        "\n",
        "class GhostSet:\n",
        "    \"\"\" enhanced interface (ghost) to retrieve class fields \"\"\"\n",
        "    def _meta(data): return {k:v for k,v in data.__dict__.items() if not isfx(v)}\n",
        "    def _at_last(_, sets): pass\n",
        "    def _set(object, **sets):\n",
        "        ''' use to fast initialize fields | needed to avoid initialization problems at copy by value '''\n",
        "        for field in sets: setattr(object, field, sets[field])\n",
        "        object._at_last(sets)\n",
        "GSet = GhostSet\n",
        "\n",
        "def meta(object):\n",
        "    ''' retrieves clonable object metadata (__dict__) as a copy '''\n",
        "    if isinstance(object, GSet): return object._meta()\n",
        "    return {}\n",
        "\n",
        "class ClonableObjectGhost:\n",
        "    \"\"\" enhanced interface (ghost) for clonable objects \"\"\"\n",
        "    def _by_val(_, depth=-1, _layer=0): pass\n",
        "GCo = ClonableObjectGhost\n",
        "\n",
        "class ClonableObject(GSet, GCo):\n",
        "    \"\"\" base clonable object \"\"\"\n",
        "    def __init__(this, **data): this._set(**data)\n",
        "    def __call__(_, **options): _._set(**options)\n",
        "    def _by_val(_, depth=-1, _layer=0):\n",
        "        copy = type(_)()\n",
        "        copy._set(**_._meta())\n",
        "        if depth<0 or depth>_layer:\n",
        "            for field in copy.__dict__:\n",
        "                if isinstance(copy.__dict__[field], ClonableObjectGhost):\n",
        "                    copy.__dict__[field] = copy.__dict__[field]._by_val(depth,_layer+1)\n",
        "        return copy\n",
        "COb = ClonableObject\n",
        "\n",
        "def copy_by_val(object, depth=-1, _layer=0):\n",
        "    if isinstance(object, GCo): return object._by_val(depth,_layer)\n",
        "    return object\n",
        "copy = by_val = vof = copy_by_val\n",
        "\n",
        "class ComparableGhost:\n",
        "    \"\"\" enhanced interface (ghost) for comparing instances \"\"\"\n",
        "    def _compare(a, b):\n",
        "        if type(a) != type(b): return False\n",
        "        if a.__dict__ == b.__dict__: return True\n",
        "        return False\n",
        "    def __eq__(a, b): return a._compare(b)\n",
        "GEq = ComparableGhost\n",
        "\n",
        "class IterableObjectGhost(GSet):\n",
        "    \"\"\" enhanced interface (ghost) for iterables: exposes __dict__,\n",
        "        therefore Iterable Objects are like lua dictionaries \"\"\"\n",
        "    def __contains__(this, key): return key in this.__dict__\n",
        "    def __iter__(this): return iter(this.__dict__)\n",
        "    def items(my): return my.__dict__.items()\n",
        "    def __getitem__(by, field): return by.__dict__[field]\n",
        "    def __setitem__(by, field, value): by.__dict__[field] = value\n",
        "    def pop(by, field): return by.__dict__.pop(field)\n",
        "GIo = IterableObjectGhost\n",
        "\n",
        "class ReprGhost:\n",
        "    \"\"\" enhanced interface (ghost) for the skeleton method _repr,\n",
        "        see implementation of Struct for a working example;\n",
        "        Record __repr__ override uses _lines_ for max lines display \"\"\"\n",
        "    _lines_ = 31\n",
        "    _chars_ = 13\n",
        "    _msgsz_ = 62\n",
        "    _ellipsis_ = ' ... '\n",
        "    def _repr(my, value):\n",
        "        _type = ''.join(''.join(str(type(value)).split('class ')).split(\"'\"))\n",
        "        _value = '{}'.format(value)\n",
        "        if len(_value)>my._chars_:\n",
        "            show = int(my._chars_/2)\n",
        "            _value = _value[:show]+my._ellipsis_+_value[-show:]\n",
        "        return '{} {}'.format(_type, _value)\n",
        "    def _resize(this, message, at=.7):\n",
        "        if len(message)>this._msgsz_:\n",
        "            start = int(at*this._msgsz_)\n",
        "            end = this._msgsz_-start\n",
        "            return message[:start]+this._ellipsis_+message[-end:]\n",
        "        return message\n",
        "GRe = ReprGhost\n",
        "\n",
        "def set_repr_to(lines): GRe._lines_ = lines\n",
        "\n",
        "class Struct(COb, GEq, GIo, GRe):\n",
        "    \"\"\" structured autoprintable object, behaves like a lua dictionary \"\"\"\n",
        "    def __repr__(_):\n",
        "        return '\\n'.join(['{}:\\t{}'.format(k, _._repr(v)) for k,v in _.items()])\n",
        "struct = Struct\n",
        "\n",
        "class RecordableGhost:\n",
        "    \"\"\" enhanced interface (ghost) for type recording,\n",
        "        see Record for a working example \"\"\"\n",
        "    @staticmethod\n",
        "    def load(filename):\n",
        "        with open(filename, 'rb') as file: return pickle.load(file)\n",
        "    def save(data, filename):\n",
        "        with open(filename, 'wb') as file: pickle.dump(data, file)\n",
        "        \n",
        "GRec = RecordableGhost\n",
        "\n",
        "class Record(GSet, GCo, GRec, GEq, GRe):\n",
        "    \"\"\" wrapper for any object or value, auto-inspects and provides load/save type structure \"\"\"\n",
        "    data = None\n",
        "    _check = dict(\n",
        "            isfunction=isfunction, ismethod=ismethod, isgeneratorfunction=isgeneratorfunction, isgenerator=isgenerator, isroutine=isroutine,\n",
        "            isabstract=isabstract, isclass=isclass, ismodule=ismodule, istraceback=istraceback, isframe=isframe, iscode=iscode, isbuiltin=isbuiltin,\n",
        "            ismethoddescriptor=ismethoddescriptor, isdatadescriptor=isdatadescriptor, isgetsetdescriptor=isgetsetdescriptor, ismemberdescriptor=ismemberdescriptor,\n",
        "            isawaitable=isawaitable, iscoroutinefunction=iscoroutinefunction, iscoroutine=iscoroutine\n",
        "                   )\n",
        "    def __init__(this, token, **meta):\n",
        "        this.data = token\n",
        "        this.__dict__.update({k:v(token) for k,v in this._check.items()})\n",
        "        super()._set(**meta)\n",
        "    @property\n",
        "    def type(_): return type(_.data)\n",
        "    def inherits(_, *types): return issubclass(_.type, types)\n",
        "    @property\n",
        "    def isbaseiterable(_): return _.inherits(tuple, list, dict, set) or _.isgenerator or _.isgeneratorfunction\n",
        "    @property\n",
        "    def isiterable(_): return isinstance(_.data, iterable) and _.type is not str\n",
        "    def _clone_iterable(_):\n",
        "        if _.inherits(dict): return _.data.copy()\n",
        "        elif _.isgenerator or _.isgeneratorfunction: return (i for i in list(_.data))\n",
        "        else: return type(_.data)(list(_.data)[:])\n",
        "    def _meta(data): return {k:v for k,v in data.__dict__.items() if k != 'data' and not isfx(v)}\n",
        "    def _by_val(_, depth=-1, layer=0):\n",
        "        data = _.data\n",
        "        if _.isiterable: data = _._clone_iterable()\n",
        "        elif _.inherits(ClonableObjectGhost): data = by_val(data, depth, layer)\n",
        "        return type(_)(data, **meta(_))\n",
        "    def __enter__(self): self._instance = self; return self\n",
        "    def __exit__(self, type, value, traceback): self._instance = None\n",
        "    def __repr__(self):\n",
        "        if not hasattr(self, '_preprint'): return Record(self.data, _preprint='', _lines=Record(Record._lines_)).__repr__()\n",
        "        if self.isbaseiterable:\n",
        "            pre, repr = self._preprint, ''\n",
        "            for n,i in enumerate(self.data):\n",
        "                if self._lines.data == 0: break\n",
        "                else: self._lines.data -= 1\n",
        "                index, item = str(n), i\n",
        "                if self.inherits(dict): index += ' ({})'.format(str(i)); item = self.data[i]\n",
        "                repr += pre+'{}: '.format(index)\n",
        "                next = Record(item, _preprint=pre+'\\t', _lines=self._lines)\n",
        "                if next.isiterable: repr += '\\n'\n",
        "                repr += next.__repr__()\n",
        "                repr += '\\n'\n",
        "            return repr\n",
        "        elif self.inherits(GCo): return Record(self.data._meta(), _preprint=self._preprint, _lines=self._lines).__repr__()\n",
        "        else: return self._repr(self.data)\n",
        "REc = Record\n",
        "\n",
        "class Bisect(list, COb):\n",
        "    \"\"\" bisect implementation using clonable objects \"\"\"\n",
        "    def __init__(set, *items, key=None, reverse=False):\n",
        "        if not key: key = lambda  x:x\n",
        "        super().__init__(sorted(items, reverse=reverse, key=key))\n",
        "    def _bisect(set, item, key, reverse, bottom, top):\n",
        "        def _(check):\n",
        "            if key: return key(check)\n",
        "            return check\n",
        "        at = int((top-bottom)/2)+bottom\n",
        "        if len(set)==0: return (0,-1)\n",
        "        if item==_(set[at]): return (at,0)\n",
        "        bigger = item<_(set[at])\n",
        "        if bigger != reverse:\n",
        "            if at-bottom>0: return set._bisect(item, key, reverse, bottom, at)\n",
        "            return (at,-1)\n",
        "        elif top-at>1: return set._bisect(item, key, reverse, at, top)\n",
        "        return (at,1)\n",
        "    def search(_, item, key=None, reverse=False):\n",
        "        if not key: key = lambda x:x\n",
        "        return _._bisect(item, key, reverse, 0, len(_))\n",
        "    def _by_val(_, depth=-1, _layer=0):\n",
        "        copy = super()._by_val(depth, _layer)\n",
        "        copy += _[:]\n",
        "        return copy\n",
        "BSx = Bisect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gv1SbG87JLJZ",
        "outputId": "d270650b-02bc-4d45-8daf-360412159f6c"
      },
      "outputs": [],
      "source": [
        "from numpy import ndarray, resize, linspace, arange\n",
        "from numpy import min, max, average, floor\n",
        "from numpy import ubyte, zeros, array\n",
        "from scipy.signal import lfilter, butter\n",
        "from matplotlib import pylab as lab\n",
        "\n",
        "_NOTCH = _FR = 50\n",
        "_SAMPLING = 500\n",
        "_CONTINUOUS = 1\n",
        "_UNIT = 'ms'\n",
        "\n",
        "class rec(table, ndarray):\n",
        "    @property\n",
        "    def dimensions(of): return len(of.shape)\n",
        "    @property\n",
        "    def is_scalar(this): return this.shape is ()\n",
        "    @property\n",
        "    def is_vector(this): return len(this.shape)==1\n",
        "    @property\n",
        "    def is_matrix(this): return len(this.shape)>1\n",
        "    @property\n",
        "    def is_cube(this): return len(this.shape) == 3\n",
        "    @property\n",
        "    def is_binary(this): return this.dtype == ubyte and max(this) == 1\n",
        "    @property\n",
        "    def serialized(data):\n",
        "        if not data.is_scalar and data.dimensions>1:\n",
        "            return rec.read(data.T.flatten(), _deser=data.T.shape, **meta(data))\n",
        "        return data\n",
        "    @property\n",
        "    def deserialized(data):\n",
        "        if data.has('_deser'):\n",
        "            deser = rec.read(resize(data, data._deser).T, **meta(data))\n",
        "            deser.clear('_deser')\n",
        "            return deser\n",
        "        return data\n",
        "    @property\n",
        "    def as_matrix(data):    #implement numpy matrix\n",
        "        if data.is_vector: return rec.read([data], to=type(data), **meta(data))\n",
        "        return data\n",
        "    @property\n",
        "    def raw(data):\n",
        "        if data.shape[0] == 1: return rec.read(data[0], **meta(data)).raw\n",
        "        return data\n",
        "    def join(base, *parts, **sets):\n",
        "        flip, parts = None, list(parts)\n",
        "        if 'flip' in sets: flip=sets.pop('flip')\n",
        "        next = parts[0]\n",
        "        if len(parts)>1: next = rec.join(parts[0], parts[1:])\n",
        "        congruent = base.dimensions == next.dimensions and base.dimensions < 3\n",
        "        if congruent:\n",
        "            sets.update(base._clonable())\n",
        "            A, B = base, next\n",
        "            if flip: A, B = base.T, next.T\n",
        "            C = record(A.tolist()+B.tolist(), **sets)\n",
        "            if flip: return record(C.T, **sets)\n",
        "            return C\n",
        "    def get_as(this, data, cast=None):\n",
        "        source = this\n",
        "        if no(cast):\n",
        "            if issubclass(type(data), rec): cast = type(data)\n",
        "            else: cast = type(this)\n",
        "        if issubclass(type(data), ndarray): source = resize(this, data.shape)\n",
        "        return rec.read(source, to=cast, **meta(data))\n",
        "    @staticmethod\n",
        "    def read(iterable, to=None, **sets):\n",
        "        if no(to) or not issubclass(to, rec): to = rec\n",
        "        data = array(iterable).view(to)\n",
        "        data.set(**sets)\n",
        "        return data\n",
        "    def clone(this, **sets):\n",
        "        copy = this.copy().view(type(this))\n",
        "        sets.update(this._clonable())\n",
        "        copy.set(**sets)\n",
        "        return copy\n",
        "    def exclude(data, *items, **sets):\n",
        "        if no(items) or data.is_scalar: return data\n",
        "        excluded, items = None, [item for item in range(len(data)) if item not in items]\n",
        "        if data.is_vector: excluded = rec.read([data])[:,items][0]\n",
        "        else: excluded = data[items,:]\n",
        "        return rec.read(excluded, to=type(data), **meta(data), **sets)\n",
        "    def include(data, *items, **sets):\n",
        "        if no(items) or data.is_scalar: return data\n",
        "        included = []\n",
        "        if data.is_vector: included = rec.read([data])[:,items][0]\n",
        "        else: included = data[items,:]\n",
        "        return rec.read(included, to=type(data), **meta(data), **sets)\n",
        "\n",
        "create = record = rec.read\n",
        "line = linspace\n",
        "\n",
        "def series(ori,end=None,by=1):\n",
        "    if no(end): end=ori; ori=0\n",
        "    if not issubclass(type(by), int): return create(arange(ori,end,by))\n",
        "    return array(range(ori,end,by))\n",
        "\n",
        "def plot(data, at = 0., spacing = 1., color = 'k', width = 1., offset=0.): #review\n",
        "    draw = record(data, **meta(data)); at = spacing*draw.as_matrix.shape[0]\n",
        "    axes = lab.gca(); axes.set_ylim([at+max(data),0-max(data)]); at=0\n",
        "    for n, row in ni(draw.as_matrix):\n",
        "        if some(offset): row = draw[n]-average(row)+offset\n",
        "        c, w = color, width\n",
        "        if isiterable(color): c = color[n]\n",
        "        if isiterable(width): w = width[n]\n",
        "        lab.plot(at+row+n*spacing, color = c, linewidth = w)\n",
        "\n",
        "def butter_type(lowcut, highcut, fs, order=5, type='band'):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = butter(order, [low, high], btype=type)\n",
        "    return b, a\n",
        "\n",
        "def butter_filter(data, lowcut, highcut, fs, order=5, type='band'):\n",
        "    b, a = butter_type(lowcut, highcut, fs, order=order, type=type)\n",
        "    y = lfilter(b, a, data)\n",
        "    return y\n",
        "\n",
        "def _to_rec(this):\n",
        "    if not issubclass(type(this), rec): return rec.read(this, **meta(this)), rec\n",
        "    return this, type(this)\n",
        "\n",
        "def _prefilt(data, fs):\n",
        "    pre = []\n",
        "    for line in data.as_matrix:\n",
        "        vector = line.tolist()\n",
        "        pre.append(vector[:int(fs)]+vector)\n",
        "    return record(pre)\n",
        "\n",
        "def _postfilt(data, fs):\n",
        "    post = []\n",
        "    for line in data:\n",
        "        vector = line.tolist()\n",
        "        post.append(vector[int(fs):])\n",
        "    return post\n",
        "\n",
        "def notch(this, using=butter_type, fs=_SAMPLING, size=2, at=_NOTCH, order=5):\n",
        "    data, type = _to_rec(this)\n",
        "    if data.has('sampling'): fs=data.sampling\n",
        "    nyq, cutoff = fs / 2., []\n",
        "    for f in range(int(at), int(nyq), int(at)):\n",
        "        cutoff.append((f - size, f + size))\n",
        "    signal = _prefilt(data, fs)\n",
        "    for bs in cutoff:\n",
        "        low,hi = bs\n",
        "        b,a = butter_type(low,hi,fs,order,'bandstop')\n",
        "        signal = lfilter(b,a,signal)\n",
        "    return record(_postfilt(signal, fs), to=type, **meta(data))\n",
        "\n",
        "def band(this, low_high, fs=_SAMPLING, using=butter_filter, order=5):\n",
        "    data, type = _to_rec(this)\n",
        "    if data.has('sampling'): fs=data.sampling\n",
        "    low, high = min(low_high), max(low_high)\n",
        "    if low<1.: low = 1.\n",
        "    tailed = _prefilt(data, fs)\n",
        "    tailed = using(tailed, low, high, fs, order)\n",
        "    return rec.read(_postfilt(tailed, fs), to=type, **meta(data))\n",
        "\n",
        "def binarize(this):\n",
        "    data, type = _to_rec(this)\n",
        "    if data.is_binary: return data\n",
        "    rows = []\n",
        "    for row in data.as_matrix:\n",
        "        d = row - array([row[-1]]+row[:-1].tolist())\n",
        "        d[d>=0] = 1; d[d<0] = 0\n",
        "        rows.append(d.astype(ubyte))\n",
        "    return rec.read(rows, to=type).get_as(data)\n",
        "\n",
        "def halve(matrix):\n",
        "    halved, (data, type) = [], _to_rec(matrix)\n",
        "    for line in data.as_matrix:\n",
        "        h = resize(line, (int(len(line)/2), 2))\n",
        "        halved.append((h[:,0]+h[:,1])/2.)\n",
        "    return rec.read(halved, to=type, **meta(data))\n",
        "\n",
        "def dwindle(matrix, by=1):\n",
        "    if by: return dwindle(halve(matrix), by-1)\n",
        "    return matrix\n",
        "\n",
        "def upsample(matrix, fs1, fs2):\n",
        "    y=zeros((matrix.shape[0],fs2))\n",
        "    if fs1 < fs2:\n",
        "        #upsampling by a factor R\n",
        "        L=matrix.shape[1]\n",
        "        R=int(floor(fs2/fs1)+1)\n",
        "        for i,e in enumerate(matrix):\n",
        "            ups=[]\n",
        "            for j in range(L-1):\n",
        "                if j>0: ups.append(list(linspace(matrix[i][j],matrix[i][j+1],R)[1:3]))\n",
        "                else: ups.append(list(linspace(matrix[i][j],matrix[i][j+1],R)[0:3]))\n",
        "            for k,s in enumerate(sum(ups, [])): y[i][k]=s \n",
        "            y[i][-1]=y[i][-2]\n",
        "        return rec.read(y)\n",
        "    else: print(\"Error: fs1 >= fs2\")\n",
        "\n",
        "def remap(this, axis=None, base=0, top=1., e=0):\n",
        "    def map(x, b, t, e): return ((x-min(x)+e)/(max(x)-min(x)+e)+b)*(t-b)\n",
        "    data, type = _to_rec(this)\n",
        "    if no(axis): return rec.read(map(this, base, top, e), to=type, **meta(data))\n",
        "    rows = data.as_matrix\n",
        "    if axis==0 or axis>1: rows = rows.T\n",
        "    remapped = []\n",
        "    for row in rows: remapped.append(map(row, base, top, e))\n",
        "    if axis==0 or axis>1: rows = rows.T\n",
        "    return rec.read(remapped).get_as(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lkw3_yqgJLGo"
      },
      "outputs": [],
      "source": [
        "this = Record\n",
        "\n",
        "from numpy import array\n",
        "import scipy.stats as stats\n",
        "\n",
        "class Table(COb, GEq, GRe):\n",
        "    data = None\n",
        "    default = None\n",
        "    PAD = 3\n",
        "    ELLIPSIS_AT = int(GRe._lines_*.3)\n",
        "    class _axes(list):\n",
        "        def insert(_from, this, item):\n",
        "            super().insert(this, item)\n",
        "            _from.__dict__[item.name] = item\n",
        "        def __setitem__(_, pos, axis):\n",
        "            super().__setitem__(pos, axis)\n",
        "            _.__dict__[axis.name] = axis\n",
        "    class axis(list, GSet, GRe):\n",
        "        name = None\n",
        "        root = None\n",
        "        _to = 0\n",
        "        def __init__(axis, root, labels, name='ax', force_at=None):\n",
        "            if force_at: root.axes[force_at].name = None\n",
        "            with this(labels) as dim:\n",
        "                if not dim.isiterable and dim.inherits(int): labels = range(labels)\n",
        "            super().__init__(labels)\n",
        "            names = [ax.name for ax in root.axes]\n",
        "            name_, n = name, 1\n",
        "            while name in names: name = name_ + str(n); n+=1\n",
        "            axis._set(root=root, name=name)\n",
        "            if force_at: root.axes[force_at] = axis\n",
        "            else: root.axes.insert(0, axis)\n",
        "        def at(axis, field):\n",
        "            field = int(field) if this(field).inherits(str) and field.isdecimal() else field\n",
        "            found = axis.index(field) if field in axis else None\n",
        "            axis._to = found if found is not None else field\n",
        "        def __repr__(_):\n",
        "            return '{}: {}'.format(_.name, _._resize(' '.join([str(i) for i in _])))\n",
        "    def __init__(this, **table_description):\n",
        "        super().__init__(axes=this._axes())\n",
        "        this.set(**table_description)\n",
        "    def reset(data):\n",
        "        base = None\n",
        "        if len(data.axes)>0:\n",
        "            base = [data.default]*len(data.axes[-1])\n",
        "            for ax in reversed(data.axes[0:-1]): base = [base]*len(ax)\n",
        "        data._set(data=array(base))\n",
        "    @property\n",
        "    def ax_names(_): return [ax.name for ax in _.axes]\n",
        "    def at(data, axis):\n",
        "        with this(axis) as _axis:\n",
        "            if _axis.inherits(int):\n",
        "                if axis>0 and axis<len(data.axes): return data.axes[axis]\n",
        "            elif _axis.inherits(str):\n",
        "                axes = data.ax_names\n",
        "                if axis in axes: return data.axes[axes.index(axis)]\n",
        "        return None\n",
        "    def _check(build):\n",
        "        if build.data is None: build.reset()\n",
        "        return build.data\n",
        "    def _find(_, inverted, ax_field):\n",
        "        _._check()\n",
        "        def index(axis, entry):\n",
        "            fields = _.at(axis)\n",
        "            if fields is not None:\n",
        "                if this(entry).isiterable:\n",
        "                    return tuple([fields.index(field) for field in entry])\n",
        "                else: return ':'\n",
        "            return None\n",
        "        def translate(axis, found):\n",
        "            if axis.name in found:\n",
        "                _range = found[axis.name]\n",
        "                if this(_range).inherits(tuple):\n",
        "                    if inverted: found[axis.name] = tuple([field for field in range(len(axis)) if field not in _range])\n",
        "                    return \"_from['{}']\".format(axis.name)\n",
        "            return ':'\n",
        "        found={field:index(field,entry) for field,entry in ax_field.items()}\n",
        "        found={field:value for field,value in found.items() if value is not None}\n",
        "        reshape='M['+','.join([translate(axis,found) for axis in _.axes])+']'\n",
        "        _._set(_reshape_ = (reshape, found))\n",
        "    def _by_val(_, depth=-1, _layer=0):\n",
        "        M, axes = _._check(), _.axes\n",
        "        do, _from = _.__dict__.pop('_reshape_') if '_reshape_' in _._meta() else (None, {})\n",
        "        copy = super()._by_val(depth, _layer)\n",
        "        copy.axes = []\n",
        "        for ax in reversed(axes):\n",
        "            fields = [field for n,field in enumerate(ax) if n in _from[ax.name]] if ax.name in _from else ax\n",
        "            copy.set(**{ax.name:fields})\n",
        "        copy.data = eval(do) if do else M.copy()\n",
        "        return copy\n",
        "    def _translate(_, directions):\n",
        "        axes = directions.split(',')\n",
        "        for ax_dir in axes:\n",
        "            ax, field = [token.strip() for token in ax_dir.split(':')]\n",
        "            axis = _.at(ax)\n",
        "            if axis: axis.at(field)\n",
        "        return '['+','.join([str(ax._to) for ax in _.axes])+']'\n",
        "    def _get_set(_, directions, mode='get', value=None):\n",
        "        if mode == 'get' and not '_MGET' in _.sets: _._MGET = []\n",
        "        if len(directions) == len(_.axes):\n",
        "            resolve, message = True, []\n",
        "            for n,part in enumerate(directions):\n",
        "                _part = this(part)\n",
        "                if _part.inherits(int, str) or _part.isiterable and len(part)==1:\n",
        "                    token = part if _part.inherits(str, int) else part[0]\n",
        "                    message.append(':'.join([str(_.axes[n].name),str(part)]))\n",
        "                else:\n",
        "                    resolve = False\n",
        "                    for token in part:\n",
        "                        redirection = list(directions)\n",
        "                        redirection[n] = token\n",
        "                        _._get_set(tuple(redirection), mode, value)\n",
        "            if resolve:\n",
        "                message = ','.join(message)\n",
        "                if mode=='get': _._MGET.append(_[message])\n",
        "                else: _[message] = value\n",
        "    def __getitem__(by, field_directions):\n",
        "        M = by._check()\n",
        "        if this(field_directions).inherits(tuple):\n",
        "            by._get_set(field_directions)\n",
        "            result = by.__dict__.pop('_MGET')\n",
        "            return result\n",
        "        else: return eval('M'+by._translate(field_directions))\n",
        "    def __setitem__(by, field_directions, value):\n",
        "        M = by._check()\n",
        "        if this(field_directions).inherits(tuple): by._get_set(field_directions, 'set', value)\n",
        "        else: exec('M'+by._translate(field_directions)+'=value')\n",
        "    def set(data, **ax_field):\n",
        "        for name, fields in ax_field.items(): data.axis(data, fields, name)\n",
        "    def get(data, **ax_field):\n",
        "        data._find(0, ax_field)\n",
        "        return data._by_val()\n",
        "    def let(data, **ax_field):\n",
        "        data._find(1, ax_field)\n",
        "        return data._by_val()\n",
        "    @property\n",
        "    def sets(tree): return set(meta(tree))\n",
        "    def __repr__(self):\n",
        "        M = self._check()\n",
        "        _repr, dimensions = '', len(self.axes)\n",
        "        if not dimensions: _repr += 'void table\\n'\n",
        "        else:\n",
        "            dimensions = len(self.axes)\n",
        "            y = self.axes[-2] if dimensions >= 2 else None\n",
        "            if dimensions>2:\n",
        "                y = self.axes[-2]\n",
        "                for n,ax in enumerate(self.axes[:-2]): _repr += '{}{}: {}/{}\\n'.format('\\t'*n, ax.name, ax._to, len(ax))\n",
        "            mr = eval('M'+str([ax.index(ax._to) for ax in self.axes][:-2])) if dimensions>2 else M\n",
        "            pad = max([len(y.name)]+[len(str(field)) for field in y]+[len(str(value)) for line in mr for value in line])+self.PAD if dimensions>1 else 0\n",
        "            _repr, x, spaces = _repr+y.name+'\\n' if y else '', self.axes[-1], ' '*pad if pad>0 else '\\t'\n",
        "            header = spaces+''.join([str(field).ljust(pad) for field in x])\n",
        "            _repr += self._resize(header) + '\\n'\n",
        "            ellipsis_at = self._lines_-self.ELLIPSIS_AT-1\n",
        "            last_values_from = len(mr)-self.ELLIPSIS_AT\n",
        "            if last_values_from<=ellipsis_at: last_values_from = ellipsis_at+1\n",
        "            for n, line in enumerate(mr):\n",
        "                if n<ellipsis_at or n>last_values_from:\n",
        "                    values = str(y[n]).ljust(pad) if y else ''\n",
        "                    values += ''.join([str(value).ljust(pad) for value in line])\n",
        "                    _repr += self._resize(values) + '\\n'\n",
        "                elif n==ellipsis_at:\n",
        "                    _repr += self._ellipsis_ + '\\n'\n",
        "        extra = {k:v for k,v in meta(self).items() if k != 'data' and k != 'axes'}\n",
        "        _repr += self._resize(spaces*len(x)+x.name)+'\\n'+'\\n'.join(['{}:\\t{}'.format(k, self._repr(v)) for k,v in extra.items()])\n",
        "        return _repr\n",
        "TAb = tab = Table\n",
        "\n",
        "def set_repr_to(lines, ratio=.7):\n",
        "    set_repr_to(lines)\n",
        "    Table.ELLIPSIS_AT = int(Table._lines_*(1-ratio))\n",
        "\n",
        "def butter_type(lowcut, highcut, fs, order=5, type='band'):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = butter(order, [low, high], btype=type)\n",
        "    return b, a\n",
        "\n",
        "def butter_filter(data, lowcut, highcut, fs, order=5, type='band'):\n",
        "    b, a = butter_type(lowcut, highcut, fs, order=order, type=type)\n",
        "    y = lfilter(b, a, data)\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdRMr1rVJLBH",
        "outputId": "34c41788-25db-4011-9954-c98474bcac19"
      },
      "outputs": [],
      "source": [
        "!pip install pyEDFlib # installing the pyEDFlib library in the Google Colab environment\n",
        "\n",
        "import pyedflib as edf\n",
        "\n",
        "class EEG(Table):\n",
        "    LABEL_START = 'EEG '\n",
        "    BAD = ['TTL', 'ECG']\n",
        "    BP_SEP = '-'\n",
        "    class time:\n",
        "        \"\"\" converts time units to seconds by frequency sampling (fs) \"\"\"\n",
        "        unit = 'units'\n",
        "        def __init__(_, units): _.time = units\n",
        "        def __call__(_, fs=None): return _.time/fs\n",
        "        def __repr__(_): return '{} {}'.format(str(_.time),_.unit)\n",
        "    class ms(time):\n",
        "        \"\"\" converts ms to time units by frequency sampling (fs) \"\"\"\n",
        "        unit = 'ms'\n",
        "        def __call__(_, fs=1000): return int(round(_.time*fs/1000))\n",
        "    class secs(time):\n",
        "        \"\"\" converts seconds to time units by frequency sampling (fs) \"\"\"\n",
        "        unit = 's'\n",
        "        def __call__(_, fs=1000): return int(_.time*fs)\n",
        "    def _load(eeg, epoch, n):\n",
        "        data = None\n",
        "        with edf.EdfReader(eeg.file) as file:\n",
        "            data = [file.readSignal(eeg.labels[id], epoch.at, epoch.span) for id in eeg.labels]\n",
        "            file.close()\n",
        "        if data is not None:\n",
        "            eeg._set(data=np.array(data), at_epoch=(n, epoch()))\n",
        "        else:\n",
        "            if 'at_epoch' in eeg.sets: del(eeg.at_epoch)\n",
        "            eeg._set(data=None)\n",
        "    class step(GSet):\n",
        "        START = 0\n",
        "        CENTRE = 1\n",
        "        END = 2\n",
        "        def __init__(step, space, duration):\n",
        "            step._set(at=space, span=duration)\n",
        "        def reset(grid, at=0, root=None):\n",
        "            if root: grid._set(root=root)\n",
        "            else: root = grid.root\n",
        "            all_space, left = root.duration(root.fs), 0\n",
        "            if grid.at.time == 0: epochs = [EEG.step(0, all_space)]\n",
        "            else:\n",
        "                space, span, epochs = grid.at(root.fs), grid.span(root.fs), []\n",
        "                for x in range(at, all_space, space):\n",
        "                    end = x+span\n",
        "                    if end>all_space: left = all_space-x\n",
        "                    else: epochs.append(EEG.step(x, span))\n",
        "            grid._set(_all=epochs, skip=at, out=left)\n",
        "        def __call__(step, _as=None):\n",
        "            if 'root' in meta(step):\n",
        "                if _as == None: return len(step._all)\n",
        "            elif _as is not None: step.id = _as\n",
        "            elif 'id' in meta(step): return step.id\n",
        "        def items(wrapped):\n",
        "            if 'root' in meta(wrapped): return wrapped._all\n",
        "        def __getitem__(by, epoch_n):\n",
        "            if 'root' in meta(by) and epoch_n<len(by._all):\n",
        "                by.root._load(by._all[epoch_n], epoch_n)\n",
        "        def __repr__(_): return '|'.join([repr(_.at),repr(_.span)])\n",
        "    class event(GSet):\n",
        "        def __init__(event, to=None, group=None, _as=0, _from=0):\n",
        "            event._set(mode=_from, note=group, id=_as)\n",
        "            if to is not None: event.link(to)\n",
        "        def link(event, to):\n",
        "            if event.note is None or not 'event' in to.sets:\n",
        "                if event.note is None:\n",
        "                    to.event = event\n",
        "                    event.type = []\n",
        "                    return\n",
        "                else: EEG.event(to)\n",
        "            types = to.event.type\n",
        "            ids = [to.event.id]+[_type.id for _type in types]\n",
        "            while event.id in ids: event.id += 1\n",
        "            if event.note in to.notes:\n",
        "                event.at = to.notes[event.note]\n",
        "                types.append(event)\n",
        "        def __repr__(event):\n",
        "            _repr = str(event.id)\n",
        "            if 'at' in meta(event): _repr += ' at: {}'.format(event.at)\n",
        "            if 'type' in meta(event):\n",
        "                for subev in event.type: _repr += '; '+repr(subev)\n",
        "            return _repr\n",
        "    def _at_last(eeg, sets):\n",
        "        if 'epoch' in meta(eeg):\n",
        "            eeg.epoch.reset(root=eeg)\n",
        "            if len(eeg.axes.time) != eeg.epoch.span(eeg.fs): eeg.axis(eeg, eeg.epoch.span(eeg.fs), 'time', 1)\n",
        "    @staticmethod\n",
        "    def from_file(name, step=None, bad=None):\n",
        "        def correct_(label):\n",
        "            if label.startswith(EEG.LABEL_START): return label[len(EEG.LABEL_START):]\n",
        "            return label\n",
        "        eeg = EEG()\n",
        "        with edf.EdfReader(name) as file:\n",
        "            if bad is None: bad = EEG.BAD\n",
        "            duration = EEG.secs(file.getFileDuration())\n",
        "            fs = file.getSampleFrequencies()[0]\n",
        "            if step is None: step = EEG.step(EEG.secs(0), duration)\n",
        "            raw_notes = file.readAnnotations()\n",
        "            notes = {note:[] for note in set(raw_notes[-1])}\n",
        "            for n, note in enumerate(raw_notes[-1]):\n",
        "                notes[note].append(EEG.secs(raw_notes[0][n]))\n",
        "            labels = [correct_(label) for label in file.getSignalLabels()]\n",
        "            labels = {label:n for n,label in enumerate(labels) if label not in bad}\n",
        "            eeg.set(time=step.span(fs), region=tuple(labels))\n",
        "            eeg(file=name, duration=duration, fs=fs, notes=notes, labels=labels, epoch=step)\n",
        "            file.close()\n",
        "        return eeg\n",
        "    def remap(eeg, at=None, step=None):\n",
        "        sets = eeg.sets\n",
        "        if this(step).inherits(EEG.step): eeg._set(epoch=step)\n",
        "        if at is None:\n",
        "            at = eeg._best_map if '_best_map' in sets else 0\n",
        "        eeg.epoch.reset(at)\n",
        "        if 'event' in sets:\n",
        "            deltas = []\n",
        "            for epoch in eeg.epoch._all: epoch.id = None\n",
        "            for event in eeg.event.type:\n",
        "                for time in event.at:\n",
        "                    at, space, limit = time(eeg.fs), eeg.epoch.at(eeg.fs), len(eeg.epoch.items())-1\n",
        "                    for n,epoch in enumerate(eeg.epoch.items()):\n",
        "                        end = epoch.at+space if n<limit else epoch.span\n",
        "                        if at>=epoch.at and at<end:\n",
        "                            epoch(event.id)\n",
        "                            if event.mode == eeg.step.START: deltas.append(EEG.time(at-epoch.at)(eeg.fs))\n",
        "                            elif event.mode == eeg.step.END: deltas.append(EEG.time(end-at-1)(eeg.fs))\n",
        "                            else: \n",
        "                                centre = epoch.at+int(round(epoch.span/2))\n",
        "                                deltas.append(EEG.time(abs(at-centre))(eeg.fs))\n",
        "                            break\n",
        "            for epoch in eeg.epoch._all:\n",
        "                if epoch() is None: epoch(eeg.event.id)\n",
        "            eeg.deltas = deltas\n",
        "    def optimize(eeg, *events, grid=None):\n",
        "        if events:\n",
        "            for event in events: event.link(eeg)\n",
        "        eeg.remap(0, grid)\n",
        "        gaussian_space = stats.shapiro if len(eeg.deltas)<=5000 else stats.normaltest\n",
        "        def test():\n",
        "            _, p = gaussian_space(eeg.deltas) if len(eeg.deltas)>2 else 0,1\n",
        "            if p<=0.05: return p, np.median(eeg.deltas)\n",
        "            return p, np.average(eeg.deltas)    \n",
        "        (p, best), at, check = test(), 0, eeg.epoch.span(eeg.fs)\n",
        "        print('optimizing epoch position...', end=' ')\n",
        "        for _try in range(1, check):\n",
        "            eeg.remap(_try)\n",
        "            p, check = test()\n",
        "            if check<best: p, best, at = p, check, _try\n",
        "        _test = 'median' if p<0.05 else 'mean'\n",
        "        print('best frame found at {:.3f}s with a {} delay of {:.3f}s'.format(EEG.time(at)(eeg.fs), _test, EEG.time(best)(eeg.fs)))\n",
        "        eeg._set(_best_map=at)\n",
        "    class sampler(GSet, GRe):\n",
        "        eeg = None\n",
        "        def __init__(map, root, *reserve, **opts):\n",
        "            raw, proc = [step() for step in root.epoch.items()], []\n",
        "            find, key = None, {k:v for k,v in reserve}\n",
        "            for step in raw:\n",
        "                if find==step: find=None\n",
        "                if find is None: proc.append(step)\n",
        "                else: proc.append(None)\n",
        "                if step in key: find = key[step]\n",
        "            key = {k:[] for k in list(set(raw))+[None]}\n",
        "            for n,id in enumerate(proc): key[id].append(n)\n",
        "            map._set(eeg=root, key=key, mask=proc, **opts)\n",
        "        def _at_last(_, sets):\n",
        "            if 'seed' in sets: np.random.seed(_.seed)\n",
        "        def set(map, **event_range):\n",
        "            prev, key = map.key, {}\n",
        "            for k,deltas in event_range.items():\n",
        "                if k in prev:\n",
        "                    seq, key[k] = prev[k], []\n",
        "                    for item in seq: key[k] += [item+d for d in deltas]\n",
        "                    for o in prev:\n",
        "                        if o != k:\n",
        "                            for e in key[k]:\n",
        "                                if e in prev[o]: prev[o].pop(prev[o].index(e))\n",
        "            for k in prev:\n",
        "                if k not in key: key[k] = prev[k]\n",
        "            map._set(prev=prev, key=key)\n",
        "        def get(map, event, times, random_seed=None):\n",
        "            if random_seed and not 'seed' in meta(map): map._set(seed=random_seed)\n",
        "            if not 'pool' in meta(map): map._set(pool = {k:map.key[k].copy() for k in map.key})\n",
        "            resampled, sequence = [], []\n",
        "            while times:\n",
        "                if len(map.pool[event])==0: map.pool[event] = map.key[event].copy()\n",
        "                at = map.pool[event].pop(np.random.randint(len(map.pool[event])))\n",
        "                map.eeg.epoch[at]\n",
        "                resampled.append(map.eeg.data)\n",
        "                sequence.append(at)\n",
        "                times -= 1\n",
        "            return resampled, sequence\n",
        "        def __repr__(_): return _._resize('|'.join([str(id) if id!=None else ' ' for id in _.mask]))\n",
        "    def tag(event, *a_b, **event_range):\n",
        "        event._set(sample=event.sampler(event, *a_b))\n",
        "        event.sample.set(**event_range)\n",
        "        \n",
        "STEp = epoch = EEG.step\n",
        "TIME = EEG.time\n",
        "SET = EEG.event\n",
        "secs = EEG.secs\n",
        "ms = EEG.ms"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### The cell below defines the preprocessing function for resampling and notch filtering the data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess(eeg, epoch, limit=500): \n",
        "    \"\"\"Primary preprocessing. Resamples data to a limit frequency and applies a notch filter.\n",
        "\n",
        "    Args:\n",
        "        eeg (eeg): Wrapper object of the raw EEG data and metadata.\n",
        "        epoch (list): Signal epoch.\n",
        "        limit (int): Target frequency for resampling. Defaults to 500.\n",
        "\n",
        "    Returns:\n",
        "        list: Preprocessed epoch.\n",
        "    \"\"\"\n",
        "    sampling, rse = limit, epoch\n",
        "    if eeg.fs == limit: rse = epoch\n",
        "    elif eeg.fs%limit != 0: rse = upsample(epoch, eeg.fs, limit) if eeg.fs<limit else dwindle(epoch, int(eeg.fs/limit)-1) \n",
        "    else: rse = upsample(epoch, eeg.fs, limit) if eeg.fs<limit else dwindle(epoch, int(eeg.fs/limit)-2) \n",
        "    nse = notch(rse, fs=sampling, order=2)\n",
        "    return nse"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### The cell below implements six connectivity measures (seven, considering the double nature of spectral coherence - imaginary part and real part) to be computed between two nodes (signals):\n",
        "###### 1) Phase-locking value\n",
        "###### 2) Phase-lag index\n",
        "###### 3) Spectral coherence\n",
        "###### 4) Cross-correlation\n",
        "###### 5) Phase-amplitude coupling\n",
        "###### 6) Prediction error connectivity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hNilIqsJK5e"
      },
      "outputs": [],
      "source": [
        "from numpy import correlate, average, array, angle, mean, sign, exp, zeros, abs, unwrap, fromfile, unpackbits, packbits\n",
        "from scipy.signal import hilbert, csd\n",
        "\n",
        "def phaselock(signal1, signal2):\n",
        "    \"\"\"Computes the phase locking value between two notch-filtered signals.\n",
        "    \n",
        "    Args:\n",
        "        signal1 (array): Timecourse recorded from a first node.\n",
        "        signal2 (array): Timecourse recorded from a second node.\n",
        "\n",
        "    Returns:\n",
        "        float: Phase locking value.\n",
        "    \"\"\"\n",
        "    sig1_hil = hilbert(signal1)                          \n",
        "    sig2_hil = hilbert(signal2)\n",
        "    phase1 = angle(sig1_hil)                           \n",
        "    phase2 = angle(sig2_hil)\n",
        "    phase_dif = phase1-phase2                             \n",
        "    plv = abs(mean(exp(complex(0,1)*phase_dif)))    \n",
        "    return plv\n",
        "\n",
        "def phaselag(signal1, signal2):\n",
        "    \"\"\"Computes the phase lag index between two signals.\n",
        "    \n",
        "    Args:\n",
        "        signal1 (array): Timecourse recorded from a first node.\n",
        "        signal2 (array): Timecourse recorded from a second node.\n",
        "\n",
        "    Returns:\n",
        "        float: Phase lag index.\n",
        "    \"\"\"\n",
        "    sig1_hil = hilbert(signal1)                 \n",
        "    sig2_hil = hilbert(signal2)\n",
        "    phase1 = angle(sig1_hil)                 \n",
        "    phase2 = angle(sig2_hil)\n",
        "    phase_dif = phase1-phase2                   \n",
        "    pli = abs(mean(sign(phase_dif)))     \n",
        "    return pli\n",
        "\n",
        "def spectral_coherence(signal1, signal2, fs, imag=False):\n",
        "    \"\"\"Computes the spectral coherence between two signals.\n",
        "\n",
        "    Args:\n",
        "        signal1 (array): Timecourse recorded from a first node.\n",
        "        signal2 (array): Timecourse recorded from a second node.\n",
        "        fs (int): Sampling frequency.\n",
        "        imag (bool): If True, computed the imaginary part of spectral coherence, if False computes the real part. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        float: Spectral coherence.\n",
        "    \"\"\"\n",
        "    Pxy = csd(signal1,signal2,fs=fs, scaling='spectrum')[1] \n",
        "    Pxx = csd(signal1,signal1,fs=fs, scaling='spectrum')[1]\n",
        "    Pyy = csd(signal2,signal2,fs=fs, scaling='spectrum')[1]\n",
        "    if imag: return average((Pxy.imag)**2/(Pxx*Pyy))     \n",
        "    elif not imag: return average(abs(Pxy)**2/(Pxx*Pyy))\n",
        "\n",
        "def cross_correlation(signal1, signal2):\n",
        "    \"\"\"Computes the cross correlation between two signals.\n",
        "    \n",
        "    Args:\n",
        "        signal1 (array): Timecourse recorded from a first node.\n",
        "        signal2 (array): Timecourse recorded from a second node.\n",
        "\n",
        "    Returns:\n",
        "        float: Cross correlation.\n",
        "    \"\"\"\n",
        "    return correlate(signal1, signal2, mode=\"valid\")\n",
        "\n",
        "def PAC(signal1, signal2, fs):\n",
        "    \"\"\"Computes low frequency phase-high frequency amplitude phase coupling between two signals.\n",
        "    Low frequency = 1-4 Hz; High frequency = 30-70 Hz\n",
        "    Args:\n",
        "        signal1 (array): Timecourse recorded from a first node.\n",
        "        signal2 (array): Timecourse recorded from a second node.\n",
        "        fs (int): Sampling frequency.\n",
        "\n",
        "    Returns:\n",
        "        float: Phase-amplitude coupling.\n",
        "    \"\"\"   \n",
        "    low = butter_filter(signal1,1,4,fs) \n",
        "    high = butter_filter(signal2,30,70,fs) \n",
        "    low_hil = hilbert(low)\n",
        "    low_phase_angle = unwrap(angle(low_hil))   \n",
        "    high_env_hil = hilbert(abs(hilbert(high)))\n",
        "    high_phase_angle = unwrap(angle(high_env_hil))\n",
        "    phase_dif = low_phase_angle - high_phase_angle \n",
        "    plv = abs(mean(exp(complex(0,1)*phase_dif)))\n",
        "    return plv\n",
        "\n",
        "class bit:\n",
        "    def __init__(my, size = 32): my.states = size\n",
        "    def resize(this, bits):\n",
        "        n, max = 0, 1\n",
        "        for bit in bits:\n",
        "            n += bit * max\n",
        "            max <<= 1\n",
        "        n = int(round(n / float(max) * this.states)) - 1\n",
        "        max, bits = 1, []\n",
        "        while(max < this.states):\n",
        "            bits.append((n & max) / max)\n",
        "            max <<= 1\n",
        "        return bits\n",
        "\n",
        "class AWC(struct, dict):\n",
        "    train, limit = 10, 100\n",
        "    bits = 8\n",
        "    time = 8\n",
        "    blur = None\n",
        "    same = False\n",
        "    class lnx:\n",
        "        l, n = 1, 2\n",
        "        @property\n",
        "        def _clone(this):\n",
        "            copy = AWC.lnx()\n",
        "            copy.l, copy.n = this.l, this.n\n",
        "            return copy\n",
        "        def __add__(this, bit):\n",
        "            bit, learn, limit = bit\n",
        "            this.l += bit * learn\n",
        "            this.n += learn\n",
        "            if this.n > limit: this.n /= 2.; this.l /= 2.\n",
        "        def __call__(set, weight): return set.l * weight, set.n * weight\n",
        "    def __init__(context, **params):\n",
        "        context.set(_codes=[], _last=None, **params)\n",
        "    def clone(this, **changes):\n",
        "        copy = AWC()\n",
        "        copy.set(**this._clonable)\n",
        "        copy.set(**changes)\n",
        "        copy.update(this)\n",
        "        for context in copy: copy[context] = copy[context]._clone\n",
        "        return copy\n",
        "    def _make(actual, set):\n",
        "        actual._codes, context = [], []\n",
        "        for bits in set:\n",
        "            context += bits\n",
        "            actual._codes.append(tuple(context))\n",
        "    def __call__(actual):\n",
        "        l, n, w = 1., 2., 1\n",
        "        for length, context in enumerate(actual._codes):\n",
        "            w *= len(context)\n",
        "            if context in actual:\n",
        "                _l, _n = actual[context](w)\n",
        "                l += _l; n += _n\n",
        "            else: actual[context] = AWC.lnx()\n",
        "        return l/n\n",
        "    def __add__(last, bit):\n",
        "        for context in last._codes: last[context] + bit\n",
        "    def learn(symbol, data=None, file=None, tell=False):\n",
        "        if file: data = unpackbits(fromfile(file, dtype = 'ubyte'))\n",
        "        check, to = None, 0.\n",
        "        if tell: check = tell*len(data)\n",
        "        train, limit = symbol.train, symbol.limit\n",
        "        set, coded, time = [], [], symbol.time\n",
        "        while time: set.append(list()); time -= 1\n",
        "        for n, bit in enumerate(data.tolist()):\n",
        "            if tell and n%check==0: print('{:.0%}|'.format(to), end=''); to+=tell\n",
        "            if symbol.same: symbol._make([[n%symbol.bits]]+set[1:])\n",
        "            else: symbol._make(set)\n",
        "            coded.append(symbol())\n",
        "            symbol + (bit, train, limit)\n",
        "            base = set[0]\n",
        "            if len(base) == symbol.bits:\n",
        "                if symbol.blur: base = symbol.blur.resize(base)\n",
        "                set.insert(1, packbits(base).tolist())\n",
        "                set.pop(-1)\n",
        "                set[0] = [bit]\n",
        "            else: base.append(bit)\n",
        "        symbol._last = data\n",
        "        return dict(code=record(coded, **meta(data)), error=record(abs(coded-data), **meta(data)))\n",
        "\n",
        "def error(matrix, layers=1, mmult=3, tell=.1, dtail=True):\n",
        "    d = len(matrix)\n",
        "    E, pairs = zeros((d,d)), []\n",
        "    O, l = zeros((d,d)), zeros((d,d))\n",
        "    for a in range(d):\n",
        "        if dtail:\n",
        "            for b in range(d): pairs.append((a,b))\n",
        "        else:\n",
        "            for b in range(a, d): pairs.append((a,b))        \n",
        "    to, check = None, None\n",
        "    if tell: to, check = 0., int(tell*len(pairs))\n",
        "    for n, (a, b) in enumerate(pairs):\n",
        "        if tell=='all': print('.', end='')\n",
        "        elif tell<1. and n%check==0: print('{:.0%}|'.format(to), end=''); to+=tell\n",
        "        c, data = AWC(bits=2, time=2*mmult), create([matrix[a],matrix[b]])\n",
        "        R = c.learn(binarize(data).serialized)\n",
        "        if layers==1 or layers==3:\n",
        "            E[b,a] += average(R['error'].deserialized[0])\n",
        "            E[a,b] += average(R['error'].deserialized[1])\n",
        "        if layers>1:\n",
        "            O[a,b] = average(R['error'].deserialized[0])\n",
        "            l[a,b] = average(R['error'].deserialized[1])\n",
        "        del c,R\n",
        "    if tell and tell!='all' and tell<1.: print()\n",
        "    elif tell == 1.: print('.', end='')\n",
        "    if layers==1: return record(E)\n",
        "    elif layers==2: return record(O), record(l)\n",
        "    return record(E), record(O), record(l)\n",
        "\n",
        "def PEC(nse, n):\n",
        "    \"\"\"Computes prediction error connectivity.\n",
        "\n",
        "    Args:\n",
        "        nse (list): Preprocessed epoch (resampled and notched).\n",
        "        n (int): Epoch index.\n",
        "\n",
        "    Returns:\n",
        "        array: Connectivity matrix.\n",
        "    \"\"\"\n",
        "    print('{}: '.format(n), end='')\n",
        "    return array(error(nse, 2)[1]) \n",
        "    "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### The function below computes the connectivity matrix, using one of the connectivity measures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def connectivity_analysis(epochs, method, dtail=False, **opts):\n",
        "    \"\"\"Computes a connectivity matrix NN (N - number of nodes) per epoch, containing connectivity method measures for all node pairs.\n",
        "\n",
        "    Args:\n",
        "        epochs (list): List of preprocessed epochs (resampled, filtered and notched).\n",
        "        method (function): Connectivity method.\n",
        "        dtail (bool): If True, computes a square matrix; if False, computes a tringular matrix. Defaults to False.\n",
        "        opts (optional): method-specific arguments.\n",
        "    Returns:\n",
        "        list: List of connectivity matrices for all epochs.\n",
        "    \"\"\"\n",
        "    print('Connectivity Analysis: '+str(method).split()[1])\n",
        "    result = [] \n",
        "    for i,e in enumerate(epochs):    \n",
        "        mat = zeros((len(e),len(e)))                                    \n",
        "        nid, pairs = list(range(len(e))), []\n",
        "        for a in range(len(nid)):                             \n",
        "            if dtail:\n",
        "                for b in range(len(nid)): pairs.append((a,b))\n",
        "            else:\n",
        "                for b in range(a, len(nid)): pairs.append((a,b))                                       \n",
        "        for pair in pairs:                                                       \n",
        "            mat[pair[0],pair[1]] = method(e[pair[0]], e[pair[1]], **opts)\n",
        "        result.append(mat)     \n",
        "        print('{}: completed '.format(i), end='\\n')                                                                                       \n",
        "    return result"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Now that we have defined the objects for loading, handling and processing the data, we proceed with the experiment."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Define the path to your main folder with data in your Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9p_D5CksJG_e",
        "outputId": "d96f6077-b585-472b-aee9-09bbe27646e9"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "main_folder = \"/content/gdrive/My Drive/epigame-folder/\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### The user will now be asked to choose from a number of parameters (see *Guidelines*).\n",
        "###### Choose from the available options by typing the number assigned to the option."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-prEdWVEJG_e",
        "outputId": "56307249-4e40-49a6-cb2c-164b7f45ba20"
      },
      "outputs": [],
      "source": [
        "woi = input(\"Time window:\\n 1. Non-seizure (baseline)\\n 2. Pre-seizure (5 min prior to seizure)\\n 3. Pre-seizure (4 min prior to seizure)\\n 4. Pre-seizure (3 min prior to seizure)\\n 5. Pre-seizure (2 min prior to seizure)\\n 6. Pre-seizure (1 min prior to seizure)\\n 7. Transition to seizure (1 min interval)\\n 8. Transition to seizure (2 min interval)\\n 9. Transition to seizure (60% seizure length interval)\\n 10. Seizure\\n Indicate a number: \")\n",
        "\n",
        "method_idx = input(\"Connectivity method:\\n 1. PEC\\n 2. Spectral Coherence\\n 3. Phase Lock Value\\n 4. Phase-Lag Index\\n 5. Cross-correlation\\n 6. Phase-amplitude coupling\\n Indicate a number: \")\n",
        "\n",
        "ext = \"\"\n",
        "if \"2\" == method_idx: \n",
        "    im = input(\"Imaginary part (Y/N): \").upper()\n",
        "    if im == \"Y\": imag,ext = True,\"I\"\n",
        "    elif im == \"N\": imag,ext = False,\"R\"\n",
        "\n",
        "bands, Bands = input(\"Filter the signal: Y/N \").upper(), False\n",
        "\n",
        "if bands==\"N\": bands = \"w\"\n",
        "elif bands==\"Y\": \n",
        "    Bands = True\n",
        "    mn = int(input(\"Set band range min: \"))\n",
        "    mx = int(input(\"Set band range max: \"))\n",
        "    bands = (mn,mx)\n",
        "\n",
        "method_code = {'1':\"PEC\", '2':\"SC_\", '3':\"PLV\", '4':\"PLI\", '5':\"CC\", '6':\"PAC\"}   \n",
        "woi_code = {'1':\"baseline\", '2':\"preseizure5\", '3':\"preseizure4\", '4':\"preseizure3\", '5':\"preseizure2\", '6':\"preseizure1\", '7':\"transition1\", '8':\"transition2\", '9':\"transition60\", '10':\"seizure\"}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BmQOeok9JG_f"
      },
      "source": [
        "##### The next cell defines the path to the EDF file. \n",
        "###### Filename should be formatted as: patientacronym*-baseline.EDF or patientacronym*-seizure.EDF (asterix signifies a regular expression for any string of choice).\n",
        "###### Subject identifier is saved as a three-letter acronym. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7knsFvFRJG_g",
        "outputId": "17de7ac1-2d9f-4a25-a979-0f97f4fa3096"
      },
      "outputs": [],
      "source": [
        "file_baseline = main_folder + \"data/ASJ2016APR14-PAc-baseline.EDF\"\n",
        "file_seizure = main_folder + \"data/ASJ2016APR14-PAc-seizure.EDF\"\n",
        "\n",
        "subject_id = file_seizure.split(\"/\")[-1][0:3]\n",
        "print(\"Subject ID:\", subject_id)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BxybCmSKJG_h"
      },
      "source": [
        "##### Connectivity is measured in epochs of data. The cell below loads the EDF files and defines the epochs. \n",
        "###### *(These epochs will be classified by the support vector machine (a machine-learning classifier) between baseline and a window of interest (WOI) in the gcolab_find_en.ipynb notebook.)*\n",
        "###### The WOI is fragmented into epochs of 1 second (*span*), with half-second overlaps (*step*). \n",
        "###### To keep the dataset balanced for the classification, we considered a fixed number of epochs, irregardless of the WOI duration. This number was fixed to half of the all epochs in a 1-minute window (119), using the pre-set *span* and *step* parameters. \n",
        "###### The raw data is loaded as *EEG* class object, with the defined epoch parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YorwlMq3JG_h"
      },
      "outputs": [],
      "source": [
        "span, step = 1000, 500      # in ms\n",
        "min_woi_duration = 60000    # in ms\n",
        "n_epochs = int(((min_woi_duration/step)-1) / 2)\n",
        "\n",
        "print(\"Number of epochs to consider for classification =\", n_epochs)\n",
        "\n",
        "eeg_seizure = EEG.from_file(file_seizure, epoch(ms(step), ms(span)))    # load raw seizure SEEG data as an EEG object (class) \n",
        "eeg_baseline = EEG.from_file(file_baseline, epoch(ms(step), ms(span)))   # load raw baseline SEEG data as an EEG object (class) "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2k9zBqXbJG_h"
      },
      "source": [
        "##### Revise the clinical annotations, double-check if the referent annotations (see *Guidelines*) are there."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwcNsUp7JG_i",
        "outputId": "0ef4cd09-06d8-4ea2-9990-bb11333e2d07"
      },
      "outputs": [],
      "source": [
        "print(\"Seizure file annotations:\\n\\n\", [note for note in eeg_seizure.notes])\n",
        "print(\"\\nBaseline file annotations:\\n\\n\", [note for note in eeg_baseline.notes])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzcE5bTIJG_i"
      },
      "source": [
        "##### Check file duration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqQePrCxJG_i",
        "outputId": "3c915c06-403b-41c0-80f0-f8e4d97475d6"
      },
      "outputs": [],
      "source": [
        "print(\"Seizure file duration:\", eeg_seizure.duration)\n",
        "print(\"Baseline file duration:\", eeg_baseline.duration)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5y3gx1LLJG_i"
      },
      "source": [
        "##### Check node labels.\n",
        "###### The seizure and baseline files should have the equivalent number of nodes. If this is not the case, the code below removes the extra nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BD3A-BYQJG_i",
        "outputId": "ecd0e29b-da2c-4e83-960e-b378335c2c3c"
      },
      "outputs": [],
      "source": [
        "nodes_seizure = list(eeg_seizure.axes.region)\n",
        "nodes_baseline = list(eeg_baseline.axes.region)\n",
        "\n",
        "nodes=[]\n",
        "if nodes_seizure==nodes_baseline: nodes = nodes_seizure\n",
        "else:\n",
        "    missing_nodes=[]\n",
        "    if len(nodes_seizure)>len(nodes_baseline): \n",
        "        missing_nodes, nodes = [node for node in nodes_seizure if node not in nodes_baseline], nodes_seizure\n",
        "        for missing in missing_nodes: \n",
        "            print(f\"Node {missing} removed from seizure file as not present in baseline file.\")\n",
        "            del nodes[nodes.index(missing)]\n",
        "            eeg_seizure.axes.region.remove(missing)\n",
        "    \n",
        "    elif len(nodes_seizure)<len(nodes_baseline): \n",
        "        missing_nodes, nodes = [node for node in nodes_baseline if node not in nodes_seizure], nodes_baseline\n",
        "        for missing in missing_nodes: \n",
        "            for missing in missing_nodes: \n",
        "                print(f\"Node {missing} removed from baseline file as not present in seizure file.\")\n",
        "                del nodes[nodes.index(missing)]\n",
        "                eeg_baseline.axes.region.remove(missing)\n",
        "\n",
        "print(\"\\nFiles now have the same nodes:\", nodes_seizure==nodes_baseline)\n",
        "print(\"Number of nodes =\", len(nodes))\n",
        "print(nodes)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rXiZshNnJG_j"
      },
      "source": [
        "##### Check the sampling frequency. \n",
        "###### The code below defines the frequency to which data will be resampled to (500 Hz), to homogenize across files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7t50kTpvJG_j",
        "outputId": "066cf585-c90e-4ac2-df17-9785aefb8cf8"
      },
      "outputs": [],
      "source": [
        "if eeg_baseline.fs == 50 and eeg_seizure.fs == 50: \n",
        "    # this condition was used for patient data with misfiled sampling frequency    \n",
        "    print(\"WARNING: The sampling frequency may have been incorrectly filed... Overwrite with data from subsmeta.xlsx table for now; check with Ale later\")\n",
        "    eeg_seizure._set(fs = 500)\n",
        "    eeg_baseline._set(fs = 500)\n",
        "\n",
        "print(\"Sampling freqency (seizure file) =\", eeg_seizure.fs)\n",
        "print(\"Sampling freqency (baseline file) =\", eeg_baseline.fs)\n",
        "\n",
        "fs_min = eeg_seizure.fs\n",
        " \n",
        "# set the resampled frequency to 500 Hz if sampling is not already 512 Hz\n",
        "resampling = 512 if fs_min==512 else 500"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SnBLQyfNJG_j"
      },
      "source": [
        "##### The following cell labels the baseline and seizure epochs relative to the position of clinical annotations (see *Guidelines*). \n",
        "###### WOI is always defined relative to the *sz_start_note*.\n",
        "###### Baseline file has arbitrary annotations which serve to split the recording into two halves, for computing the baseline connectivity change (change from the first half to the second half)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnO_M-28JG_j",
        "outputId": "c9bf5c5e-aa5f-4ae4-e29b-44e67cfde590"
      },
      "outputs": [],
      "source": [
        "sz_start_note, sz_end_note, base_center_note, base_end_note = 'EEG inicio', 'EEG fin', 'mitad-NS', 'NS-fin'\n",
        "\n",
        "SET(eeg_seizure, _as='N')                      # N - baseline (non-seizure)\n",
        "SET(eeg_seizure, sz_start_note, 'W')            # W - WOI\n",
        "SET(eeg_seizure, sz_end_note, 'S', epoch.END)    # S - seizure\n",
        "\n",
        "SET(eeg_baseline, _as='N')\n",
        "SET(eeg_baseline, base_center_note, 'W')            # W - middle point\n",
        "SET(eeg_baseline, base_end_note, 'S', epoch.END)    # S - terminal point (end of recording)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KMPW7uJuJG_j"
      },
      "source": [
        "##### Next, we optimize the positions of epochs relative to the clinical annotations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eAnkJy0JG_j",
        "outputId": "8414fdbb-7796-447a-b586-4d2b23d5983b"
      },
      "outputs": [],
      "source": [
        "eeg_seizure.optimize()\n",
        "eeg_seizure.remap()\n",
        "\n",
        "eeg_baseline.optimize()\n",
        "eeg_baseline.remap()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FSxPBfM2JG_j"
      },
      "source": [
        "##### The seizure duration is saved in the *units* variable, as the number of epochs present between the clinical annotations of seizure start and end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhiHBHDlJG_k",
        "outputId": "f847a1fc-cd00-421b-9e4b-77f8bf8bdba5"
      },
      "outputs": [],
      "source": [
        "units = int((eeg_seizure.notes[sz_end_note][0].time - eeg_seizure.notes[sz_start_note][0].time)*(span/step))\n",
        "print(\"Seizure length =\", units/2, \"s\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qHnTnj_PJG_k"
      },
      "source": [
        "##### Next, we introduce a virtual mask into the *EEG* class object, which indicates the time window to which an epoch belongs to (baseline, WOI or seizure)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRnijgJPJG_k"
      },
      "outputs": [],
      "source": [
        "if woi == \"1\":\n",
        "    woi_start = -units\n",
        "    woi_end = 0\n",
        "\n",
        "elif woi in [str(n) for n in [2,3,4,5,6]]:\n",
        "    woi_start = - int(woi_code[woi][-1])*n_epochs\n",
        "    woi_end = - (int(woi_code[woi][-1])-1)*n_epochs\n",
        "\n",
        "elif woi in [str(n) for n in [7,8]]:\n",
        "    woi_start = - int(round(int(woi_code[woi][-1])*60/2))\n",
        "    woi_end = - woi_start\n",
        "\n",
        "elif woi == \"9\":\n",
        "    woi_start = - int(round(units*.3))\n",
        "    woi_end = - woi_start\n",
        "\n",
        "elif woi == \"10\":\n",
        "    woi_start = -1\n",
        "    woi_end = 0\n",
        "\n",
        "eeg_seizure.tag(('W', 'S'), W=range(int(woi_start),int(woi_end),1), S=range(0,-units,-1))\n",
        "\n",
        "eeg_baseline.tag(('W', 'S'), W=range(int(woi_start),int(woi_end),1), S=range(0,-units,-1))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nbtrFb6QJG_k"
      },
      "source": [
        "##### Fetch the WOI (*a*) and baseline (*b*) epochs, their indices and labels.\n",
        "###### The epoch labels are set as 0 for seizure and 1 for baseline. These labels will be referred to as ground truth by the classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "t3JLvilWJG_k",
        "outputId": "e7b110dc-81db-4cb7-c429-bc7dc18b12f3"
      },
      "outputs": [],
      "source": [
        "a, ai = eeg_seizure.sample.get('W', n_epochs)   # fetch epochs and epoch indices\n",
        "b, bi = eeg_baseline.sample.get('W', n_epochs)  \n",
        "i = ai + bi                                     # save indices                  \n",
        "x = a + b                                       # save epochs\n",
        "y = [0]*n_epochs + [1]*n_epochs                 # save epoch labels (0 for seizure, 1 for baseline)\n",
        "\n",
        "print(\"Total number of epochs (seizure + baseline) =\", len(x))\n",
        "print(\"Epoch shape =\", a[0].shape)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from random import randint\n",
        "\n",
        "j = randint(0, n_epochs)\n",
        "plt.figure(figsize=(3,3))\n",
        "plt.plot(a[j][0])\n",
        "plt.title(\"Epoch example (seizure file)\")\n",
        "plt.show()\n",
        "plt.figure(figsize=(3,3))\n",
        "plt.plot(b[j-1][0])\n",
        "plt.title(\"Epoch example (baseline file)\")\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WdBYFq7jJG_k"
      },
      "source": [
        "##### Resample and notch-filter the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NyItPiZJG_l",
        "outputId": "00886f27-96b0-4be7-c952-9352e52955bc"
      },
      "outputs": [],
      "source": [
        "pp_seizure = [preprocess(eeg_seizure, ep, resampling) for i,ep in enumerate(x)] \n",
        "print(\"Resampled to\", pp_seizure[0].shape)\n",
        "\n",
        "pp_baseline = [preprocess(eeg_baseline, ep, resampling) for i,ep in enumerate(x)] \n",
        "print(\"Resampled to\", pp_baseline[0].shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RfJmXnpVJG_l"
      },
      "source": [
        "##### Filter the data in the set frequency band, if defined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPGF4de7JG_l"
      },
      "outputs": [],
      "source": [
        "fpp_seizure = [band(e, bands, pp_seizure[0].shape[1]) for e in pp_seizure] if Bands else pp_seizure\n",
        "fpp_baseline = [band(e, bands, pp_baseline[0].shape[1]) for e in pp_baseline] if Bands else pp_baseline"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Compute the connectivity matrices for all epochs. \n",
        "###### The matrices are saved in the *struct* class object, which serves as a table and facilitates accessing the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3CprrN87JG_l",
        "outputId": "085c7d4d-7aab-47f9-89cd-3973e8e76d5d"
      },
      "outputs": [],
      "source": [
        "cm = struct(x=array(x), y=array(y), i=array(i)) # initiating an object for storing a connecivity matrix with shape (x, y) and epoch indices\n",
        "\n",
        "cm._set(nodes = nodes)\n",
        "\n",
        "if method_code[method_idx] == \"SC_\":\n",
        "    cm._set(X = connectivity_analysis(fpp_seizure, spectral_coherence, fs=fpp_seizure[0].shape[1], imag=imag))\n",
        "\n",
        "elif method_code[method_idx] == \"PEC\": \n",
        "    cm._set(X = [PEC(ep,i+1) for i,ep in enumerate(fpp_seizure)]) \n",
        "\n",
        "elif method_code[method_idx] in \"PLV\":\n",
        "    cm._set(X = connectivity_analysis(fpp_seizure, phaselock))\n",
        "\n",
        "elif method_code[method_idx] == \"PLI\":\n",
        "    cm._set(X = connectivity_analysis(fpp_seizure, phaselag))\n",
        "\n",
        "elif method_code[method_idx] == \"CC\":\n",
        "    cm._set(X = connectivity_analysis(fpp_seizure, cross_correlation))\n",
        "\n",
        "elif method_code[method_idx] == \"PAC\":\n",
        "    cm._set(X = connectivity_analysis(fpp_seizure, PAC, True, fpp_seizure[0].shape[1]))\n",
        "\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.imshow(cm.X[-1], cmap='Blues', interpolation='nearest')\n",
        "plt.title(\"Connectivity matrix example\")\n",
        "plt.show()\n",
        "print(cm.X[-1])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nTFdTGEsJG_l"
      },
      "source": [
        "##### The connectivity matrices are saved in a subfolder called *connectivity_matrices*, as files with a custom extension (*PREP*). \n",
        "###### The filenames indicate the preprocessing parameters, as: \n",
        "###### \"*SUB-preseizure5-CC-(70,180).prep*\" if the signal was filtered, and\n",
        "###### \"*SUB-preseizure5-PEC.prep*\" if the signal was not filtered."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZe5N8RiJG_l"
      },
      "outputs": [],
      "source": [
        "path_cm = main_folder + \"connectivity_matrices/\"\n",
        "\n",
        "if Bands:          REc(cm).save(path_cm + f\"{subject_id}-{woi_code[woi]}-{method_code[method_idx]}{ext}-{bands}.prep\".replace(\" \",\"\")) \n",
        "elif not Bands:    REc(cm).save(path_cm + f\"{subject_id}-{woi_code[woi]}-{method_code[method_idx]}{ext}.prep\") "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
