{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect import isfunction, ismethod, isgeneratorfunction, isgenerator, isroutine\n",
    "from inspect import isabstract, isclass, ismodule, istraceback, isframe, iscode, isbuiltin\n",
    "from inspect import ismethoddescriptor, isdatadescriptor, isgetsetdescriptor, ismemberdescriptor\n",
    "from datetime import timedelta as _time\n",
    "from datetime import datetime\n",
    "from collections.abc import Iterable as iterable\n",
    "\n",
    "def some(field): return (field is not None and field != [] and field != {} and field != ()) or field == True\n",
    "def no(field): return not some(field) or field==False or field==''\n",
    "\n",
    "class class_of:\n",
    "    _instance = None\n",
    "    def __init__(_, object):\n",
    "        _._is = type(object)\n",
    "    def inherits(_, *types):\n",
    "        return issubclass(_._is, types)\n",
    "    def has(_, *types): return _.inherits(*types)\n",
    "    def __enter__(self):\n",
    "        self._instance = self\n",
    "        return self\n",
    "    def __exit__(self, type, value, traceback): self._instance = None\n",
    "    @staticmethod\n",
    "    def each_in(list):\n",
    "        if isiterable(list):\n",
    "            return [type(item) for item in list]\n",
    "\n",
    "class struct:\n",
    "    def __init__(table, **sets): table.__dict__.update(sets)\n",
    "    @property\n",
    "    def sets(this): return set(dir(this)) - set(dir(type(this)))\n",
    "    def set(object, **fields):\n",
    "        for field in fields: setattr(object, field, fields[field])\n",
    "    def get(object, *fields): return [getattr(object, field) for field in fields if field in object.__dict__]\n",
    "    def _clonable(set, mask=None):\n",
    "        check = set.__dict__.copy()\n",
    "        clonable = check.copy()\n",
    "        if some(mask): pass\n",
    "#            for field in check:\n",
    "#                if sum([int(_(check[field])) for _ in mask])+sum([int(_(field)) for _ in mask]): clonable.pop(field)\n",
    "        return clonable\n",
    "    @staticmethod\n",
    "    def _from(type):\n",
    "        if hasattr(type, '__dict__'): return struct(**type.__dict__.copy())\n",
    "        return struct()\n",
    "\n",
    "def meta(data, *mask): return struct._from(data)._clonable(mask)\n",
    "def get(data, *fields):\n",
    "    if not issubclass(type(data), dict): data=struct._from(data)._clonable()\n",
    "    return struct(**data).get(*fields)\n",
    "\n",
    "class table(struct):\n",
    "    def _default(field, name, value):\n",
    "        try: return getattr(field, name)\n",
    "        except: setattr(field, name, value)\n",
    "        return value\n",
    "    def clear(this, *fields):\n",
    "        sets = this.sets\n",
    "        if not fields: fields = sets\n",
    "        if fields:\n",
    "            set = [field for field in fields if hasattr(this,field) and not ismethod(getattr(this, field))]\n",
    "            for field in set: delattr(this, field)\n",
    "    def has(this, *fields):\n",
    "        return all([hasattr(this, field) for field in fields])\n",
    "    def has_not(this, *fields): return not this.has(*fields)\n",
    "    def check(this, **KV):\n",
    "        try: check = [KV[key]==this.__dict__[key] for key in KV]\n",
    "        except: return False\n",
    "        return all(check)\n",
    "    def find(this, _type):\n",
    "        return [value for value in this.sets if class_of(get(this,value)[0]).inherits(_type)]\n",
    "    def clone(this): \n",
    "        clone = type(this)()\n",
    "        sets = this._clonable()\n",
    "        clone.set(**sets)\n",
    "        return clone\n",
    "\n",
    "def isiterable(this): return isinstance(this, iterable) and type(this) is not str\n",
    "def default(field, name, value): return table(**field)._default(name, value)\n",
    "\n",
    "def ni(list):\n",
    "    if isiterable(list):\n",
    "        for n,i in enumerate(list): yield n,i\n",
    "    else:\n",
    "        for n,i in enumerate(list.__dict__.keys()): yield n,i\n",
    "\n",
    "class at(table):\n",
    "    DAY, HOUR, MIN = 86400, 3600, 60\n",
    "    def __init__(_, dtime=None, **sets):\n",
    "        _.set(**sets)\n",
    "        if some(dtime) and issubclass(type(dtime), _time): _._time = dtime\n",
    "        else:\n",
    "            d,h,m,s,ms = _._default('d',0), _._default('h',0), _._default('m',0), _._default('s',0), _._default('ms',0)\n",
    "            if not any([d,h,m,s,ms]): now=datetime.now(); _._time = now-datetime(now.year, now.month, now.day)\n",
    "            else: _._time = _time(days=d, hours=h, minutes=m, seconds=s, milliseconds=ms)\n",
    "        _.clear('d','h','m','s','ms')\n",
    "    def __sub__(_, dtime):\n",
    "        of=type(dtime); sets=_._clonable()\n",
    "        if issubclass(of, _time): return at(_._time-dtime, **sets)\n",
    "        elif issubclass(of, at): sets.update(dtime._clonable()); return at(_._time-dtime._time, **sets)\n",
    "    def __add__(_, dtime):\n",
    "        of=type(dtime); sets=_._clonable()\n",
    "        if issubclass(of, _time): return at(_._time+dtime, **sets)\n",
    "        elif issubclass(of, at): sets.update(dtime._clonable()); return at(_._time+dtime._time, **sets)\n",
    "    def __str__(_): return str(_._time)\n",
    "    @property\n",
    "    def seconds(_): return _._time.seconds\n",
    "    @property\n",
    "    def S(_): return _.seconds\n",
    "    @property\n",
    "    def minutes(_): return _._time.seconds/60\n",
    "    @property\n",
    "    def M(_): return _.minutes\n",
    "    @property\n",
    "    def hours(_): return _.minutes/60\n",
    "    @property\n",
    "    def H(_): return _.hours\n",
    "    @property\n",
    "    def days(_): return _._time.days\n",
    "    @property\n",
    "    def D(_): return _.days\n",
    "    @staticmethod\n",
    "    def zero(): return at(_time())\n",
    "\n",
    "from inspect import isfunction, ismethod, isgeneratorfunction, isgenerator, isroutine\n",
    "from inspect import isabstract, isclass, ismodule, istraceback, isframe, iscode, isbuiltin\n",
    "from inspect import ismethoddescriptor, isdatadescriptor, isgetsetdescriptor, ismemberdescriptor\n",
    "from inspect import isawaitable, iscoroutinefunction, iscoroutine\n",
    "\n",
    "from collections.abc import Iterable as iterable\n",
    "\n",
    "import pickle\n",
    "\n",
    "def isfx(field): return ismethod(field) or isfunction(field)\n",
    "\n",
    "class GhostSet:\n",
    "    \"\"\" enhanced interface (ghost) to retrieve class fields \"\"\"\n",
    "    def _meta(data): return {k:v for k,v in data.__dict__.items() if not isfx(v)}\n",
    "    def _at_last(_, sets): pass\n",
    "    def _set(object, **sets):\n",
    "        ''' use to fast initialize fields | needed to avoid initialization problems at copy by value '''\n",
    "        for field in sets: setattr(object, field, sets[field])\n",
    "        object._at_last(sets)\n",
    "GSet = GhostSet\n",
    "\n",
    "def meta(object):\n",
    "    ''' retrieves clonable object metadata (__dict__) as a copy '''\n",
    "    if isinstance(object, GSet): return object._meta()\n",
    "    return {}\n",
    "\n",
    "class ClonableObjectGhost:\n",
    "    \"\"\" enhanced interface (ghost) for clonable objects \"\"\"\n",
    "    def _by_val(_, depth=-1, _layer=0): pass\n",
    "GCo = ClonableObjectGhost\n",
    "\n",
    "class ClonableObject(GSet, GCo):\n",
    "    \"\"\" base clonable object \"\"\"\n",
    "    def __init__(this, **data): this._set(**data)\n",
    "    def __call__(_, **options): _._set(**options)\n",
    "    def _by_val(_, depth=-1, _layer=0):\n",
    "        copy = type(_)()\n",
    "        copy._set(**_._meta())\n",
    "        if depth<0 or depth>_layer:\n",
    "            for field in copy.__dict__:\n",
    "                if isinstance(copy.__dict__[field], ClonableObjectGhost):\n",
    "                    copy.__dict__[field] = copy.__dict__[field]._by_val(depth,_layer+1)\n",
    "        return copy\n",
    "COb = ClonableObject\n",
    "\n",
    "def copy_by_val(object, depth=-1, _layer=0):\n",
    "    if isinstance(object, GCo): return object._by_val(depth,_layer)\n",
    "    return object\n",
    "copy = by_val = vof = copy_by_val\n",
    "\n",
    "class ComparableGhost:\n",
    "    \"\"\" enhanced interface (ghost) for comparing instances \"\"\"\n",
    "    def _compare(a, b):\n",
    "        if type(a) != type(b): return False\n",
    "        if a.__dict__ == b.__dict__: return True\n",
    "        return False\n",
    "    def __eq__(a, b): return a._compare(b)\n",
    "GEq = ComparableGhost\n",
    "\n",
    "class IterableObjectGhost(GSet):\n",
    "    \"\"\" enhanced interface (ghost) for iterables: exposes __dict__,\n",
    "        therefore Iterable Objects are like lua dictionaries \"\"\"\n",
    "    def __contains__(this, key): return key in this.__dict__\n",
    "    def __iter__(this): return iter(this.__dict__)\n",
    "    def items(my): return my.__dict__.items()\n",
    "    def __getitem__(by, field): return by.__dict__[field]\n",
    "    def __setitem__(by, field, value): by.__dict__[field] = value\n",
    "    def pop(by, field): return by.__dict__.pop(field)\n",
    "GIo = IterableObjectGhost\n",
    "\n",
    "class ReprGhost:\n",
    "    \"\"\" enhanced interface (ghost) for the skeleton method _repr,\n",
    "        see implementation of Struct for a working example;\n",
    "        Record __repr__ override uses _lines_ for max lines display \"\"\"\n",
    "    _lines_ = 31\n",
    "    _chars_ = 13\n",
    "    _msgsz_ = 62\n",
    "    _ellipsis_ = ' ... '\n",
    "    def _repr(my, value):\n",
    "        _type = ''.join(''.join(str(type(value)).split('class ')).split(\"'\"))\n",
    "        _value = '{}'.format(value)\n",
    "        if len(_value)>my._chars_:\n",
    "            show = int(my._chars_/2)\n",
    "            _value = _value[:show]+my._ellipsis_+_value[-show:]\n",
    "        return '{} {}'.format(_type, _value)\n",
    "    def _resize(this, message, at=.7):\n",
    "        if len(message)>this._msgsz_:\n",
    "            start = int(at*this._msgsz_)\n",
    "            end = this._msgsz_-start\n",
    "            return message[:start]+this._ellipsis_+message[-end:]\n",
    "        return message\n",
    "GRe = ReprGhost\n",
    "\n",
    "def set_repr_to(lines): GRe._lines_ = lines\n",
    "\n",
    "class Struct(COb, GEq, GIo, GRe):\n",
    "    \"\"\" structured autoprintable object, behaves like a lua dictionary \"\"\"\n",
    "    def __repr__(_):\n",
    "        return '\\n'.join(['{}:\\t{}'.format(k, _._repr(v)) for k,v in _.items()])\n",
    "struct = Struct\n",
    "\n",
    "class RecordableGhost:\n",
    "    \"\"\" enhanced interface (ghost) for type recording,\n",
    "        see Record for a working example \"\"\"\n",
    "    @staticmethod\n",
    "    def load(filename):\n",
    "        with open(filename, 'rb') as file: return pickle.load(file)\n",
    "    def save(data, filename):\n",
    "        with open(filename, 'wb') as file: pickle.dump(data, file)\n",
    "        \n",
    "GRec = RecordableGhost\n",
    "\n",
    "class Record(GSet, GCo, GRec, GEq, GRe):\n",
    "    \"\"\" wrapper for any object or value, auto-inspects and provides load/save type structure \"\"\"\n",
    "    data = None\n",
    "    _check = dict(\n",
    "            isfunction=isfunction, ismethod=ismethod, isgeneratorfunction=isgeneratorfunction, isgenerator=isgenerator, isroutine=isroutine,\n",
    "            isabstract=isabstract, isclass=isclass, ismodule=ismodule, istraceback=istraceback, isframe=isframe, iscode=iscode, isbuiltin=isbuiltin,\n",
    "            ismethoddescriptor=ismethoddescriptor, isdatadescriptor=isdatadescriptor, isgetsetdescriptor=isgetsetdescriptor, ismemberdescriptor=ismemberdescriptor,\n",
    "            isawaitable=isawaitable, iscoroutinefunction=iscoroutinefunction, iscoroutine=iscoroutine\n",
    "                   )\n",
    "    def __init__(this, token, **meta):\n",
    "        this.data = token\n",
    "        this.__dict__.update({k:v(token) for k,v in this._check.items()})\n",
    "        super()._set(**meta)\n",
    "    @property\n",
    "    def type(_): return type(_.data)\n",
    "    def inherits(_, *types): return issubclass(_.type, types)\n",
    "    @property\n",
    "    def isbaseiterable(_): return _.inherits(tuple, list, dict, set) or _.isgenerator or _.isgeneratorfunction\n",
    "    @property\n",
    "    def isiterable(_): return isinstance(_.data, iterable) and _.type is not str\n",
    "    def _clone_iterable(_):\n",
    "        if _.inherits(dict): return _.data.copy()\n",
    "        elif _.isgenerator or _.isgeneratorfunction: return (i for i in list(_.data))\n",
    "        else: return type(_.data)(list(_.data)[:])\n",
    "    def _meta(data): return {k:v for k,v in data.__dict__.items() if k != 'data' and not isfx(v)}\n",
    "    def _by_val(_, depth=-1, layer=0):\n",
    "        data = _.data\n",
    "        if _.isiterable: data = _._clone_iterable()\n",
    "        elif _.inherits(ClonableObjectGhost): data = by_val(data, depth, layer)\n",
    "        return type(_)(data, **meta(_))\n",
    "    def __enter__(self): self._instance = self; return self\n",
    "    def __exit__(self, type, value, traceback): self._instance = None\n",
    "    def __repr__(self):\n",
    "        if not hasattr(self, '_preprint'): return Record(self.data, _preprint='', _lines=Record(Record._lines_)).__repr__()\n",
    "        if self.isbaseiterable:\n",
    "            pre, repr = self._preprint, ''\n",
    "            for n,i in enumerate(self.data):\n",
    "                if self._lines.data == 0: break\n",
    "                else: self._lines.data -= 1\n",
    "                index, item = str(n), i\n",
    "                if self.inherits(dict): index += ' ({})'.format(str(i)); item = self.data[i]\n",
    "                repr += pre+'{}: '.format(index)\n",
    "                next = Record(item, _preprint=pre+'\\t', _lines=self._lines)\n",
    "                if next.isiterable: repr += '\\n'\n",
    "                repr += next.__repr__()\n",
    "                repr += '\\n'\n",
    "            return repr\n",
    "        elif self.inherits(GCo): return Record(self.data._meta(), _preprint=self._preprint, _lines=self._lines).__repr__()\n",
    "        else: return self._repr(self.data)\n",
    "REc = Record\n",
    "\n",
    "class Bisect(list, COb):\n",
    "    \"\"\" bisect implementation using clonable objects \"\"\"\n",
    "    def __init__(set, *items, key=None, reverse=False):\n",
    "        if not key: key = lambda  x:x\n",
    "        super().__init__(sorted(items, reverse=reverse, key=key))\n",
    "    def _bisect(set, item, key, reverse, bottom, top):\n",
    "        def _(check):\n",
    "            if key: return key(check)\n",
    "            return check\n",
    "        at = int((top-bottom)/2)+bottom\n",
    "        if len(set)==0: return (0,-1)\n",
    "        if item==_(set[at]): return (at,0)\n",
    "        bigger = item<_(set[at])\n",
    "        if bigger != reverse:\n",
    "            if at-bottom>0: return set._bisect(item, key, reverse, bottom, at)\n",
    "            return (at,-1)\n",
    "        elif top-at>1: return set._bisect(item, key, reverse, at, top)\n",
    "        return (at,1)\n",
    "    def search(_, item, key=None, reverse=False):\n",
    "        if not key: key = lambda x:x\n",
    "        return _._bisect(item, key, reverse, 0, len(_))\n",
    "    def _by_val(_, depth=-1, _layer=0):\n",
    "        copy = super()._by_val(depth, _layer)\n",
    "        copy += _[:]\n",
    "        return copy\n",
    "BSx = Bisect\n",
    "\n",
    "from numpy import ndarray, resize, linspace, arange\n",
    "from numpy import min, max, average, floor\n",
    "from numpy import ubyte, zeros, array\n",
    "from scipy.signal import lfilter, butter\n",
    "from matplotlib import pylab as lab\n",
    "\n",
    "_NOTCH = _FR = 50\n",
    "_SAMPLING = 500\n",
    "_CONTINUOUS = 1\n",
    "_UNIT = 'ms'\n",
    "\n",
    "class rec(table, ndarray):\n",
    "    @property\n",
    "    def dimensions(of): return len(of.shape)\n",
    "    @property\n",
    "    def is_scalar(this): return this.shape is ()\n",
    "    @property\n",
    "    def is_vector(this): return len(this.shape)==1\n",
    "    @property\n",
    "    def is_matrix(this): return len(this.shape)>1\n",
    "    @property\n",
    "    def is_cube(this): return len(this.shape) == 3\n",
    "    @property\n",
    "    def is_binary(this): return this.dtype == ubyte and max(this) == 1\n",
    "    @property\n",
    "    def serialized(data):\n",
    "        if not data.is_scalar and data.dimensions>1:\n",
    "            return rec.read(data.T.flatten(), _deser=data.T.shape, **meta(data))\n",
    "        return data\n",
    "    @property\n",
    "    def deserialized(data):\n",
    "        if data.has('_deser'):\n",
    "            deser = rec.read(resize(data, data._deser).T, **meta(data))\n",
    "            deser.clear('_deser')\n",
    "            return deser\n",
    "        return data\n",
    "    @property\n",
    "    def as_matrix(data):    #implement numpy matrix\n",
    "        if data.is_vector: return rec.read([data], to=type(data), **meta(data))\n",
    "        return data\n",
    "    @property\n",
    "    def raw(data):\n",
    "        if data.shape[0] == 1: return rec.read(data[0], **meta(data)).raw\n",
    "        return data\n",
    "    def join(base, *parts, **sets):\n",
    "        flip, parts = None, list(parts)\n",
    "        if 'flip' in sets: flip=sets.pop('flip')\n",
    "        next = parts[0]\n",
    "        if len(parts)>1: next = rec.join(parts[0], parts[1:])\n",
    "        congruent = base.dimensions == next.dimensions and base.dimensions < 3\n",
    "        if congruent:\n",
    "            sets.update(base._clonable())\n",
    "            A, B = base, next\n",
    "            if flip: A, B = base.T, next.T\n",
    "            C = record(A.tolist()+B.tolist(), **sets)\n",
    "            if flip: return record(C.T, **sets)\n",
    "            return C\n",
    "    def get_as(this, data, cast=None):\n",
    "        source = this\n",
    "        if no(cast):\n",
    "            if issubclass(type(data), rec): cast = type(data)\n",
    "            else: cast = type(this)\n",
    "        if issubclass(type(data), ndarray): source = resize(this, data.shape)\n",
    "        return rec.read(source, to=cast, **meta(data))\n",
    "    @staticmethod\n",
    "    def read(iterable, to=None, **sets):\n",
    "        if no(to) or not issubclass(to, rec): to = rec\n",
    "        data = array(iterable).view(to)\n",
    "        data.set(**sets)\n",
    "        return data\n",
    "    def clone(this, **sets):\n",
    "        copy = this.copy().view(type(this))\n",
    "        sets.update(this._clonable())\n",
    "        copy.set(**sets)\n",
    "        return copy\n",
    "    def exclude(data, *items, **sets):\n",
    "        if no(items) or data.is_scalar: return data\n",
    "        excluded, items = None, [item for item in range(len(data)) if item not in items]\n",
    "        if data.is_vector: excluded = rec.read([data])[:,items][0]\n",
    "        else: excluded = data[items,:]\n",
    "        return rec.read(excluded, to=type(data), **meta(data), **sets)\n",
    "    def include(data, *items, **sets):\n",
    "        if no(items) or data.is_scalar: return data\n",
    "        included = []\n",
    "        if data.is_vector: included = rec.read([data])[:,items][0]\n",
    "        else: included = data[items,:]\n",
    "        return rec.read(included, to=type(data), **meta(data), **sets)\n",
    "\n",
    "create = record = rec.read\n",
    "line = linspace\n",
    "\n",
    "def series(ori,end=None,by=1):\n",
    "    if no(end): end=ori; ori=0\n",
    "    if not issubclass(type(by), int): return create(arange(ori,end,by))\n",
    "    return array(range(ori,end,by))\n",
    "\n",
    "def plot(data, at = 0., spacing = 1., color = 'k', width = 1., offset=0.): #review\n",
    "    draw = record(data, **meta(data)); at = spacing*draw.as_matrix.shape[0]\n",
    "    axes = lab.gca(); axes.set_ylim([at+max(data),0-max(data)]); at=0\n",
    "    for n, row in ni(draw.as_matrix):\n",
    "        if some(offset): row = draw[n]-average(row)+offset\n",
    "        c, w = color, width\n",
    "        if isiterable(color): c = color[n]\n",
    "        if isiterable(width): w = width[n]\n",
    "        lab.plot(at+row+n*spacing, color = c, linewidth = w)\n",
    "\n",
    "def butter_type(lowcut, highcut, fs, order=5, type='band'):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype=type)\n",
    "    return b, a\n",
    "\n",
    "def butter_filter(data, lowcut, highcut, fs, order=5, type='band'):\n",
    "    b, a = butter_type(lowcut, highcut, fs, order=order, type=type)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "def _to_rec(this):\n",
    "    if not issubclass(type(this), rec): return rec.read(this, **meta(this)), rec\n",
    "    return this, type(this)\n",
    "\n",
    "def _prefilt(data, fs):\n",
    "    pre = []\n",
    "    for line in data.as_matrix:\n",
    "        vector = line.tolist()\n",
    "        pre.append(vector[:int(fs)]+vector)\n",
    "    return record(pre)\n",
    "\n",
    "def _postfilt(data, fs):\n",
    "    post = []\n",
    "    for line in data:\n",
    "        vector = line.tolist()\n",
    "        post.append(vector[int(fs):])\n",
    "    return post\n",
    "\n",
    "def notch(this, using=butter_type, fs=_SAMPLING, size=2, at=_NOTCH, order=5):\n",
    "    data, type = _to_rec(this)\n",
    "    if data.has('sampling'): fs=data.sampling\n",
    "    nyq, cutoff = fs / 2., []\n",
    "    for f in range(int(at), int(nyq), int(at)):\n",
    "        cutoff.append((f - size, f + size))\n",
    "    signal = _prefilt(data, fs)\n",
    "    for bs in cutoff:\n",
    "        low,hi = bs\n",
    "        b,a = butter_type(low,hi,fs,order,'bandstop')\n",
    "        signal = lfilter(b,a,signal)\n",
    "    return record(_postfilt(signal, fs), to=type, **meta(data))\n",
    "\n",
    "def band(this, low_high, fs=_SAMPLING, using=butter_filter, order=5):\n",
    "    data, type = _to_rec(this)\n",
    "    if data.has('sampling'): fs=data.sampling\n",
    "    low, high = min(low_high), max(low_high)\n",
    "    if low<1.: low = 1.\n",
    "    tailed = _prefilt(data, fs)\n",
    "    tailed = using(tailed, low, high, fs, order)\n",
    "    return rec.read(_postfilt(tailed, fs), to=type, **meta(data))\n",
    "\n",
    "def binarize(this):\n",
    "    data, type = _to_rec(this)\n",
    "    if data.is_binary: return data\n",
    "    rows = []\n",
    "    for row in data.as_matrix:\n",
    "        d = row - array([row[-1]]+row[:-1].tolist())\n",
    "        d[d>=0] = 1; d[d<0] = 0\n",
    "        rows.append(d.astype(ubyte))\n",
    "    return rec.read(rows, to=type).get_as(data)\n",
    "\n",
    "def halve(matrix):\n",
    "    halved, (data, type) = [], _to_rec(matrix)\n",
    "    for line in data.as_matrix:\n",
    "        h = resize(line, (int(len(line)/2), 2))\n",
    "        halved.append((h[:,0]+h[:,1])/2.)\n",
    "    return rec.read(halved, to=type, **meta(data))\n",
    "\n",
    "def dwindle(matrix, by=1):\n",
    "    if by: return dwindle(halve(matrix), by-1)\n",
    "    return matrix\n",
    "\n",
    "def upsample(matrix, fs1, fs2):\n",
    "    y=zeros((matrix.shape[0],fs2))\n",
    "    if fs1 < fs2:\n",
    "        #upsampling by a factor R\n",
    "        L=matrix.shape[1]\n",
    "        R=int(floor(fs2/fs1)+1)\n",
    "        for i,e in enumerate(matrix):\n",
    "            ups=[]\n",
    "            for j in range(L-1):\n",
    "                if j>0: ups.append(list(linspace(matrix[i][j],matrix[i][j+1],R)[1:3]))\n",
    "                else: ups.append(list(linspace(matrix[i][j],matrix[i][j+1],R)[0:3]))\n",
    "            for k,s in enumerate(sum(ups, [])): y[i][k]=s \n",
    "            y[i][-1]=y[i][-2]\n",
    "        return rec.read(y)\n",
    "    else: print(\"Error: fs1 >= fs2\")\n",
    "\n",
    "def remap(this, axis=None, base=0, top=1., e=0):\n",
    "    def map(x, b, t, e): return ((x-min(x)+e)/(max(x)-min(x)+e)+b)*(t-b)\n",
    "    data, type = _to_rec(this)\n",
    "    if no(axis): return rec.read(map(this, base, top, e), to=type, **meta(data))\n",
    "    rows = data.as_matrix\n",
    "    if axis==0 or axis>1: rows = rows.T\n",
    "    remapped = []\n",
    "    for row in rows: remapped.append(map(row, base, top, e))\n",
    "    if axis==0 or axis>1: rows = rows.T\n",
    "    return rec.read(remapped).get_as(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "import random\n",
    "\n",
    "def classify_epochs(set, node_group, kratio=.1, random_state=31, **mopts):\n",
    "    \"\"\"Quantifies connectivity change of specified node group.\n",
    "    The support vector machine classifies time frame epochs using the connectivity measures as features. \n",
    "    K-fold cross validation scores measure the connectivity change.\n",
    "\n",
    "    Args:\n",
    "        set (core.rec): Preprocessed data - connectivity matrices per epoch.\n",
    "        nodes (set): Node group for which connectivity change is quantified. \n",
    "        kratio (float): Ratio of epochs considered in a fold. Defaults to 0.1, which is ~10s of data if epoch span is 1s.\n",
    "        random_state (int): KFold argument; Controls randomness of each fold. Defaults to 31.\n",
    "\n",
    "    Returns:\n",
    "        ndarray of float: Array of scores for each fold.\n",
    "    \"\"\"\n",
    "    X, Y = array([array((record(x).include(*node_group).T).include(*node_group)).flatten() for x in set.X]), set.y\n",
    "    model, scaler, k = SVC, StandardScaler, int(round(len(Y)*kratio))\n",
    "    if random_state == None: random_state = random.randint(0xFFFFFFFF)\n",
    "    C = Pipeline([('scaler', scaler()), ('model', model(**mopts))])\n",
    "    cv = KFold(k, shuffle=True, random_state=random_state)\n",
    "    cvs = cross_val_score(C, X, Y, cv=cv)\n",
    "    return cvs\n",
    "\n",
    "def evaluate_nodes(nodes, labels, results, symbol='<->'):\n",
    "    \"\"\"Service function. \n",
    "    Creates sets with node indices, labels, cross validation scores.\n",
    "\n",
    "    Args:\n",
    "        nodes ([type]): [description]\n",
    "        labels ([type]): [description]\n",
    "        results (ndarray): Cross validation score array.\n",
    "        symbol (str): Symbol used to connect node labels within a network. Defaults to '<->'.\n",
    "\n",
    "    Returns:\n",
    "        set: A set of all the arguments.\n",
    "    \"\"\"\n",
    "    tag = symbol.join([labels[n] for n in nodes])\n",
    "    return (nodes, tag, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_folder = \"/home/kivi/gdrive/epigame-folder/\"\n",
    "\n",
    "path_cm = main_folder + \"connectivity_matrices/\" \n",
    "path_net = main_folder + \"selected_network/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "woi_code = {'1':\"baseline\", '2':\"preseizure5\", '3':\"preseizure4\", '4':\"preseizure3\", '5':\"preseizure2\", '6':\"preseizure1\", '7':\"transition1\", '8':\"transition2\", '9':\"transition60\", '10':\"seizure\"}\n",
    "woi_code_inv = dict((v,k) for k, v in woi_code.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "file_path = path_cm + \"9-preseizure5-CC-(0,4).prep\"\n",
    "\n",
    "print(\"File:\", file_path)\n",
    "\n",
    "filename = file_path.split(\"/\")[-1]\n",
    "subject_id = filename.split(\"-\")[0]\n",
    "woi = woi_code_inv[filename.split(\"-\")[1]]\n",
    "measure = filename.split(\"-\")[2] if len(filename.split(\"-\"))==4 else filename.split(\"-\")[-1].split(\".\")[0]\n",
    "bands = filename.split(\"-\")[-1].split(\".\")[0] if len(filename.split(\"-\"))==4 else None\n",
    "\n",
    "print(\"Connectivity matrices of\", subject_id)\n",
    "\n",
    "cm = REc.load(file_path).data\n",
    "\n",
    "nodes = cm.nodes\n",
    "node_ids = list(range(len(nodes))) \n",
    "print(\"Number of nodes =\",len(nodes))\n",
    "print(\"\\nNodes:\", nodes)\n",
    "\n",
    "print(\"\\nTotal number of epochs =\", len(cm.X))\n",
    "print(\"Connectivity matrix shape =\", cm.X[0].shape)\n",
    "print(\"All matrices have the same shape:\", all([m.shape==(len(nodes),len(nodes)) for m in cm.X]))\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(cm.X[-1], cmap='Blues', interpolation='nearest')\n",
    "plt.show()\n",
    "print(cm.X[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
